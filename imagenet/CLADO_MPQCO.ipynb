{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f1149d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,torchvision,os,time\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models             \n",
    "\n",
    "from mqbench.prepare_by_platform import prepare_by_platform   # add quant nodes for specific Backend\n",
    "from mqbench.prepare_by_platform import BackendType           # contain various Backend, like TensorRT, NNIE, etc.\n",
    "from mqbench.utils.state import enable_calibration            # turn on calibration algorithm, determine scale, zero_point, etc.\n",
    "from mqbench.utils.state import enable_quantization           # turn on actually quantization, like FP32 -> INT8\n",
    "from mqbench.utils.state import disable_all           \n",
    "from copy import deepcopy\n",
    "from mqbench.advanced_ptq import ptq_reconstruction\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48f39020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/zdeng/.conda/envs/mltls/lib/python3.9/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "from mltools.data import I1K\n",
    "import torchvision as tv\n",
    "\n",
    "modelname = 'resnet18'\n",
    "adv_ptq = False\n",
    "dataset = 'I1K'\n",
    "mn = dataset.lower()+ '_' + modelname\n",
    "ds = I1K(data_dir=os.path.join('/tools/d-matrix/ml/data', \"imagenet\"),\n",
    "         train_batch_size=64,test_batch_size=64,cuda=True)\n",
    "model = eval(\"tv.models.\" + modelname)(pretrained=True).cuda()\n",
    "ds.train.num_workers = 12\n",
    "ds.val.num_workers = 12\n",
    "\n",
    "train = ds.train\n",
    "test = ds.val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a4e0889",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "def evaluate(val_loader, model,\n",
    "             criterion = torch.nn.CrossEntropyLoss().cuda(),device='cuda'):\n",
    "    s_time = time.time()\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    count,top1,top5,losses = 0,0,0,0\n",
    "    with torch.no_grad():\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            images, target = images.to(device), target.to(device)\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            losses = losses * count/(count+images.size(0)) + loss * images.size(0)/(count+images.size(0))\n",
    "            top1 = top1 * count/(count+images.size(0)) + acc1 * images.size(0)/(count+images.size(0))\n",
    "            top5 = top5 * count/(count+images.size(0)) + acc5 * images.size(0)/(count+images.size(0))\n",
    "            count += images.size(0)\n",
    "    test_time = time.time() - s_time\n",
    "    \n",
    "    return {'top1':top1,'top5':top5,'loss':losses,'time':test_time}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caf1b06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/zdeng/.conda/envs/mltls/lib/python3.9/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'top1': tensor([69.7600], device='cuda:0'),\n",
       " 'top5': tensor([89.0800], device='cuda:0'),\n",
       " 'loss': tensor(1.2469, device='cuda:0'),\n",
       " 'time': 89.31889748573303}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(test,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f67af0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# calibration data used to calibrate PTQ and MPQ\n",
    "calib_data = []\n",
    "stacked_tensor = []\n",
    "calib_fp_output = []\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    for img,label in train:\n",
    "        i += 1\n",
    "        # stacked_tensor is to calibrate the model\n",
    "        # calib_data (part of it, as defined later) is the data to calculate ltilde\n",
    "        if i<= 16:\n",
    "            stacked_tensor.append(img)\n",
    "        \n",
    "        calib_data.append((img,label))\n",
    "        #calib_fp_output.append(model(img.cuda()))\n",
    "        print(i)\n",
    "        if i>=300:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3d31b23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MPQ_scheme = (2,4,8)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b007f436",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare 2bits model using MQBench\n",
      "[MQBENCH] INFO: Quantize model Scheme: BackendType.Academic Mode: Eval\n",
      "[MQBENCH] INFO: Weight Qconfig:\n",
      "    FakeQuantize: FixedFakeQuantize Params: {}\n",
      "    Oberver:      MSEObserver Params: Symmetric: True / Bitwidth: 2 / Per channel: False / Pot scale: False / Extra kwargs: {}\n",
      "[MQBENCH] INFO: Activation Qconfig:\n",
      "    FakeQuantize: FixedFakeQuantize Params: {}\n",
      "    Oberver:      EMAMSEObserver Params: Symmetric: False / Bitwidth: 8 / Per channel: False / Pot scale: False / Extra kwargs: {}\n",
      "[MQBENCH] INFO: Replace module to qat module.\n",
      "[MQBENCH] INFO: Set layer conv1 to 8 bit.\n",
      "[MQBENCH] INFO: Set layer fc to 8 bit.\n",
      "dbg node_to_quantize_output\n",
      " odict_keys([x, maxpool, layer1_0_relu, layer1_0_relu_1, layer1_1_relu, layer1_1_relu_1, layer2_0_relu, layer2_0_relu_1, layer2_1_relu, layer2_1_relu_1, layer3_0_relu, layer3_0_relu_1, layer3_1_relu, layer3_1_relu_1, layer4_0_relu, layer4_0_relu_1, layer4_1_relu, flatten])\n",
      "[MQBENCH] INFO: Set x post act quantize to 8 bit.\n",
      "[MQBENCH] INFO: Insert act quant x_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant maxpool_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_0_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_0_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_1_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_1_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_0_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_0_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_1_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_1_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_0_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_0_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_1_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_1_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer4_0_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer4_0_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer4_1_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Set flatten post act quantize to 8 bit.\n",
      "[MQBENCH] INFO: Insert act quant flatten_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Enable observer and Disable quantize.\n",
      "Prepare 4bits model using MQBench\n",
      "[MQBENCH] INFO: Quantize model Scheme: BackendType.Academic Mode: Eval\n",
      "[MQBENCH] INFO: Weight Qconfig:\n",
      "    FakeQuantize: FixedFakeQuantize Params: {}\n",
      "    Oberver:      MSEObserver Params: Symmetric: True / Bitwidth: 4 / Per channel: False / Pot scale: False / Extra kwargs: {}\n",
      "[MQBENCH] INFO: Activation Qconfig:\n",
      "    FakeQuantize: FixedFakeQuantize Params: {}\n",
      "    Oberver:      EMAMSEObserver Params: Symmetric: False / Bitwidth: 8 / Per channel: False / Pot scale: False / Extra kwargs: {}\n",
      "[MQBENCH] INFO: Replace module to qat module.\n",
      "[MQBENCH] INFO: Set layer conv1 to 8 bit.\n",
      "[MQBENCH] INFO: Set layer fc to 8 bit.\n",
      "dbg node_to_quantize_output\n",
      " odict_keys([x, maxpool, layer1_0_relu, layer1_0_relu_1, layer1_1_relu, layer1_1_relu_1, layer2_0_relu, layer2_0_relu_1, layer2_1_relu, layer2_1_relu_1, layer3_0_relu, layer3_0_relu_1, layer3_1_relu, layer3_1_relu_1, layer4_0_relu, layer4_0_relu_1, layer4_1_relu, flatten])\n",
      "[MQBENCH] INFO: Set x post act quantize to 8 bit.\n",
      "[MQBENCH] INFO: Insert act quant x_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant maxpool_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_0_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_0_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_1_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_1_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_0_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_0_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_1_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_1_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_0_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_0_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_1_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_1_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer4_0_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer4_0_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer4_1_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Set flatten post act quantize to 8 bit.\n",
      "[MQBENCH] INFO: Insert act quant flatten_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Enable observer and Disable quantize.\n",
      "Prepare 8bits model using MQBench\n",
      "[MQBENCH] INFO: Quantize model Scheme: BackendType.Academic Mode: Eval\n",
      "[MQBENCH] INFO: Weight Qconfig:\n",
      "    FakeQuantize: FixedFakeQuantize Params: {}\n",
      "    Oberver:      MSEObserver Params: Symmetric: True / Bitwidth: 8 / Per channel: False / Pot scale: False / Extra kwargs: {}\n",
      "[MQBENCH] INFO: Activation Qconfig:\n",
      "    FakeQuantize: FixedFakeQuantize Params: {}\n",
      "    Oberver:      EMAMSEObserver Params: Symmetric: False / Bitwidth: 8 / Per channel: False / Pot scale: False / Extra kwargs: {}\n",
      "[MQBENCH] INFO: Replace module to qat module.\n",
      "[MQBENCH] INFO: Set layer conv1 to 8 bit.\n",
      "[MQBENCH] INFO: Set layer fc to 8 bit.\n",
      "dbg node_to_quantize_output\n",
      " odict_keys([x, maxpool, layer1_0_relu, layer1_0_relu_1, layer1_1_relu, layer1_1_relu_1, layer2_0_relu, layer2_0_relu_1, layer2_1_relu, layer2_1_relu_1, layer3_0_relu, layer3_0_relu_1, layer3_1_relu, layer3_1_relu_1, layer4_0_relu, layer4_0_relu_1, layer4_1_relu, flatten])\n",
      "[MQBENCH] INFO: Set x post act quantize to 8 bit.\n",
      "[MQBENCH] INFO: Insert act quant x_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant maxpool_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_0_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_0_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_1_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_1_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_0_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_0_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_1_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_1_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_0_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_0_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_1_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_1_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer4_0_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer4_0_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer4_1_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Set flatten post act quantize to 8 bit.\n",
      "[MQBENCH] INFO: Insert act quant flatten_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Enable observer and Disable quantize.\n"
     ]
    }
   ],
   "source": [
    "# configuration\n",
    "ptq_reconstruction_config_init = {\n",
    "    'pattern': 'block',                   #? 'layer' for Adaround or 'block' for BRECQ and QDROP\n",
    "    'scale_lr': 4.0e-5,                   #? learning rate for learning step size of activation\n",
    "    'warm_up': 0.2,                       #? 0.2 * max_count iters without regularization to floor or ceil\n",
    "    'weight': 0.01,                       #? loss weight for regularization item\n",
    "    'max_count': 1,                   #? optimization iteration\n",
    "    'b_range': [20,2],                    #? beta decaying range\n",
    "    'keep_gpu': True,                     #? calibration data restore in gpu or cpu\n",
    "    'round_mode': 'learned_hard_sigmoid', #? ways to reconstruct the weight, currently only support learned_hard_sigmoid\n",
    "    'prob': 0.5,                          #? dropping probability of QDROP, 1.0 for Adaround and BRECQ\n",
    "}\n",
    "\n",
    "\n",
    "ptq_reconstruction_config = {\n",
    "    'pattern': 'block',                   #? 'layer' for Adaround or 'block' for BRECQ and QDROP\n",
    "    'scale_lr': 4.0e-5,                   #? learning rate for learning step size of activation\n",
    "    'warm_up': 0.2,                       #? 0.2 * max_count iters without regularization to floor or ceil\n",
    "    'weight': 0.01,                       #? loss weight for regularization item\n",
    "    'max_count': 20000,                   #? optimization iteration\n",
    "    'b_range': [20,2],                    #? beta decaying range\n",
    "    'keep_gpu': True,                     #? calibration data restore in gpu or cpu\n",
    "    'round_mode': 'learned_hard_sigmoid', #? ways to reconstruct the weight, currently only support learned_hard_sigmoid\n",
    "    'prob': 0.5,                          #? dropping probability of QDROP, 1.0 for Adaround and BRECQ\n",
    "}\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "ptq_reconstruction_config = dotdict(ptq_reconstruction_config)\n",
    "ptq_reconstruction_config_init = dotdict(ptq_reconstruction_config_init)\n",
    "\n",
    "def getModuleByName(model,moduleName):\n",
    "    '''\n",
    "        replace module with name modelName.moduleName with newModule\n",
    "    '''\n",
    "    tokens = moduleName.split('.')\n",
    "    m = model\n",
    "    for tok in tokens:\n",
    "        m = getattr(m,tok)\n",
    "    return m\n",
    "\n",
    "for b in MPQ_scheme:\n",
    "    mqb_fp_model = deepcopy(model)\n",
    "    \n",
    "    # MSE calibration on model parameters\n",
    "    backend = BackendType.Academic\n",
    "    extra_config = {\n",
    "        'extra_qconfig_dict': {\n",
    "            'w_observer': 'MSEObserver',                              # custom weight observer\n",
    "            'a_observer': 'EMAMSEObserver',                              # custom activation observer\n",
    "            'w_fakequantize': 'AdaRoundFakeQuantize' if adv_ptq else 'FixedFakeQuantize',\n",
    "            'a_fakequantize': 'QDropFakeQuantize' if adv_ptq else 'FixedFakeQuantize',\n",
    "            'w_qscheme': {\n",
    "                'bit': b,                                             # custom bitwidth for weight,\n",
    "                'symmetry': True,                                    # custom whether quant is symmetric for weight,\n",
    "                'per_channel': False,                                  # custom whether quant is per-channel or per-tensor for weight,\n",
    "                'pot_scale': False,                                   # custom whether scale is power of two for weight.\n",
    "            },\n",
    "            'a_qscheme': {\n",
    "                'bit': 8,                                             # custom bitwidth for activation,\n",
    "                'symmetry': False,                                    # custom whether quant is symmetric for activation,\n",
    "                'per_channel': False,                                  # custom whether quant is per-channel or per-tensor for activation,\n",
    "                'pot_scale': False,                                   # custom whether scale is power of two for activation.\n",
    "            }\n",
    "        }                                                         # custom tracer behavior, checkout https://github.com/pytorch/pytorch/blob/efcbbb177eacdacda80b94ad4ce34b9ed6cf687a/torch/fx/_symbolic_trace.py#L836\n",
    "    }\n",
    "    print(f'Prepare {b}bits model using MQBench')\n",
    "\n",
    "    exec(f'mqb_{b}bits_model=prepare_by_platform(mqb_fp_model, backend,extra_config).cuda()')\n",
    "    \n",
    "    # calibration loop\n",
    "    enable_calibration(eval(f'mqb_{b}bits_model'))\n",
    "    for img in stacked_tensor:\n",
    "        eval(f'mqb_{b}bits_model')(img.cuda())\n",
    "    \n",
    "    if adv_ptq:\n",
    "        if os.path.exists(f'QDROP_{b}bits_{mn}.pt'):\n",
    "            exec(f'mqb_{b}bits_model=ptq_reconstruction(mqb_{b}bits_model, stacked_tensor, ptq_reconstruction_config_init).cuda()')\n",
    "            print(f'QDROP model already saved, now loading QDROP_{b}bits_{mn}.pt')\n",
    "            load_from = f'QDROP_{b}bits_{mn}.pt'\n",
    "            exec(f'mqb_{b}bits_model.load_state_dict(torch.load(load_from))')\n",
    "        else:\n",
    "            \n",
    "            exec(f'mqb_{b}bits_model=ptq_reconstruction(mqb_{b}bits_model, stacked_tensor, ptq_reconstruction_config).cuda()')\n",
    "            print(f'saving QDROP tuned model: QDROP_{b}bits_{mn}.pt...')\n",
    "            torch.save(eval(f'mqb_{b}bits_model').state_dict(),f'QDROP_{b}bits_{mn}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c08c97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MQBENCH] INFO: Disable observer and Disable quantize.\n",
      "[MQBENCH] INFO: Disable observer and Enable quantize.\n",
      "evaluate mqb 2bits model\n",
      "{'top1': tensor([0.1180], device='cuda:0'), 'top5': tensor([0.5520], device='cuda:0'), 'loss': tensor(7.0920, device='cuda:0'), 'time': 81.07145833969116}\n",
      "[MQBENCH] INFO: Disable observer and Disable quantize.\n",
      "[MQBENCH] INFO: Disable observer and Enable quantize.\n",
      "evaluate mqb 4bits model\n",
      "{'top1': tensor([46.2740], device='cuda:0'), 'top5': tensor([71.4781], device='cuda:0'), 'loss': tensor(2.4289, device='cuda:0'), 'time': 81.22445607185364}\n",
      "[MQBENCH] INFO: Disable observer and Disable quantize.\n",
      "[MQBENCH] INFO: Disable observer and Enable quantize.\n",
      "evaluate mqb 8bits model\n",
      "{'top1': tensor([69.4741], device='cuda:0'), 'top5': tensor([88.9981], device='cuda:0'), 'loss': tensor(1.2566, device='cuda:0'), 'time': 81.39034652709961}\n"
     ]
    }
   ],
   "source": [
    "for b in MPQ_scheme: \n",
    "    disable_all(eval(f'mqb_{b}bits_model'))\n",
    "    # evaluation loop\n",
    "    enable_quantization(eval(f'mqb_{b}bits_model'))\n",
    "    eval(f'mqb_{b}bits_model').eval()\n",
    "    print(f'evaluate mqb {b}bits model')\n",
    "    print(evaluate(test,eval(f'mqb_{b}bits_model')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f1d8226",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MQBENCH] INFO: Disable observer and Disable quantize.\n",
      "input of  conv1  is  x_post_act_fake_quantizer\n",
      "input of  layer1.0.conv1  is  maxpool_post_act_fake_quantizer\n",
      "input of  layer1.0.conv2  is  layer1_0_relu_post_act_fake_quantizer\n",
      "input of  layer1.1.conv1  is  layer1_0_relu_1_post_act_fake_quantizer\n",
      "input of  layer1.1.conv2  is  layer1_1_relu_post_act_fake_quantizer\n",
      "input of  layer2.0.conv1  is  layer1_1_relu_1_post_act_fake_quantizer\n",
      "input of  layer2.0.conv2  is  layer2_0_relu_post_act_fake_quantizer\n",
      "input of  layer2.0.downsample.0  is  layer1_1_relu_1_post_act_fake_quantizer\n",
      "input of  layer2.1.conv1  is  layer2_0_relu_1_post_act_fake_quantizer\n",
      "input of  layer2.1.conv2  is  layer2_1_relu_post_act_fake_quantizer\n",
      "input of  layer3.0.conv1  is  layer2_1_relu_1_post_act_fake_quantizer\n",
      "input of  layer3.0.conv2  is  layer3_0_relu_post_act_fake_quantizer\n",
      "input of  layer3.0.downsample.0  is  layer2_1_relu_1_post_act_fake_quantizer\n",
      "input of  layer3.1.conv1  is  layer3_0_relu_1_post_act_fake_quantizer\n",
      "input of  layer3.1.conv2  is  layer3_1_relu_post_act_fake_quantizer\n",
      "input of  layer4.0.conv1  is  layer3_1_relu_1_post_act_fake_quantizer\n",
      "input of  layer4.0.conv2  is  layer4_0_relu_post_act_fake_quantizer\n",
      "input of  layer4.0.downsample.0  is  layer3_1_relu_1_post_act_fake_quantizer\n",
      "input of  layer4.1.conv1  is  layer4_0_relu_1_post_act_fake_quantizer\n",
      "input of  layer4.1.conv2  is  layer4_1_relu_post_act_fake_quantizer\n",
      "input of  fc  is  flatten_post_act_fake_quantizer\n"
     ]
    }
   ],
   "source": [
    "mqb_fp_model = deepcopy(mqb_8bits_model)\n",
    "disable_all(mqb_fp_model)\n",
    "mqb_mix_model = deepcopy(mqb_fp_model)\n",
    "\n",
    "# 1. record all modules we want to consider\n",
    "types_to_quant = (torch.nn.Conv2d,torch.nn.Linear)\n",
    "\n",
    "layer_input_map = {}\n",
    "\n",
    "for node in mqb_8bits_model.graph.nodes:\n",
    "    try:\n",
    "        node_target = getModuleByName(mqb_mix_model,node.target)\n",
    "        if isinstance(node_target,types_to_quant):\n",
    "            node_args = node.args[0]\n",
    "            print('input of ',node.target,' is ',node_args)\n",
    "            layer_input_map[node.target] = str(node_args.target)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecaa6af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ref_metric = ('loss',evaluate(calib_data,mqb_fp_model)['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdeec63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb(perturb_scheme):\n",
    "    # perturb_scheme: {layer_name:(act_bits,weight_bits)}\n",
    "    for layer_name in perturb_scheme:\n",
    "        a_bits,w_bits = perturb_scheme[layer_name]\n",
    "        \n",
    "        if w_bits is not None:\n",
    "            mix_module = getModuleByName(mqb_mix_model,layer_name)\n",
    "            tar_module = getModuleByName(eval(f'mqb_{w_bits}bits_model'),layer_name)\n",
    "            # replace weight quant to use a_bits quantization\n",
    "            w_cmd = f'mix_module.weight_fake_quant=tar_module.weight_fake_quant'\n",
    "            exec(w_cmd)\n",
    "        \n",
    "        if a_bits is not None:\n",
    "        \n",
    "            # replace act quant to use w_bits quantization\n",
    "            a_cmd = f'mqb_mix_model.{layer_input_map[layer_name]}=mqb_{a_bits}bits_model.{layer_input_map[layer_name]}'\n",
    "            exec(a_cmd)\n",
    "        \n",
    "        #print(layer_name)\n",
    "        #print(a_cmd)\n",
    "        #print(w_cmd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d64f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def estimate_deltaL(eval_data,wbit_choices=[2,4,8]):\n",
    "    \n",
    "    tot_batches = len(eval_data)\n",
    "    \n",
    "    processed_batches = 0\n",
    "    \n",
    "    print(f'MPPCO Ltilde: {processed_batches}/{tot_batches} batch of data processed')\n",
    "    \n",
    "    for batch_img,batch_label in eval_data:\n",
    "        \n",
    "        s_time = time.time()\n",
    "        # init deltaL dictionary\n",
    "        deltaL = {}\n",
    "        for layer in layer_input_map:\n",
    "            if layer in ('conv1','fc'):\n",
    "                continue\n",
    "            deltaL[layer] = {}\n",
    "            for wbit in wbit_choices:\n",
    "                deltaL[layer][wbit] = 0\n",
    "        \n",
    "        for i in range(batch_img.size(0)):\n",
    "            \n",
    "            img,label = batch_img[i].unsqueeze(0),batch_label[i]\n",
    "            model.zero_grad()\n",
    "            logits = model(img.cuda())       \n",
    "            logits[0][label].backward()\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                for layer_name in layer_input_map:\n",
    "                    if layer_name in ('conv1','fc'):\n",
    "                        continue\n",
    "                    for w_bits in wbit_choices:\n",
    "                        tar_module = getModuleByName(eval(f'mqb_{w_bits}bits_model'),layer_name)\n",
    "                        dw = tar_module.weight_fake_quant(tar_module.weight) - tar_module.weight\n",
    "                        dl = (dw * getModuleByName(model,layer_name).weight.grad).sum()\n",
    "                        dl /= logits[0][label]\n",
    "                        dl = dl ** 2\n",
    "                        deltaL[layer_name][w_bits] += dl.cpu().numpy()\n",
    "        \n",
    "        for layer in layer_input_map:\n",
    "            if layer in ('conv1','fc'):\n",
    "                continue\n",
    "            for wbit in wbit_choices:\n",
    "                deltaL[layer][wbit] /= 2 * batch_img.size(0)\n",
    "        \n",
    "        deltaL['n_samples'] = batch_img.size(0)\n",
    "        \n",
    "        #print(deltaL)\n",
    "        with open(f'MPQCO_DELTAL_resnet18_batch{processed_batches}(size64).pkl','wb') as f:\n",
    "            pickle.dump(deltaL,f)\n",
    "            \n",
    "        processed_batches += 1\n",
    "        \n",
    "        print(f'MPPCO Ltilde: {processed_batches}/{tot_batches} batch of data processed')\n",
    "        print(f'batch cost:{time.time()-s_time:.2f} seconds')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4fa39f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the main step to generate deltal\n",
    "# already done and saved\n",
    "# estimate_deltaL(eval_data=calib_data,wbit_choices=[2,4,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8846ea48",
   "metadata": {},
   "source": [
    "## CLADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa6716b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "kl_loss = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "def kldiv(quant_logit,fp_logit):\n",
    "    inp = F.log_softmax(quant_logit,dim=-1)\n",
    "    tar = F.softmax(fp_logit,dim=-1)\n",
    "    return kl_loss(inp,tar)\n",
    "\n",
    "def perturb_loss(perturb_scheme,ref_metric,\n",
    "                 eval_data=calib_data,printInfo=False,KL=False):\n",
    "    \n",
    "    global mqb_mix_model\n",
    "    mqb_mix_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # perturb layers\n",
    "        perturb(perturb_scheme)\n",
    "            \n",
    "        # do evaluation\n",
    "        if not KL:\n",
    "            res = evaluate(eval_data,mqb_mix_model)\n",
    "            perturbed_loss = res[ref_metric[0]] - ref_metric[1]\n",
    "        else:\n",
    "            perturbed_loss = []\n",
    "            \n",
    "            for (data,fp_out) in zip(calib_data,calib_fp_output):\n",
    "                img,label = data\n",
    "                quant_out = mqb_mix_model(img.cuda())\n",
    "                perturbed_loss.append(kldiv(quant_out,fp_out))\n",
    "            #print(perturbed_loss)\n",
    "            perturbed_loss = torch.tensor(perturbed_loss).mean()    \n",
    "        \n",
    "        if printInfo:\n",
    "            print(f'use kl {KL} perturbed loss {perturbed_loss}')\n",
    "        \n",
    "        # recover layers\n",
    "        mqb_mix_model = deepcopy(mqb_fp_model)\n",
    "            \n",
    "    return perturbed_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de4162ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perturb loss functionality check\n",
    "# del layer_input_map['conv1']\n",
    "# del layer_input_map['fc']\n",
    "\n",
    "# for layer in layer_input_map:\n",
    "#     for a_bits in MPQ_scheme:\n",
    "#         for w_bits in MPQ_scheme:\n",
    "#             print(f'{layer} (a:{a_bits} bits,w:{w_bits} bits))')\n",
    "#             p = perturb_loss({layer:(a_bits,w_bits)},eval_data=test,printInfo=True,KL=False)\n",
    "#             #print(f'{layer} (a:{a_bits} bits,w:{w_bits} bits), accuracy degradation: {p*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47dd85f",
   "metadata": {},
   "source": [
    "## Check Ltilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7319de56",
   "metadata": {},
   "outputs": [],
   "source": [
    "del layer_input_map['conv1']\n",
    "del layer_input_map['fc']\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "s_time = time.time()\n",
    "cached = {}\n",
    "aw_scheme = []\n",
    "for a_bits in MPQ_scheme:\n",
    "    for w_bits in MPQ_scheme:\n",
    "        aw_scheme.append((a_bits,w_bits))\n",
    "\n",
    "aw_scheme = [(8,2),(8,4),(8,8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f370df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part is done\n",
    "# saved Ltilde are stored under Ltilde_resnet18 folder\n",
    "# KL=False\n",
    "\n",
    "# for clado_batch in range(len(calib_data)):\n",
    "#     print(f'clado batch {clado_batch+1}/{len(calib_data)}')\n",
    "#     ref_metric = ('loss',\n",
    "#                   evaluate([calib_data[clado_batch],],mqb_fp_model)['loss'])\n",
    "    \n",
    "#     s_time = time.time()\n",
    "#     cached = {}\n",
    "#     for n in layer_input_map:\n",
    "#         for m in layer_input_map:\n",
    "#             for naw in aw_scheme:\n",
    "#                 for maw in aw_scheme:\n",
    "#                     if (n,m,naw,maw) not in cached:\n",
    "#                         if n == m:\n",
    "#                             if naw == maw:\n",
    "#                                 p = perturb_loss({n:naw},ref_metric,\n",
    "#                                                  [calib_data[clado_batch],],KL=KL)\n",
    "#                                 #print(f'perturb layer {n} to A{naw[0]}W{naw[1]} p={p}')\n",
    "#                             else:\n",
    "#                                 p = 0\n",
    "\n",
    "#                         else:\n",
    "#                             p = perturb_loss({n:naw,m:maw},ref_metric,\n",
    "#                                              [calib_data[clado_batch],],KL=KL)\n",
    "#                             #print(f'perturb layer {n} to A{naw[0]}W{naw[1]} and layer {m} to A{maw[0]}W{maw[1]} p={p}')\n",
    "\n",
    "#                         cached[(n,m,naw,maw)] = cached[(m,n,maw,naw)] = p\n",
    "\n",
    "#     print(f'{time.time()-s_time:.2f} seconds elapsed')\n",
    "    \n",
    "#     # layer index and index2layerscheme map\n",
    "#     layer_index = {}\n",
    "#     cnt = 0\n",
    "#     for layer in layer_input_map:\n",
    "#         for s in aw_scheme:\n",
    "#             layer_index[layer+f'{s}bits'] = cnt\n",
    "#             cnt += 1\n",
    "#     L = cnt\n",
    "\n",
    "#     import numpy as np\n",
    "#     hm = np.zeros(shape=(L,L))\n",
    "#     for n in layer_input_map:\n",
    "#         for m in layer_input_map:\n",
    "#             for naw in aw_scheme:\n",
    "#                 for maw in aw_scheme:\n",
    "#                     hm[layer_index[n+f'{naw}bits'],layer_index[m+f'{maw}bits']] = cached[(n,m,naw,maw)]\n",
    "\n",
    "#     index2layerscheme = [None for i in range(hm.shape[0])]\n",
    "\n",
    "#     for name in layer_index:\n",
    "#         index = layer_index[name]\n",
    "#         layer_name = name[:-10]\n",
    "#         scheme = name[-10:]\n",
    "\n",
    "#         index2layerscheme[index] = (layer_name,scheme)\n",
    "    \n",
    "#     import pickle\n",
    "\n",
    "#     saveas = f'Ltilde_resnet18/Ltilde_batch{clado_batch}(size64)_'\n",
    "#     saveas += 'QDROP' if adv_ptq else ''\n",
    "#     saveas += str(aw_scheme)\n",
    "#     saveas += mn\n",
    "#     saveas += 'KL' if KL else ''\n",
    "#     saveas += '.pkl'\n",
    "\n",
    "#     with open(saveas,'wb') as f:\n",
    "#         pickle.dump({'Ltilde':hm,'layer_index':layer_index,'index2layerscheme':index2layerscheme},f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bf5977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Ltilde_resnet18/Ltilde_batch0(size64)_[(8, 2), (8, 4), (8, 8)]i1k_resnet18.pkl','rb') as f:\n",
    "    hm = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b0a356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_layer_index = hm['layer_index']\n",
    "ref_index2layerscheme = hm['index2layerscheme']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e4de1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_Ltildes_clado = []\n",
    "for batch_id in range(128):\n",
    "    with open(f'Ltilde_resnet18/Ltilde_batch{batch_id}(size64)_[(8, 2), (8, 4), (8, 8)]i1k_resnet18.pkl','rb') as f:\n",
    "        hm = pickle.load(f)\n",
    "    \n",
    "    assert hm['layer_index'] == ref_layer_index\n",
    "    batch_Ltildes_clado.append(hm['Ltilde'])\n",
    "batch_Ltildes_clado = np.array(batch_Ltildes_clado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "144035b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_Ltilde_clado = batch_Ltildes_clado.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7331958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_Ltildes_mpqco = []\n",
    "for batch_id in range(128):\n",
    "    with open(f'Ltilde_resnet18/MPQCO_DELTAL_resnet18_batch{batch_id}(size64).pkl','rb') as f:\n",
    "        hm = pickle.load(f)\n",
    "        \n",
    "    deltal = np.zeros(ref_Ltilde_clado.shape)\n",
    "    \n",
    "    for layer_id in range(len(ref_index2layerscheme)):\n",
    "        layer_name,scheme = ref_index2layerscheme[layer_id]\n",
    "        wbit = eval(scheme[:-4])[1]\n",
    "        deltal[layer_id,layer_id] = hm[layer_name][wbit]\n",
    "    \n",
    "    batch_Ltildes_mpqco.append(deltal)\n",
    "batch_Ltildes_mpqco = np.array(batch_Ltildes_mpqco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0a5b70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_Ltilde_mpqco = batch_Ltildes_mpqco.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dea34571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.72430306e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 1.21279000e-02, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 4.13480306e-04, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       ...,\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        2.49214630e-01, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 6.94756301e-03, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 7.57598502e-05]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_Ltilde_mpqco \n",
    "# note: this matrix is to be directly put into optimize func with naive=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e1806d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.56769905e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        2.86954624e+00, 1.54701936e+00, 1.56092381e+00],\n",
       "       [0.00000000e+00, 7.93386525e-02, 0.00000000e+00, ...,\n",
       "        1.61352164e+00, 9.80280922e-02, 8.04051580e-02],\n",
       "       [0.00000000e+00, 0.00000000e+00, 9.55908559e-04, ...,\n",
       "        1.53179326e+00, 2.10755626e-02, 2.51121772e-03],\n",
       "       ...,\n",
       "       [2.86954624e+00, 1.61352164e+00, 1.53179326e+00, ...,\n",
       "        1.52865512e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.54701936e+00, 9.80280922e-02, 2.10755626e-02, ...,\n",
       "        0.00000000e+00, 1.96765019e-02, 0.00000000e+00],\n",
       "       [1.56092381e+00, 8.04051580e-02, 2.51121772e-03, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.31084956e-03]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_Ltilde_clado \n",
    "# note: this matrix needs to be \n",
    "#(1) processed to cached_grad (linear equations in manuscript) \n",
    "#(2) applied PSD approximation \n",
    "# and then put into optimize func with naive=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864bfca8",
   "metadata": {},
   "source": [
    "### Hook to record input and output shapes of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88a839b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer_hook(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(layer_hook, self).__init__()\n",
    "        self.in_shape = None\n",
    "        self.out_shape = None\n",
    "\n",
    "    def hook(self, module, inp, outp):\n",
    "        self.in_shape = inp[0].size()\n",
    "        self.out_shape = outp.size()\n",
    "    \n",
    "\n",
    "hooks = {}\n",
    "\n",
    "for layer in ref_layer_index:\n",
    "    m = getModuleByName(model,layer[:-10])\n",
    "    hook = layer_hook()\n",
    "    hooks[layer[:-10]] = (hook,m.register_forward_hook(hook.hook))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1bc42d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in hooks:\n",
    "#     hooks[layer][1].remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f56512fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for img,label in calib_data:\n",
    "        model(img.cuda())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76e26a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_bitops(layer_name,a_bits,w_bits):\n",
    "    m = getModuleByName(model,layer_name)\n",
    "    if isinstance(m,torch.nn.Conv2d):\n",
    "        _,cin,_,_ = hooks[layer_name][0].in_shape \n",
    "        _,cout,hout,wout = hooks[layer_name][0].out_shape\n",
    "        n_muls = cin * m.weight.size()[2] * m.weight.size()[3] * cout * hout * wout \n",
    "        n_accs = (cin * m.weight.size()[2] * m.weight.size()[3] - 1) * cout * hout * wout\n",
    "        #bitops_per_mul = 2 * a_bits * w_bits\n",
    "        #bitops_per_acc = (a_bits + w_bits) + np.ceil(np.log2(cin * m.weight.size()[2] * m.weight.size()[\n",
    "        bitops_per_mul = 5*a_bits*w_bits - 5*a_bits-3*w_bits+3 \n",
    "        bitops_per_acc = 3*a_bits + 3*w_bits + 29\n",
    "    return n_muls * bitops_per_mul + n_accs * bitops_per_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "235c0301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ref_layer_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4555d2",
   "metadata": {},
   "source": [
    "### Calculate sizes and numbers of bitoperations for layers under different quantization options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5886f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_size = np.array([0 for i in range(len(ref_layer_index))])\n",
    "layer_bitops = np.array([0 for i in range(len(ref_layer_index))])\n",
    "for l in ref_layer_index:\n",
    "    index = ref_layer_index[l]\n",
    "    layer_name, scheme = ref_index2layerscheme[index]\n",
    "    a_bits,w_bits = eval(scheme[:-4])\n",
    "    layer_size[index] = torch.numel(getModuleByName(model,layer_name).weight) * int(w_bits)\n",
    "    layer_bitops[index] = get_layer_bitops(layer_name,a_bits,w_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04760381",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = ref_Ltilde_clado.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d72c510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_decision(v,printInfo=False,test=test):\n",
    "    global mqb_mix_model\n",
    "    v = v.detach()\n",
    "    # alpha = torch.nn.Softmax(dim=1)(v.reshape(-1,len(MPQ_scheme)))\n",
    "    offset = torch.ones(int(L/len(aw_scheme)),dtype=int) * len(aw_scheme)\n",
    "    offset = offset.cumsum(dim=-1) - len(aw_scheme)\n",
    "    select = v.reshape(-1,len(aw_scheme)).argmax(dim=1) + offset\n",
    "    \n",
    "    modelsize = (layer_size[select]).sum()/8/1024/1024\n",
    "    bitops = (layer_bitops[select]).sum()/10**9\n",
    "    \n",
    "    decisions = {}\n",
    "    for scheme_id in select.numpy():\n",
    "        layer,scheme = ref_index2layerscheme[scheme_id]\n",
    "        decisions[layer] = eval(scheme[:-4])\n",
    "    \n",
    "    print(\"evaluate_decision\\n\",decisions)\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # perturb layers\n",
    "        perturb(decisions)\n",
    "            \n",
    "        # do evaluation\n",
    "        res = evaluate(test,mqb_mix_model)\n",
    "        \n",
    "        # recover layers\n",
    "        mqb_mix_model = deepcopy(mqb_fp_model)\n",
    "    return res,modelsize,bitops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a23a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "\n",
    "def MIQCP_optimize(cached_grad,layer_bitops,layer_size,\n",
    "                   schemes_per_layer=len(aw_scheme),\n",
    "                   bitops_bound=np.inf,size_bound=np.inf,\n",
    "                   naive=False,PSD=True):\n",
    "    \n",
    "    if cached_grad.__class__ == torch.Tensor:\n",
    "        cached_grad = cached_grad.cpu().numpy()\n",
    "    \n",
    "    x = cp.Variable(cached_grad.shape[0], boolean=True)\n",
    "    schemes_per_layer = schemes_per_layer\n",
    "    assert cached_grad.shape[0]%schemes_per_layer == 0, 'cached_gradient shape[0] does not divde schemes per layer'\n",
    "    num_layers = cached_grad.shape[0]//schemes_per_layer\n",
    "    \n",
    "    if not naive:\n",
    "        # convexation of cached_grad\n",
    "        es,us = np.linalg.eig(cached_grad)\n",
    "        if PSD:\n",
    "            es[es<0] = 0\n",
    "        C = us@np.diag(es)@us.T\n",
    "        C = (C+C.T)/2\n",
    "        C = cp.atoms.affine.wraps.psd_wrap(C)\n",
    "        objective = cp.Minimize(cp.quad_form(x,C))\n",
    "    else:\n",
    "        objective = cp.Minimize(np.diagonal(cached_grad)@x)\n",
    "\n",
    "    equality_constraint_matrix = []\n",
    "    for i in range(num_layers):\n",
    "        col = np.zeros(cached_grad.shape[0])\n",
    "        col[i*schemes_per_layer:(i+1)*schemes_per_layer] = 1\n",
    "        equality_constraint_matrix.append(col)\n",
    "\n",
    "    equality_constraint_matrix = np.array(equality_constraint_matrix)\n",
    "\n",
    "    constraints = [equality_constraint_matrix@x == np.ones((num_layers,)),\n",
    "                   layer_bitops@x/10**9<=bitops_bound,\n",
    "                   layer_size@x/8/1024/1024<=size_bound]\n",
    "\n",
    "    prob = cp.Problem(objective,constraints)\n",
    "    prob.solve(verbose=False,TimeLimit=60)\n",
    "    \n",
    "    # Print result.\n",
    "    print(\"Solution status\", prob.status)\n",
    "    print(\"A solution x is\")\n",
    "    print(x.value)\n",
    "    #print(f\"bitops: {x.value@layer_bitops}\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036b4939",
   "metadata": {},
   "source": [
    "## Sanity Check: no constraint optimization\n",
    "Without constraint, optimization should return (ideally) an 8-bit model, or performance close to 8-bit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a60987e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ltilde2CachedGrad(Ltilde):\n",
    "    \n",
    "    cached_grad = np.zeros_like(Ltilde)\n",
    "        \n",
    "    for i in range(cached_grad.shape[0]):\n",
    "        for j in range(cached_grad.shape[0]):\n",
    "            layer_i,scheme_i = ref_index2layerscheme[i]\n",
    "            layer_j,scheme_j = ref_index2layerscheme[j]\n",
    "            if layer_i == layer_j:\n",
    "                if scheme_i == scheme_j:\n",
    "                    cached_grad[i,j] = cached_grad[j,i] = 2 * Ltilde[i,j]\n",
    "                else:\n",
    "                    cached_grad[i,j] = cached_grad[j,i] = 0\n",
    "            else:\n",
    "                cached_grad[i,j] = cached_grad[j,i] = Ltilde[i,j] - Ltilde[i,i] - Ltilde[j,j]\n",
    "    return cached_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9556fe5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution status optimal\n",
      "A solution x is\n",
      "[0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 1.]\n",
      "evaluate_decision\n",
      " {'layer1.0.conv1': (8, 8), 'layer1.0.conv2': (8, 8), 'layer1.1.conv1': (8, 8), 'layer1.1.conv2': (8, 8), 'layer2.0.conv1': (8, 8), 'layer2.0.conv2': (8, 8), 'layer2.0.downsample.0': (8, 8), 'layer2.1.conv1': (8, 8), 'layer2.1.conv2': (8, 8), 'layer3.0.conv1': (8, 8), 'layer3.0.conv2': (8, 8), 'layer3.0.downsample.0': (8, 8), 'layer3.1.conv1': (8, 8), 'layer3.1.conv2': (8, 8), 'layer4.0.conv1': (8, 8), 'layer4.0.conv2': (8, 8), 'layer4.0.downsample.0': (8, 8), 'layer4.1.conv1': (8, 8), 'layer4.1.conv2': (8, 8)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'top1': tensor([69.4841], device='cuda:0'),\n",
       "  'top5': tensor([88.9362], device='cuda:0'),\n",
       "  'loss': tensor(1.2558, device='cuda:0'),\n",
       "  'time': 80.89391040802002},\n",
       " 10.640625,\n",
       " 569.57449472)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MPQCO Way\n",
    "v = MIQCP_optimize(cached_grad=ref_Ltilde_mpqco,\n",
    "                   layer_bitops=layer_bitops,\n",
    "                   layer_size=layer_size,\n",
    "                   schemes_per_layer=len(aw_scheme),\n",
    "                   bitops_bound=np.inf,size_bound=np.inf,\n",
    "                   naive=True)\n",
    "v = torch.Tensor(v.value)\n",
    "evaluate_decision(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5acba60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution status optimal\n",
      "A solution x is\n",
      "[ 0. -0.  1.  0. -0.  1.  0. -0.  1.  0. -0.  1.  0. -0.  1.  0. -0.  1.\n",
      "  0. -0.  1.  0. -0.  1.  0. -0.  1.  0. -0.  1.  0.  0.  1.  0. -0.  1.\n",
      "  0. -0.  1.  0.  0.  1.  0. -0.  1.  0.  1. -0.  0. -0.  1.  0.  1. -0.\n",
      "  0.  1.  0.]\n",
      "evaluate_decision\n",
      " {'layer1.0.conv1': (8, 8), 'layer1.0.conv2': (8, 8), 'layer1.1.conv1': (8, 8), 'layer1.1.conv2': (8, 8), 'layer2.0.conv1': (8, 8), 'layer2.0.conv2': (8, 8), 'layer2.0.downsample.0': (8, 8), 'layer2.1.conv1': (8, 8), 'layer2.1.conv2': (8, 8), 'layer3.0.conv1': (8, 8), 'layer3.0.conv2': (8, 8), 'layer3.0.downsample.0': (8, 8), 'layer3.1.conv1': (8, 8), 'layer3.1.conv2': (8, 8), 'layer4.0.conv1': (8, 8), 'layer4.0.conv2': (8, 4), 'layer4.0.downsample.0': (8, 8), 'layer4.1.conv1': (8, 4), 'layer4.1.conv2': (8, 4)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'top1': tensor([67.5640], device='cuda:0'),\n",
       "  'top5': tensor([87.9401], device='cuda:0'),\n",
       "  'loss': tensor(1.3294, device='cuda:0'),\n",
       "  'time': 82.22663879394531},\n",
       " 7.265625,\n",
       " 514.084755968)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MPQCO Way\n",
    "v = MIQCP_optimize(cached_grad=ref_Ltilde_mpqco,\n",
    "                   layer_bitops=layer_bitops,\n",
    "                   layer_size=layer_size,\n",
    "                   schemes_per_layer=len(aw_scheme),\n",
    "                   bitops_bound=np.inf,size_bound=8,\n",
    "                   naive=True)\n",
    "v = torch.Tensor(v.value)\n",
    "evaluate_decision(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0217953a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution status optimal\n",
      "A solution x is\n",
      "[0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 1.]\n",
      "evaluate_decision\n",
      " {'layer1.0.conv1': (8, 8), 'layer1.0.conv2': (8, 8), 'layer1.1.conv1': (8, 8), 'layer1.1.conv2': (8, 8), 'layer2.0.conv1': (8, 8), 'layer2.0.conv2': (8, 8), 'layer2.0.downsample.0': (8, 8), 'layer2.1.conv1': (8, 8), 'layer2.1.conv2': (8, 8), 'layer3.0.conv1': (8, 8), 'layer3.0.conv2': (8, 8), 'layer3.0.downsample.0': (8, 8), 'layer3.1.conv1': (8, 8), 'layer3.1.conv2': (8, 8), 'layer4.0.conv1': (8, 8), 'layer4.0.conv2': (8, 8), 'layer4.0.downsample.0': (8, 8), 'layer4.1.conv1': (8, 8), 'layer4.1.conv2': (8, 8)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'top1': tensor([69.4841], device='cuda:0'),\n",
       "  'top5': tensor([88.9362], device='cuda:0'),\n",
       "  'loss': tensor(1.2558, device='cuda:0'),\n",
       "  'time': 81.4640884399414},\n",
       " 10.640625,\n",
       " 569.57449472)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ZeroQ Way (but cached_grad needs to be measured using KL)\n",
    "cached_grad = Ltilde2CachedGrad(ref_Ltilde_clado)\n",
    "v = MIQCP_optimize(cached_grad=cached_grad,\n",
    "                   layer_bitops=layer_bitops,\n",
    "                   layer_size=layer_size,\n",
    "                   schemes_per_layer=len(aw_scheme),\n",
    "                   bitops_bound=np.inf,size_bound=np.inf,\n",
    "                   naive=True)\n",
    "v = torch.Tensor(v.value)\n",
    "evaluate_decision(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "25b70a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution status optimal\n",
      "A solution x is\n",
      "[ 0. -0.  1.  0. -0.  1.  0. -0.  1.  0.  0.  1.  0. -0.  1.  0. -0.  1.\n",
      "  0. -0.  1.  0. -0.  1.  0. -0.  1.  0. -0.  1.  0. -0.  1.  0.  0.  1.\n",
      "  0. -0.  1.  0. -0.  1.  0.  1.  0.  0.  1. -0.  0. -0.  1.  0. -0.  1.\n",
      "  0.  1. -0.]\n",
      "evaluate_decision\n",
      " {'layer1.0.conv1': (8, 8), 'layer1.0.conv2': (8, 8), 'layer1.1.conv1': (8, 8), 'layer1.1.conv2': (8, 8), 'layer2.0.conv1': (8, 8), 'layer2.0.conv2': (8, 8), 'layer2.0.downsample.0': (8, 8), 'layer2.1.conv1': (8, 8), 'layer2.1.conv2': (8, 8), 'layer3.0.conv1': (8, 8), 'layer3.0.conv2': (8, 8), 'layer3.0.downsample.0': (8, 8), 'layer3.1.conv1': (8, 8), 'layer3.1.conv2': (8, 8), 'layer4.0.conv1': (8, 4), 'layer4.0.conv2': (8, 4), 'layer4.0.downsample.0': (8, 8), 'layer4.1.conv1': (8, 8), 'layer4.1.conv2': (8, 4)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'top1': tensor([67.9981], device='cuda:0'),\n",
       "  'top5': tensor([88.1441], device='cuda:0'),\n",
       "  'loss': tensor(1.3118, device='cuda:0'),\n",
       "  'time': 81.11278557777405},\n",
       " 7.828125,\n",
       " 523.333196288)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ZeroQ Way (but cached_grad needs to be measured using KL)\n",
    "cached_grad = Ltilde2CachedGrad(ref_Ltilde_clado)\n",
    "v = MIQCP_optimize(cached_grad=cached_grad,\n",
    "                   layer_bitops=layer_bitops,\n",
    "                   layer_size=layer_size,\n",
    "                   schemes_per_layer=len(aw_scheme),\n",
    "                   bitops_bound=np.inf,size_bound=8,\n",
    "                   naive=True)\n",
    "v = torch.Tensor(v.value)\n",
    "evaluate_decision(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4406a404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution status optimal\n",
      "A solution x is\n",
      "[ 0. -0.  1.  0. -0.  1.  0. -0.  1.  0.  0.  1.  0. -0.  1.  0. -0.  1.\n",
      "  0. -0.  1.  0. -0.  1.  0.  0.  1.  0. -0.  1.  0. -0.  1.  0. -0.  1.\n",
      "  0. -0.  1.  0.  1.  0.  0. -0.  1.  0.  1. -0.  0. -0.  1.  0.  1. -0.\n",
      "  0.  1. -0.]\n",
      "evaluate_decision\n",
      " {'layer1.0.conv1': (8, 8), 'layer1.0.conv2': (8, 8), 'layer1.1.conv1': (8, 8), 'layer1.1.conv2': (8, 8), 'layer2.0.conv1': (8, 8), 'layer2.0.conv2': (8, 8), 'layer2.0.downsample.0': (8, 8), 'layer2.1.conv1': (8, 8), 'layer2.1.conv2': (8, 8), 'layer3.0.conv1': (8, 8), 'layer3.0.conv2': (8, 8), 'layer3.0.downsample.0': (8, 8), 'layer3.1.conv1': (8, 8), 'layer3.1.conv2': (8, 4), 'layer4.0.conv1': (8, 8), 'layer4.0.conv2': (8, 4), 'layer4.0.downsample.0': (8, 8), 'layer4.1.conv1': (8, 4), 'layer4.1.conv2': (8, 4)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'top1': tensor([67.2739], device='cuda:0'),\n",
       "  'top5': tensor([87.6720], device='cuda:0'),\n",
       "  'loss': tensor(1.3446, device='cuda:0'),\n",
       "  'time': 82.00978064537048},\n",
       " 6.984375,\n",
       " 495.58847744)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ZeroQ Way (but cached_grad needs to be measured using KL)\n",
    "cached_grad = Ltilde2CachedGrad(ref_Ltilde_clado)\n",
    "v = MIQCP_optimize(cached_grad=cached_grad,\n",
    "                   layer_bitops=layer_bitops,\n",
    "                   layer_size=layer_size,\n",
    "                   schemes_per_layer=len(aw_scheme),\n",
    "                   bitops_bound=np.inf,size_bound=7.23,\n",
    "                   naive=True)\n",
    "v = torch.Tensor(v.value)\n",
    "evaluate_decision(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dcdb0f18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution status optimal\n",
      "A solution x is\n",
      "[-0. -0.  1. -0. -0.  1. -0. -0.  1. -0. -0.  1.  0.  0.  1. -0.  0.  1.\n",
      " -0. -0.  1.  0.  0.  1.  0.  0.  1. -0. -0.  1.  0. -0.  1.  0.  0.  1.\n",
      "  0.  0.  1. -0.  0.  1.  0. -0.  1. -0.  1.  0. -0.  0.  1.  0.  1.  0.\n",
      "  0.  1.  0.]\n",
      "evaluate_decision\n",
      " {'layer1.0.conv1': (8, 8), 'layer1.0.conv2': (8, 8), 'layer1.1.conv1': (8, 8), 'layer1.1.conv2': (8, 8), 'layer2.0.conv1': (8, 8), 'layer2.0.conv2': (8, 8), 'layer2.0.downsample.0': (8, 8), 'layer2.1.conv1': (8, 8), 'layer2.1.conv2': (8, 8), 'layer3.0.conv1': (8, 8), 'layer3.0.conv2': (8, 8), 'layer3.0.downsample.0': (8, 8), 'layer3.1.conv1': (8, 8), 'layer3.1.conv2': (8, 8), 'layer4.0.conv1': (8, 8), 'layer4.0.conv2': (8, 4), 'layer4.0.downsample.0': (8, 8), 'layer4.1.conv1': (8, 4), 'layer4.1.conv2': (8, 4)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'top1': tensor([67.5640], device='cuda:0'),\n",
       "  'top5': tensor([87.9401], device='cuda:0'),\n",
       "  'loss': tensor(1.3294, device='cuda:0'),\n",
       "  'time': 81.02575325965881},\n",
       " 7.265625,\n",
       " 514.084755968)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CLADO Way\n",
    "cached_grad = Ltilde2CachedGrad(ref_Ltilde_clado)\n",
    "v = MIQCP_optimize(cached_grad=cached_grad,\n",
    "                   layer_bitops=layer_bitops,\n",
    "                   layer_size=layer_size,\n",
    "                   schemes_per_layer=len(aw_scheme),\n",
    "                   bitops_bound=np.inf,size_bound=8,\n",
    "                   naive=False,PSD=True)\n",
    "v = torch.Tensor(v.value)\n",
    "evaluate_decision(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7abbea83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution status optimal\n",
      "A solution x is\n",
      "[0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 1.]\n",
      "evaluate_decision\n",
      " {'layer1.0.conv1': (8, 8), 'layer1.0.conv2': (8, 8), 'layer1.1.conv1': (8, 8), 'layer1.1.conv2': (8, 8), 'layer2.0.conv1': (8, 8), 'layer2.0.conv2': (8, 8), 'layer2.0.downsample.0': (8, 8), 'layer2.1.conv1': (8, 8), 'layer2.1.conv2': (8, 8), 'layer3.0.conv1': (8, 8), 'layer3.0.conv2': (8, 8), 'layer3.0.downsample.0': (8, 8), 'layer3.1.conv1': (8, 8), 'layer3.1.conv2': (8, 8), 'layer4.0.conv1': (8, 8), 'layer4.0.conv2': (8, 8), 'layer4.0.downsample.0': (8, 8), 'layer4.1.conv1': (8, 8), 'layer4.1.conv2': (8, 8)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'top1': tensor([69.4841], device='cuda:0'),\n",
       "  'top5': tensor([88.9362], device='cuda:0'),\n",
       "  'loss': tensor(1.2558, device='cuda:0'),\n",
       "  'time': 81.49979090690613},\n",
       " 10.640625,\n",
       " 569.57449472)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CLADO Way\n",
    "cached_grad = Ltilde2CachedGrad(ref_Ltilde_clado)\n",
    "v = MIQCP_optimize(cached_grad=cached_grad,\n",
    "                   layer_bitops=layer_bitops,\n",
    "                   layer_size=layer_size,\n",
    "                   schemes_per_layer=len(aw_scheme),\n",
    "                   bitops_bound=np.inf,size_bound=np.inf,\n",
    "                   naive=False,PSD=True)\n",
    "v = torch.Tensor(v.value)\n",
    "evaluate_decision(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3dded0",
   "metadata": {},
   "source": [
    "## Variance Study "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8348fdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 batches sid 0\n",
      "clado with size bound 5.0\n",
      "Solution status optimal\n",
      "A solution x is\n",
      "[-0.  0.  1. -0. -0.  1. -0.  0.  1. -0.  1.  0.  1. -0.  0. -0. -0.  1.\n",
      " -0. -0.  1.  1. -0.  0.  0.  0.  1.  1. -0.  0.  1.  0. -0. -0. -0.  1.\n",
      "  1. -0.  0. -0.  1.  0.  1.  0. -0. -0. -0.  1. -0. -0.  1.  1.  0. -0.\n",
      "  1.  0. -0.]\n",
      "evaluate_decision\n",
      " {'layer1.0.conv1': (8, 8), 'layer1.0.conv2': (8, 8), 'layer1.1.conv1': (8, 8), 'layer1.1.conv2': (8, 4), 'layer2.0.conv1': (8, 2), 'layer2.0.conv2': (8, 8), 'layer2.0.downsample.0': (8, 8), 'layer2.1.conv1': (8, 2), 'layer2.1.conv2': (8, 8), 'layer3.0.conv1': (8, 2), 'layer3.0.conv2': (8, 2), 'layer3.0.downsample.0': (8, 8), 'layer3.1.conv1': (8, 2), 'layer3.1.conv2': (8, 4), 'layer4.0.conv1': (8, 2), 'layer4.0.conv2': (8, 8), 'layer4.0.downsample.0': (8, 8), 'layer4.1.conv1': (8, 2), 'layer4.1.conv2': (8, 2)}\n",
      "naive with size bound 5.0\n",
      "Solution status optimal\n",
      "A solution x is\n",
      "[ 0. -0.  1.  0. -0.  1.  0. -0.  1.  0. -0.  1.  0. -0.  1.  0. -0.  1.\n",
      "  0. -0.  1.  0.  1.  0.  0.  1.  0.  0.  1. -0.  0.  1. -0.  0.  1.  0.\n",
      "  0.  1. -0.  0.  1. -0.  0.  1. -0.  1. -0. -0.  0. -0.  1.  0.  1. -0.\n",
      "  0.  1. -0.]\n",
      "evaluate_decision\n",
      " {'layer1.0.conv1': (8, 8), 'layer1.0.conv2': (8, 8), 'layer1.1.conv1': (8, 8), 'layer1.1.conv2': (8, 8), 'layer2.0.conv1': (8, 8), 'layer2.0.conv2': (8, 8), 'layer2.0.downsample.0': (8, 8), 'layer2.1.conv1': (8, 4), 'layer2.1.conv2': (8, 4), 'layer3.0.conv1': (8, 4), 'layer3.0.conv2': (8, 4), 'layer3.0.downsample.0': (8, 4), 'layer3.1.conv1': (8, 4), 'layer3.1.conv2': (8, 4), 'layer4.0.conv1': (8, 4), 'layer4.0.conv2': (8, 2), 'layer4.0.downsample.0': (8, 8), 'layer4.1.conv1': (8, 4), 'layer4.1.conv2': (8, 4)}\n",
      "MPQCO with size bound 5.0\n",
      "Solution status optimal\n",
      "A solution x is\n",
      "[ 0.  1.  0.  0.  1. -0.  0.  1.  0.  0.  1.  0.  0.  1.  0.  0.  1. -0.\n",
      "  0.  1.  0.  0.  1.  0.  0.  1. -0.  0.  1. -0.  1. -0. -0.  1. -0. -0.\n",
      "  0.  1. -0.  1. -0. -0.  0.  1. -0.  0.  1. -0.  1. -0. -0.  0.  1. -0.\n",
      "  0.  1. -0.]\n",
      "evaluate_decision\n",
      " {'layer1.0.conv1': (8, 4), 'layer1.0.conv2': (8, 4), 'layer1.1.conv1': (8, 4), 'layer1.1.conv2': (8, 4), 'layer2.0.conv1': (8, 4), 'layer2.0.conv2': (8, 4), 'layer2.0.downsample.0': (8, 4), 'layer2.1.conv1': (8, 4), 'layer2.1.conv2': (8, 4), 'layer3.0.conv1': (8, 4), 'layer3.0.conv2': (8, 2), 'layer3.0.downsample.0': (8, 2), 'layer3.1.conv1': (8, 4), 'layer3.1.conv2': (8, 2), 'layer4.0.conv1': (8, 4), 'layer4.0.conv2': (8, 4), 'layer4.0.downsample.0': (8, 2), 'layer4.1.conv1': (8, 4), 'layer4.1.conv2': (8, 4)}\n",
      "clado with size bound 5.5\n",
      "Solution status optimal\n",
      "A solution x is\n",
      "[-0. -0.  1. -0. -0.  1. -0.  0.  1. -0.  0.  1. -0.  0.  1. -0.  1.  0.\n",
      " -0. -0.  1. -0. -0.  1. -0.  1.  0. -0.  1.  0. -0.  1. -0. -0.  1.  0.\n",
      " -0.  1. -0.  0.  1. -0. -0.  1. -0. -0.  1. -0. -0.  1. -0. -0.  1. -0.\n",
      " -0.  1. -0.]\n",
      "evaluate_decision\n",
      " {'layer1.0.conv1': (8, 8), 'layer1.0.conv2': (8, 8), 'layer1.1.conv1': (8, 8), 'layer1.1.conv2': (8, 8), 'layer2.0.conv1': (8, 8), 'layer2.0.conv2': (8, 4), 'layer2.0.downsample.0': (8, 8), 'layer2.1.conv1': (8, 8), 'layer2.1.conv2': (8, 4), 'layer3.0.conv1': (8, 4), 'layer3.0.conv2': (8, 4), 'layer3.0.downsample.0': (8, 4), 'layer3.1.conv1': (8, 4), 'layer3.1.conv2': (8, 4), 'layer4.0.conv1': (8, 4), 'layer4.0.conv2': (8, 4), 'layer4.0.downsample.0': (8, 4), 'layer4.1.conv1': (8, 4), 'layer4.1.conv2': (8, 4)}\n",
      "naive with size bound 5.5\n",
      "Solution status optimal\n",
      "A solution x is\n",
      "[0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0.\n",
      " 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 1. 0. 0. 1. 0. 0. 1. 0.]\n",
      "evaluate_decision\n",
      " {'layer1.0.conv1': (8, 8), 'layer1.0.conv2': (8, 8), 'layer1.1.conv1': (8, 8), 'layer1.1.conv2': (8, 8), 'layer2.0.conv1': (8, 8), 'layer2.0.conv2': (8, 8), 'layer2.0.downsample.0': (8, 8), 'layer2.1.conv1': (8, 4), 'layer2.1.conv2': (8, 4), 'layer3.0.conv1': (8, 4), 'layer3.0.conv2': (8, 4), 'layer3.0.downsample.0': (8, 4), 'layer3.1.conv1': (8, 4), 'layer3.1.conv2': (8, 4), 'layer4.0.conv1': (8, 4), 'layer4.0.conv2': (8, 4), 'layer4.0.downsample.0': (8, 4), 'layer4.1.conv1': (8, 4), 'layer4.1.conv2': (8, 4)}\n",
      "MPQCO with size bound 5.5\n",
      "Solution status optimal\n",
      "A solution x is\n",
      "[ 0. -0.  1.  0. -0.  1.  0. -0.  1.  0. -0.  1.  0. -0.  1.  0. -0.  1.\n",
      "  0. -0.  1.  0.  1.  0.  0.  1.  0.  0.  1.  0.  0.  1.  0.  0.  1.  0.\n",
      "  0.  1.  0.  0.  1.  0.  0.  1.  0.  0.  1.  0.  0.  1.  0.  0.  1.  0.\n",
      "  0.  1.  0.]\n",
      "evaluate_decision\n",
      " {'layer1.0.conv1': (8, 8), 'layer1.0.conv2': (8, 8), 'layer1.1.conv1': (8, 8), 'layer1.1.conv2': (8, 8), 'layer2.0.conv1': (8, 8), 'layer2.0.conv2': (8, 8), 'layer2.0.downsample.0': (8, 8), 'layer2.1.conv1': (8, 4), 'layer2.1.conv2': (8, 4), 'layer3.0.conv1': (8, 4), 'layer3.0.conv2': (8, 4), 'layer3.0.downsample.0': (8, 4), 'layer3.1.conv1': (8, 4), 'layer3.1.conv2': (8, 4), 'layer4.0.conv1': (8, 4), 'layer4.0.conv2': (8, 4), 'layer4.0.downsample.0': (8, 4), 'layer4.1.conv1': (8, 4), 'layer4.1.conv2': (8, 4)}\n",
      "clado with size bound 6.0\n",
      "Solution status optimal\n",
      "A solution x is\n",
      "[-0. -0.  1. -0. -0.  1. -0. -0.  1. -0.  1.  0. -0. -0.  1. -0. -0.  1.\n",
      " -0. -0.  1. -0. -0.  1. -0.  1.  0. -0. -0.  1. -0. -0.  1. -0. -0.  1.\n",
      " -0.  1.  0. -0.  1.  0. -0.  1. -0. -0.  1. -0. -0.  1.  0. -0.  1. -0.\n",
      " -0.  1. -0.]\n",
      "evaluate_decision\n",
      " {'layer1.0.conv1': (8, 8), 'layer1.0.conv2': (8, 8), 'layer1.1.conv1': (8, 8), 'layer1.1.conv2': (8, 4), 'layer2.0.conv1': (8, 8), 'layer2.0.conv2': (8, 8), 'layer2.0.downsample.0': (8, 8), 'layer2.1.conv1': (8, 8), 'layer2.1.conv2': (8, 4), 'layer3.0.conv1': (8, 8), 'layer3.0.conv2': (8, 8), 'layer3.0.downsample.0': (8, 8), 'layer3.1.conv1': (8, 4), 'layer3.1.conv2': (8, 4), 'layer4.0.conv1': (8, 4), 'layer4.0.conv2': (8, 4), 'layer4.0.downsample.0': (8, 4), 'layer4.1.conv1': (8, 4), 'layer4.1.conv2': (8, 4)}\n",
      "naive with size bound 6.0\n",
      "Solution status optimal\n",
      "A solution x is\n",
      "[ 0. -0.  1.  0. -0.  1.  0. -0.  1.  0. -0.  1.  0. -0.  1.  0. -0.  1.\n",
      "  0. -0.  1.  0. -0.  1.  0. -0.  1.  0.  1.  0.  0.  1.  0.  0. -0.  1.\n",
      "  0.  0.  1.  0.  1.  0.  0.  1.  0.  0.  1.  0.  0. -0.  1.  0.  1.  0.\n",
      "  0.  1.  0.]\n",
      "evaluate_decision\n",
      " {'layer1.0.conv1': (8, 8), 'layer1.0.conv2': (8, 8), 'layer1.1.conv1': (8, 8), 'layer1.1.conv2': (8, 8), 'layer2.0.conv1': (8, 8), 'layer2.0.conv2': (8, 8), 'layer2.0.downsample.0': (8, 8), 'layer2.1.conv1': (8, 8), 'layer2.1.conv2': (8, 8), 'layer3.0.conv1': (8, 4), 'layer3.0.conv2': (8, 4), 'layer3.0.downsample.0': (8, 8), 'layer3.1.conv1': (8, 8), 'layer3.1.conv2': (8, 4), 'layer4.0.conv1': (8, 4), 'layer4.0.conv2': (8, 4), 'layer4.0.downsample.0': (8, 8), 'layer4.1.conv1': (8, 4), 'layer4.1.conv2': (8, 4)}\n",
      "MPQCO with size bound 6.0\n",
      "Solution status optimal\n",
      "A solution x is\n",
      "[ 0. -0.  1.  0. -0.  1.  0. -0.  1.  0. -0.  1.  0. -0.  1.  0. -0.  1.\n",
      "  0. -0.  1.  0. -0.  1.  0. -0.  1.  0. -0.  1.  0.  1. -0.  0.  0.  1.\n",
      "  0.  1.  0.  0.  1.  0.  0.  1. -0.  0.  1. -0.  0. -0.  1.  0.  1. -0.\n",
      "  0.  1. -0.]\n",
      "evaluate_decision\n",
      " {'layer1.0.conv1': (8, 8), 'layer1.0.conv2': (8, 8), 'layer1.1.conv1': (8, 8), 'layer1.1.conv2': (8, 8), 'layer2.0.conv1': (8, 8), 'layer2.0.conv2': (8, 8), 'layer2.0.downsample.0': (8, 8), 'layer2.1.conv1': (8, 8), 'layer2.1.conv2': (8, 8), 'layer3.0.conv1': (8, 8), 'layer3.0.conv2': (8, 4), 'layer3.0.downsample.0': (8, 8), 'layer3.1.conv1': (8, 4), 'layer3.1.conv2': (8, 4), 'layer4.0.conv1': (8, 4), 'layer4.0.conv2': (8, 4), 'layer4.0.downsample.0': (8, 8), 'layer4.1.conv1': (8, 4), 'layer4.1.conv2': (8, 4)}\n",
      "clado with size bound 6.5\n",
      "Solution status optimal\n",
      "A solution x is\n",
      "[-0. -0.  1. -0.  0.  1.  0.  0.  1. -0.  0.  1. -0.  0.  1. -0.  0.  1.\n",
      " -0. -0.  1.  0. -0.  1. -0.  0.  1. -0. -0.  1. -0.  1.  0. -0.  0.  1.\n",
      " -0. -0.  1. -0. -0.  1. -0.  1.  0.  0.  1.  0. -0.  0.  1. -0.  1.  0.\n",
      " -0.  1. -0.]\n",
      "evaluate_decision\n",
      " {'layer1.0.conv1': (8, 8), 'layer1.0.conv2': (8, 8), 'layer1.1.conv1': (8, 8), 'layer1.1.conv2': (8, 8), 'layer2.0.conv1': (8, 8), 'layer2.0.conv2': (8, 8), 'layer2.0.downsample.0': (8, 8), 'layer2.1.conv1': (8, 8), 'layer2.1.conv2': (8, 8), 'layer3.0.conv1': (8, 8), 'layer3.0.conv2': (8, 4), 'layer3.0.downsample.0': (8, 8), 'layer3.1.conv1': (8, 8), 'layer3.1.conv2': (8, 8), 'layer4.0.conv1': (8, 4), 'layer4.0.conv2': (8, 4), 'layer4.0.downsample.0': (8, 8), 'layer4.1.conv1': (8, 4), 'layer4.1.conv2': (8, 4)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive with size bound 6.5\n",
      "Solution status optimal\n",
      "A solution x is\n",
      "[ 0. -0.  1.  0. -0.  1.  0. -0.  1.  0. -0.  1.  0. -0.  1.  0. -0.  1.\n",
      "  0. -0.  1.  0. -0.  1.  0. -0.  1.  0. -0.  1.  0. -0.  1.  0. -0.  1.\n",
      "  0. -0.  1.  0.  1.  0.  0.  1.  0.  0.  1.  0.  0. -0.  1.  0.  1.  0.\n",
      "  0.  1.  0.]\n",
      "evaluate_decision\n",
      " {'layer1.0.conv1': (8, 8), 'layer1.0.conv2': (8, 8), 'layer1.1.conv1': (8, 8), 'layer1.1.conv2': (8, 8), 'layer2.0.conv1': (8, 8), 'layer2.0.conv2': (8, 8), 'layer2.0.downsample.0': (8, 8), 'layer2.1.conv1': (8, 8), 'layer2.1.conv2': (8, 8), 'layer3.0.conv1': (8, 8), 'layer3.0.conv2': (8, 8), 'layer3.0.downsample.0': (8, 8), 'layer3.1.conv1': (8, 8), 'layer3.1.conv2': (8, 4), 'layer4.0.conv1': (8, 4), 'layer4.0.conv2': (8, 4), 'layer4.0.downsample.0': (8, 8), 'layer4.1.conv1': (8, 4), 'layer4.1.conv2': (8, 4)}\n",
      "MPQCO with size bound 6.5\n",
      "Solution status optimal\n",
      "A solution x is\n",
      "[0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 1. 0.]\n",
      "evaluate_decision\n",
      " {'layer1.0.conv1': (8, 8), 'layer1.0.conv2': (8, 8), 'layer1.1.conv1': (8, 8), 'layer1.1.conv2': (8, 8), 'layer2.0.conv1': (8, 8), 'layer2.0.conv2': (8, 8), 'layer2.0.downsample.0': (8, 8), 'layer2.1.conv1': (8, 8), 'layer2.1.conv2': (8, 8), 'layer3.0.conv1': (8, 8), 'layer3.0.conv2': (8, 8), 'layer3.0.downsample.0': (8, 8), 'layer3.1.conv1': (8, 8), 'layer3.1.conv2': (8, 4), 'layer4.0.conv1': (8, 4), 'layer4.0.conv2': (8, 4), 'layer4.0.downsample.0': (8, 8), 'layer4.1.conv1': (8, 4), 'layer4.1.conv2': (8, 4)}\n"
     ]
    }
   ],
   "source": [
    "for n_batch in (1,4,16,64,128,256):\n",
    "    for sid in range(5):\n",
    "        \n",
    "        print(f'{n_batch} batches sid {sid}')\n",
    "        \n",
    "        clado_perf,clado_size,clado_bitops = [],[],[]\n",
    "        mpqco_perf,mpqco_size,mpqco_bitops = [],[],[]\n",
    "        naive_perf,naive_size,naive_bitops = [],[],[]\n",
    "        \n",
    "        \n",
    "        shuffle = np.random.choice(300,n_batch,replace=False)\n",
    "        \n",
    "        clado_ltilde = batch_Ltildes_clado[shuffle].mean(axis=0)\n",
    "        mpqco_ltilde = batch_Ltildes_mpqco[shuffle].mean(axis=0)\n",
    "        \n",
    "        with open(f'Ltilde_resnet18/res_{n_batch}batches(size64)_sid{sid}.pkl','wb') as f:\n",
    "            pickle.dump({'shuffle':shuffle,\n",
    "                         'clado_perf':clado_perf,'clado_size':clado_size,'clado_bitops':clado_bitops,\n",
    "                         'naive_perf':naive_perf,'naive_size':naive_size,'naive_bitops':naive_bitops,\n",
    "                         'mpqco_perf':mpqco_perf,'mpqco_size':mpqco_size,'mpqco_bitops':mpqco_bitops},f)\n",
    "            \n",
    "        for size_bound in np.linspace(5,10,11):\n",
    "            # CLADO Way\n",
    "            print(f'clado with size bound {size_bound}')\n",
    "            cached_grad = Ltilde2CachedGrad(clado_ltilde)\n",
    "            v = MIQCP_optimize(cached_grad=cached_grad,\n",
    "                               layer_bitops=layer_bitops,\n",
    "                               layer_size=layer_size,\n",
    "                               schemes_per_layer=len(aw_scheme),\n",
    "                               bitops_bound=np.inf,size_bound=size_bound,\n",
    "                               naive=False,PSD=True)\n",
    "            v = torch.Tensor(v.value)\n",
    "            perf,size,bitops = evaluate_decision(v)\n",
    "            clado_perf.append(perf)\n",
    "            clado_size.append(size)\n",
    "            clado_bitops.append(bitops)\n",
    "            \n",
    "            # naive Way\n",
    "            print(f'naive with size bound {size_bound}')\n",
    "            cached_grad = Ltilde2CachedGrad(clado_ltilde)\n",
    "            v = MIQCP_optimize(cached_grad=cached_grad,\n",
    "                               layer_bitops=layer_bitops,\n",
    "                               layer_size=layer_size,\n",
    "                               schemes_per_layer=len(aw_scheme),\n",
    "                               bitops_bound=np.inf,size_bound=size_bound,\n",
    "                               naive=True)\n",
    "            v = torch.Tensor(v.value)\n",
    "            perf,size,bitops = evaluate_decision(v)\n",
    "            naive_perf.append(perf)\n",
    "            naive_size.append(size)\n",
    "            naive_bitops.append(bitops)\n",
    "            \n",
    "            # MPQCO Way\n",
    "            print(f'MPQCO with size bound {size_bound}')\n",
    "            v = MIQCP_optimize(cached_grad=mpqco_ltilde,\n",
    "                               layer_bitops=layer_bitops,\n",
    "                               layer_size=layer_size,\n",
    "                               schemes_per_layer=len(aw_scheme),\n",
    "                               bitops_bound=np.inf,size_bound=size_bound,\n",
    "                               naive=True)\n",
    "            v = torch.Tensor(v.value)\n",
    "            perf,size,bitops = evaluate_decision(v)\n",
    "            mpqco_perf.append(perf)\n",
    "            mpqco_size.append(size)\n",
    "            mpqco_bitops.append(bitops)\n",
    "        \n",
    "        with open(f'Ltilde_resnet18/res_{n_batch}batches(size64)_sid{sid}.pkl','wb') as f:\n",
    "            pickle.dump({'shuffle':shuffle,\n",
    "                         'clado_perf':clado_perf,'clado_size':clado_size,'clado_bitops':clado_bitops,\n",
    "                         'naive_perf':naive_perf,'naive_size':naive_size,'naive_bitops':naive_bitops,\n",
    "                         'mpqco_perf':mpqco_perf,'mpqco_size':mpqco_size,'mpqco_bitops':mpqco_bitops},f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432eacd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
