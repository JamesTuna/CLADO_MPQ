Using cache found in /homes/zdeng/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master
Files already downloaded and verified
Files already downloaded and verified
[MQBENCH] INFO: Quantize model Scheme: BackendType.Academic Mode: Eval
[MQBENCH] INFO: Weight Qconfig:
    FakeQuantize: AdaRoundFakeQuantize Params: {}
    Oberver:      MSEObserver Params: Symmetric: True / Bitwidth: 4 / Per channel: False / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Activation Qconfig:
    FakeQuantize: QDropFakeQuantize Params: {}
    Oberver:      EMAMSEObserver Params: Symmetric: False / Bitwidth: 8 / Per channel: False / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Replace module to qat module.
[MQBENCH] INFO: Set layer conv1 to 8 bit.
[MQBENCH] INFO: Set layer fc to 8 bit.
dbg node_to_quantize_output
 odict_keys([x, relu, layer1_0_relu, layer1_0_relu_1, layer1_1_relu, layer1_1_relu_1, layer1_2_relu, layer1_2_relu_1, layer1_3_relu, layer1_3_relu_1, layer1_4_relu, layer1_4_relu_1, layer1_5_relu, layer1_5_relu_1, layer1_6_relu, layer1_6_relu_1, layer1_7_relu, layer1_7_relu_1, layer1_8_relu, layer1_8_relu_1, layer2_0_relu, layer2_0_relu_1, layer2_1_relu, layer2_1_relu_1, layer2_2_relu, layer2_2_relu_1, layer2_3_relu, layer2_3_relu_1, layer2_4_relu, layer2_4_relu_1, layer2_5_relu, layer2_5_relu_1, layer2_6_relu, layer2_6_relu_1, layer2_7_relu, layer2_7_relu_1, layer2_8_relu, layer2_8_relu_1, layer3_0_relu, layer3_0_relu_1, layer3_1_relu, layer3_1_relu_1, layer3_2_relu, layer3_2_relu_1, layer3_3_relu, layer3_3_relu_1, layer3_4_relu, layer3_4_relu_1, layer3_5_relu, layer3_5_relu_1, layer3_6_relu, layer3_6_relu_1, layer3_7_relu, layer3_7_relu_1, layer3_8_relu, view])
[MQBENCH] INFO: Set x post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant x_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_4_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_5_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_6_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_7_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_8_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_4_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_5_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_6_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_7_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_8_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_4_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_5_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_6_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_7_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: Set view post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant view_post_act_fake_quantizer
[MQBENCH] INFO: Enable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: prepare block reconstruction for conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [x_post_act_fake_quantizer, conv1, bn1, relu, relu_post_act_fake_quantizer]
Traceback (most recent call last):
  File "/homes/zdeng/sum2022/QAVAT_PLUS/FeintLady.py", line 112, in <module>
    mqb_model = ptq_reconstruction(mqb_model,calib_data,ptq_rec_config)
  File "/homes/zdeng/.conda/envs/mltls/lib/python3.9/site-packages/MQBench-0.0.6-py3.9.egg/mqbench/advanced_ptq.py", line 643, in ptq_reconstruction
    _, fp32_inps = save_inp_oup_data(fp32_model, None, fp32_inp_module, cali_data, 
  File "/homes/zdeng/.conda/envs/mltls/lib/python3.9/site-packages/MQBench-0.0.6-py3.9.egg/mqbench/advanced_ptq.py", line 118, in save_inp_oup_data
    _ = model(to_device(batch, device))
  File "/homes/zdeng/.conda/envs/mltls/lib/python3.9/site-packages/torch/fx/graph_module.py", line 616, in wrapped_call
    raise e.with_traceback(None)
TypeError: conv2d() received an invalid combination of arguments - got (tuple, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:
 * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)
      didn't match because some of the arguments have invalid types: (!tuple!, !Parameter!, !NoneType!, !tuple!, !tuple!, !tuple!, int)
 * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)
      didn't match because some of the arguments have invalid types: (!tuple!, !Parameter!, !NoneType!, !tuple!, !tuple!, !tuple!, int)

