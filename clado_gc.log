Using cache found in /homes/zdeng/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master
Files already downloaded and verified
Files already downloaded and verified
fp32 model: {'mean_acc': 0.7263, 'qtl_acc': 0.7263, 'mean_loss': 1.2951814660543128, 'qtl_loss': 1.2951814660543128, 'test time': 2.2739670276641846, 'acc_list': array([0.7263]), 'loss_list': array([1.29518147])}
Prepare 2bits model using MQBench
[MQBENCH] INFO: Quantize model Scheme: BackendType.Academic Mode: Eval
[MQBENCH] INFO: Weight Qconfig:
    FakeQuantize: AdaRoundFakeQuantize Params: {}
    Oberver:      MSEObserver Params: Symmetric: True / Bitwidth: 2 / Per channel: False / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Activation Qconfig:
    FakeQuantize: QDropFakeQuantize Params: {}
    Oberver:      EMAMSEObserver Params: Symmetric: False / Bitwidth: 2 / Per channel: False / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Replace module to qat module.
[MQBENCH] INFO: Set layer conv1 to 8 bit.
[MQBENCH] INFO: Set layer fc to 8 bit.
dbg node_to_quantize_output
 odict_keys([x, relu, layer1_0_relu, layer1_0_relu_1, layer1_1_relu, layer1_1_relu_1, layer1_2_relu, layer1_2_relu_1, layer1_3_relu, layer1_3_relu_1, layer1_4_relu, layer1_4_relu_1, layer1_5_relu, layer1_5_relu_1, layer1_6_relu, layer1_6_relu_1, layer1_7_relu, layer1_7_relu_1, layer1_8_relu, layer1_8_relu_1, layer2_0_relu, layer2_0_relu_1, layer2_1_relu, layer2_1_relu_1, layer2_2_relu, layer2_2_relu_1, layer2_3_relu, layer2_3_relu_1, layer2_4_relu, layer2_4_relu_1, layer2_5_relu, layer2_5_relu_1, layer2_6_relu, layer2_6_relu_1, layer2_7_relu, layer2_7_relu_1, layer2_8_relu, layer2_8_relu_1, layer3_0_relu, layer3_0_relu_1, layer3_1_relu, layer3_1_relu_1, layer3_2_relu, layer3_2_relu_1, layer3_3_relu, layer3_3_relu_1, layer3_4_relu, layer3_4_relu_1, layer3_5_relu, layer3_5_relu_1, layer3_6_relu, layer3_6_relu_1, layer3_7_relu, layer3_7_relu_1, layer3_8_relu, view])
[MQBENCH] INFO: Set x post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant x_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_4_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_5_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_6_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_7_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_8_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_4_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_5_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_6_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_7_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_8_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_4_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_5_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_6_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_7_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: Set view post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant view_post_act_fake_quantizer
[MQBENCH] INFO: Enable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: prepare block reconstruction for conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [x_post_act_fake_quantizer, conv1, bn1, relu, relu_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, x_post_act_fake_quantizer):
    conv1 = self.conv1(x_post_act_fake_quantizer);  x_post_act_fake_quantizer = None
    bn1 = self.bn1(conv1);  conv1 = None
    relu = self.relu(bn1);  bn1 = None
    relu_post_act_fake_quantizer = self.relu_post_act_fake_quantizer(relu);  relu = None
    return relu_post_act_fake_quantizer
    
Init alpha to be FP32
[MQBENCH] INFO: learn the scale for relu_post_act_fake_quantizer
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_0_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [relu_post_act_fake_quantizer, layer1_0_conv1, layer1_0_bn1, layer1_0_relu, layer1_0_relu_post_act_fake_quantizer, layer1_0_conv2, layer1_0_bn2, add, layer1_0_relu_1, layer1_0_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, relu_post_act_fake_quantizer):
    layer1_0_conv1 = getattr(self.layer1, "0").conv1(relu_post_act_fake_quantizer)
    layer1_0_bn1 = getattr(self.layer1, "0").bn1(layer1_0_conv1);  layer1_0_conv1 = None
    layer1_0_relu = getattr(self.layer1, "0").relu(layer1_0_bn1);  layer1_0_bn1 = None
    layer1_0_relu_post_act_fake_quantizer = self.layer1_0_relu_post_act_fake_quantizer(layer1_0_relu);  layer1_0_relu = None
    layer1_0_conv2 = getattr(self.layer1, "0").conv2(layer1_0_relu_post_act_fake_quantizer);  layer1_0_relu_post_act_fake_quantizer = None
    layer1_0_bn2 = getattr(self.layer1, "0").bn2(layer1_0_conv2);  layer1_0_conv2 = None
    add = layer1_0_bn2 + relu_post_act_fake_quantizer;  layer1_0_bn2 = relu_post_act_fake_quantizer = None
    layer1_0_relu_1 = getattr(self.layer1, "0").relu_dup1(add);  add = None
    layer1_0_relu_1_post_act_fake_quantizer = self.layer1_0_relu_1_post_act_fake_quantizer(layer1_0_relu_1);  layer1_0_relu_1 = None
    return layer1_0_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_0_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_1_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_0_relu_1_post_act_fake_quantizer, layer1_1_conv1, layer1_1_bn1, layer1_1_relu, layer1_1_relu_post_act_fake_quantizer, layer1_1_conv2, layer1_1_bn2, add_1, layer1_1_relu_1, layer1_1_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_0_relu_1_post_act_fake_quantizer):
    layer1_1_conv1 = getattr(self.layer1, "1").conv1(layer1_0_relu_1_post_act_fake_quantizer)
    layer1_1_bn1 = getattr(self.layer1, "1").bn1(layer1_1_conv1);  layer1_1_conv1 = None
    layer1_1_relu = getattr(self.layer1, "1").relu(layer1_1_bn1);  layer1_1_bn1 = None
    layer1_1_relu_post_act_fake_quantizer = self.layer1_1_relu_post_act_fake_quantizer(layer1_1_relu);  layer1_1_relu = None
    layer1_1_conv2 = getattr(self.layer1, "1").conv2(layer1_1_relu_post_act_fake_quantizer);  layer1_1_relu_post_act_fake_quantizer = None
    layer1_1_bn2 = getattr(self.layer1, "1").bn2(layer1_1_conv2);  layer1_1_conv2 = None
    add_1 = layer1_1_bn2 + layer1_0_relu_1_post_act_fake_quantizer;  layer1_1_bn2 = layer1_0_relu_1_post_act_fake_quantizer = None
    layer1_1_relu_1 = getattr(self.layer1, "1").relu_dup1(add_1);  add_1 = None
    layer1_1_relu_1_post_act_fake_quantizer = self.layer1_1_relu_1_post_act_fake_quantizer(layer1_1_relu_1);  layer1_1_relu_1 = None
    return layer1_1_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_1_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_2_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_1_relu_1_post_act_fake_quantizer, layer1_2_conv1, layer1_2_bn1, layer1_2_relu, layer1_2_relu_post_act_fake_quantizer, layer1_2_conv2, layer1_2_bn2, add_2, layer1_2_relu_1, layer1_2_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_1_relu_1_post_act_fake_quantizer):
    layer1_2_conv1 = getattr(self.layer1, "2").conv1(layer1_1_relu_1_post_act_fake_quantizer)
    layer1_2_bn1 = getattr(self.layer1, "2").bn1(layer1_2_conv1);  layer1_2_conv1 = None
    layer1_2_relu = getattr(self.layer1, "2").relu(layer1_2_bn1);  layer1_2_bn1 = None
    layer1_2_relu_post_act_fake_quantizer = self.layer1_2_relu_post_act_fake_quantizer(layer1_2_relu);  layer1_2_relu = None
    layer1_2_conv2 = getattr(self.layer1, "2").conv2(layer1_2_relu_post_act_fake_quantizer);  layer1_2_relu_post_act_fake_quantizer = None
    layer1_2_bn2 = getattr(self.layer1, "2").bn2(layer1_2_conv2);  layer1_2_conv2 = None
    add_2 = layer1_2_bn2 + layer1_1_relu_1_post_act_fake_quantizer;  layer1_2_bn2 = layer1_1_relu_1_post_act_fake_quantizer = None
    layer1_2_relu_1 = getattr(self.layer1, "2").relu_dup1(add_2);  add_2 = None
    layer1_2_relu_1_post_act_fake_quantizer = self.layer1_2_relu_1_post_act_fake_quantizer(layer1_2_relu_1);  layer1_2_relu_1 = None
    return layer1_2_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_2_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_3_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_2_relu_1_post_act_fake_quantizer, layer1_3_conv1, layer1_3_bn1, layer1_3_relu, layer1_3_relu_post_act_fake_quantizer, layer1_3_conv2, layer1_3_bn2, add_3, layer1_3_relu_1, layer1_3_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_2_relu_1_post_act_fake_quantizer):
    layer1_3_conv1 = getattr(self.layer1, "3").conv1(layer1_2_relu_1_post_act_fake_quantizer)
    layer1_3_bn1 = getattr(self.layer1, "3").bn1(layer1_3_conv1);  layer1_3_conv1 = None
    layer1_3_relu = getattr(self.layer1, "3").relu(layer1_3_bn1);  layer1_3_bn1 = None
    layer1_3_relu_post_act_fake_quantizer = self.layer1_3_relu_post_act_fake_quantizer(layer1_3_relu);  layer1_3_relu = None
    layer1_3_conv2 = getattr(self.layer1, "3").conv2(layer1_3_relu_post_act_fake_quantizer);  layer1_3_relu_post_act_fake_quantizer = None
    layer1_3_bn2 = getattr(self.layer1, "3").bn2(layer1_3_conv2);  layer1_3_conv2 = None
    add_3 = layer1_3_bn2 + layer1_2_relu_1_post_act_fake_quantizer;  layer1_3_bn2 = layer1_2_relu_1_post_act_fake_quantizer = None
    layer1_3_relu_1 = getattr(self.layer1, "3").relu_dup1(add_3);  add_3 = None
    layer1_3_relu_1_post_act_fake_quantizer = self.layer1_3_relu_1_post_act_fake_quantizer(layer1_3_relu_1);  layer1_3_relu_1 = None
    return layer1_3_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_3_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_4_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_3_relu_1_post_act_fake_quantizer, layer1_4_conv1, layer1_4_bn1, layer1_4_relu, layer1_4_relu_post_act_fake_quantizer, layer1_4_conv2, layer1_4_bn2, add_4, layer1_4_relu_1, layer1_4_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_3_relu_1_post_act_fake_quantizer):
    layer1_4_conv1 = getattr(self.layer1, "4").conv1(layer1_3_relu_1_post_act_fake_quantizer)
    layer1_4_bn1 = getattr(self.layer1, "4").bn1(layer1_4_conv1);  layer1_4_conv1 = None
    layer1_4_relu = getattr(self.layer1, "4").relu(layer1_4_bn1);  layer1_4_bn1 = None
    layer1_4_relu_post_act_fake_quantizer = self.layer1_4_relu_post_act_fake_quantizer(layer1_4_relu);  layer1_4_relu = None
    layer1_4_conv2 = getattr(self.layer1, "4").conv2(layer1_4_relu_post_act_fake_quantizer);  layer1_4_relu_post_act_fake_quantizer = None
    layer1_4_bn2 = getattr(self.layer1, "4").bn2(layer1_4_conv2);  layer1_4_conv2 = None
    add_4 = layer1_4_bn2 + layer1_3_relu_1_post_act_fake_quantizer;  layer1_4_bn2 = layer1_3_relu_1_post_act_fake_quantizer = None
    layer1_4_relu_1 = getattr(self.layer1, "4").relu_dup1(add_4);  add_4 = None
    layer1_4_relu_1_post_act_fake_quantizer = self.layer1_4_relu_1_post_act_fake_quantizer(layer1_4_relu_1);  layer1_4_relu_1 = None
    return layer1_4_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_4_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_5_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_4_relu_1_post_act_fake_quantizer, layer1_5_conv1, layer1_5_bn1, layer1_5_relu, layer1_5_relu_post_act_fake_quantizer, layer1_5_conv2, layer1_5_bn2, add_5, layer1_5_relu_1, layer1_5_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_4_relu_1_post_act_fake_quantizer):
    layer1_5_conv1 = getattr(self.layer1, "5").conv1(layer1_4_relu_1_post_act_fake_quantizer)
    layer1_5_bn1 = getattr(self.layer1, "5").bn1(layer1_5_conv1);  layer1_5_conv1 = None
    layer1_5_relu = getattr(self.layer1, "5").relu(layer1_5_bn1);  layer1_5_bn1 = None
    layer1_5_relu_post_act_fake_quantizer = self.layer1_5_relu_post_act_fake_quantizer(layer1_5_relu);  layer1_5_relu = None
    layer1_5_conv2 = getattr(self.layer1, "5").conv2(layer1_5_relu_post_act_fake_quantizer);  layer1_5_relu_post_act_fake_quantizer = None
    layer1_5_bn2 = getattr(self.layer1, "5").bn2(layer1_5_conv2);  layer1_5_conv2 = None
    add_5 = layer1_5_bn2 + layer1_4_relu_1_post_act_fake_quantizer;  layer1_5_bn2 = layer1_4_relu_1_post_act_fake_quantizer = None
    layer1_5_relu_1 = getattr(self.layer1, "5").relu_dup1(add_5);  add_5 = None
    layer1_5_relu_1_post_act_fake_quantizer = self.layer1_5_relu_1_post_act_fake_quantizer(layer1_5_relu_1);  layer1_5_relu_1 = None
    return layer1_5_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_5_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_6_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_5_relu_1_post_act_fake_quantizer, layer1_6_conv1, layer1_6_bn1, layer1_6_relu, layer1_6_relu_post_act_fake_quantizer, layer1_6_conv2, layer1_6_bn2, add_6, layer1_6_relu_1, layer1_6_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_5_relu_1_post_act_fake_quantizer):
    layer1_6_conv1 = getattr(self.layer1, "6").conv1(layer1_5_relu_1_post_act_fake_quantizer)
    layer1_6_bn1 = getattr(self.layer1, "6").bn1(layer1_6_conv1);  layer1_6_conv1 = None
    layer1_6_relu = getattr(self.layer1, "6").relu(layer1_6_bn1);  layer1_6_bn1 = None
    layer1_6_relu_post_act_fake_quantizer = self.layer1_6_relu_post_act_fake_quantizer(layer1_6_relu);  layer1_6_relu = None
    layer1_6_conv2 = getattr(self.layer1, "6").conv2(layer1_6_relu_post_act_fake_quantizer);  layer1_6_relu_post_act_fake_quantizer = None
    layer1_6_bn2 = getattr(self.layer1, "6").bn2(layer1_6_conv2);  layer1_6_conv2 = None
    add_6 = layer1_6_bn2 + layer1_5_relu_1_post_act_fake_quantizer;  layer1_6_bn2 = layer1_5_relu_1_post_act_fake_quantizer = None
    layer1_6_relu_1 = getattr(self.layer1, "6").relu_dup1(add_6);  add_6 = None
    layer1_6_relu_1_post_act_fake_quantizer = self.layer1_6_relu_1_post_act_fake_quantizer(layer1_6_relu_1);  layer1_6_relu_1 = None
    return layer1_6_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_6_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_7_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_6_relu_1_post_act_fake_quantizer, layer1_7_conv1, layer1_7_bn1, layer1_7_relu, layer1_7_relu_post_act_fake_quantizer, layer1_7_conv2, layer1_7_bn2, add_7, layer1_7_relu_1, layer1_7_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_6_relu_1_post_act_fake_quantizer):
    layer1_7_conv1 = getattr(self.layer1, "7").conv1(layer1_6_relu_1_post_act_fake_quantizer)
    layer1_7_bn1 = getattr(self.layer1, "7").bn1(layer1_7_conv1);  layer1_7_conv1 = None
    layer1_7_relu = getattr(self.layer1, "7").relu(layer1_7_bn1);  layer1_7_bn1 = None
    layer1_7_relu_post_act_fake_quantizer = self.layer1_7_relu_post_act_fake_quantizer(layer1_7_relu);  layer1_7_relu = None
    layer1_7_conv2 = getattr(self.layer1, "7").conv2(layer1_7_relu_post_act_fake_quantizer);  layer1_7_relu_post_act_fake_quantizer = None
    layer1_7_bn2 = getattr(self.layer1, "7").bn2(layer1_7_conv2);  layer1_7_conv2 = None
    add_7 = layer1_7_bn2 + layer1_6_relu_1_post_act_fake_quantizer;  layer1_7_bn2 = layer1_6_relu_1_post_act_fake_quantizer = None
    layer1_7_relu_1 = getattr(self.layer1, "7").relu_dup1(add_7);  add_7 = None
    layer1_7_relu_1_post_act_fake_quantizer = self.layer1_7_relu_1_post_act_fake_quantizer(layer1_7_relu_1);  layer1_7_relu_1 = None
    return layer1_7_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_7_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_8_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_7_relu_1_post_act_fake_quantizer, layer1_8_conv1, layer1_8_bn1, layer1_8_relu, layer1_8_relu_post_act_fake_quantizer, layer1_8_conv2, layer1_8_bn2, add_8, layer1_8_relu_1, layer1_8_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_7_relu_1_post_act_fake_quantizer):
    layer1_8_conv1 = getattr(self.layer1, "8").conv1(layer1_7_relu_1_post_act_fake_quantizer)
    layer1_8_bn1 = getattr(self.layer1, "8").bn1(layer1_8_conv1);  layer1_8_conv1 = None
    layer1_8_relu = getattr(self.layer1, "8").relu(layer1_8_bn1);  layer1_8_bn1 = None
    layer1_8_relu_post_act_fake_quantizer = self.layer1_8_relu_post_act_fake_quantizer(layer1_8_relu);  layer1_8_relu = None
    layer1_8_conv2 = getattr(self.layer1, "8").conv2(layer1_8_relu_post_act_fake_quantizer);  layer1_8_relu_post_act_fake_quantizer = None
    layer1_8_bn2 = getattr(self.layer1, "8").bn2(layer1_8_conv2);  layer1_8_conv2 = None
    add_8 = layer1_8_bn2 + layer1_7_relu_1_post_act_fake_quantizer;  layer1_8_bn2 = layer1_7_relu_1_post_act_fake_quantizer = None
    layer1_8_relu_1 = getattr(self.layer1, "8").relu_dup1(add_8);  add_8 = None
    layer1_8_relu_1_post_act_fake_quantizer = self.layer1_8_relu_1_post_act_fake_quantizer(layer1_8_relu_1);  layer1_8_relu_1 = None
    return layer1_8_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_8_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_0_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_8_relu_1_post_act_fake_quantizer, layer2_0_conv1, layer2_0_bn1, layer2_0_relu, layer2_0_relu_post_act_fake_quantizer, layer2_0_conv2, layer2_0_bn2, layer2_0_downsample_0, layer2_0_downsample_1, add_9, layer2_0_relu_1, layer2_0_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_8_relu_1_post_act_fake_quantizer):
    layer2_0_conv1 = getattr(self.layer2, "0").conv1(layer1_8_relu_1_post_act_fake_quantizer)
    layer2_0_bn1 = getattr(self.layer2, "0").bn1(layer2_0_conv1);  layer2_0_conv1 = None
    layer2_0_relu = getattr(self.layer2, "0").relu(layer2_0_bn1);  layer2_0_bn1 = None
    layer2_0_relu_post_act_fake_quantizer = self.layer2_0_relu_post_act_fake_quantizer(layer2_0_relu);  layer2_0_relu = None
    layer2_0_conv2 = getattr(self.layer2, "0").conv2(layer2_0_relu_post_act_fake_quantizer);  layer2_0_relu_post_act_fake_quantizer = None
    layer2_0_bn2 = getattr(self.layer2, "0").bn2(layer2_0_conv2);  layer2_0_conv2 = None
    layer2_0_downsample_0 = getattr(getattr(self.layer2, "0").downsample, "0")(layer1_8_relu_1_post_act_fake_quantizer);  layer1_8_relu_1_post_act_fake_quantizer = None
    layer2_0_downsample_1 = getattr(getattr(self.layer2, "0").downsample, "1")(layer2_0_downsample_0);  layer2_0_downsample_0 = None
    add_9 = layer2_0_bn2 + layer2_0_downsample_1;  layer2_0_bn2 = layer2_0_downsample_1 = None
    layer2_0_relu_1 = getattr(self.layer2, "0").relu_dup1(add_9);  add_9 = None
    layer2_0_relu_1_post_act_fake_quantizer = self.layer2_0_relu_1_post_act_fake_quantizer(layer2_0_relu_1);  layer2_0_relu_1 = None
    return layer2_0_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_0_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_1_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_0_relu_1_post_act_fake_quantizer, layer2_1_conv1, layer2_1_bn1, layer2_1_relu, layer2_1_relu_post_act_fake_quantizer, layer2_1_conv2, layer2_1_bn2, add_10, layer2_1_relu_1, layer2_1_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_0_relu_1_post_act_fake_quantizer):
    layer2_1_conv1 = getattr(self.layer2, "1").conv1(layer2_0_relu_1_post_act_fake_quantizer)
    layer2_1_bn1 = getattr(self.layer2, "1").bn1(layer2_1_conv1);  layer2_1_conv1 = None
    layer2_1_relu = getattr(self.layer2, "1").relu(layer2_1_bn1);  layer2_1_bn1 = None
    layer2_1_relu_post_act_fake_quantizer = self.layer2_1_relu_post_act_fake_quantizer(layer2_1_relu);  layer2_1_relu = None
    layer2_1_conv2 = getattr(self.layer2, "1").conv2(layer2_1_relu_post_act_fake_quantizer);  layer2_1_relu_post_act_fake_quantizer = None
    layer2_1_bn2 = getattr(self.layer2, "1").bn2(layer2_1_conv2);  layer2_1_conv2 = None
    add_10 = layer2_1_bn2 + layer2_0_relu_1_post_act_fake_quantizer;  layer2_1_bn2 = layer2_0_relu_1_post_act_fake_quantizer = None
    layer2_1_relu_1 = getattr(self.layer2, "1").relu_dup1(add_10);  add_10 = None
    layer2_1_relu_1_post_act_fake_quantizer = self.layer2_1_relu_1_post_act_fake_quantizer(layer2_1_relu_1);  layer2_1_relu_1 = None
    return layer2_1_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_1_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_2_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_1_relu_1_post_act_fake_quantizer, layer2_2_conv1, layer2_2_bn1, layer2_2_relu, layer2_2_relu_post_act_fake_quantizer, layer2_2_conv2, layer2_2_bn2, add_11, layer2_2_relu_1, layer2_2_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_1_relu_1_post_act_fake_quantizer):
    layer2_2_conv1 = getattr(self.layer2, "2").conv1(layer2_1_relu_1_post_act_fake_quantizer)
    layer2_2_bn1 = getattr(self.layer2, "2").bn1(layer2_2_conv1);  layer2_2_conv1 = None
    layer2_2_relu = getattr(self.layer2, "2").relu(layer2_2_bn1);  layer2_2_bn1 = None
    layer2_2_relu_post_act_fake_quantizer = self.layer2_2_relu_post_act_fake_quantizer(layer2_2_relu);  layer2_2_relu = None
    layer2_2_conv2 = getattr(self.layer2, "2").conv2(layer2_2_relu_post_act_fake_quantizer);  layer2_2_relu_post_act_fake_quantizer = None
    layer2_2_bn2 = getattr(self.layer2, "2").bn2(layer2_2_conv2);  layer2_2_conv2 = None
    add_11 = layer2_2_bn2 + layer2_1_relu_1_post_act_fake_quantizer;  layer2_2_bn2 = layer2_1_relu_1_post_act_fake_quantizer = None
    layer2_2_relu_1 = getattr(self.layer2, "2").relu_dup1(add_11);  add_11 = None
    layer2_2_relu_1_post_act_fake_quantizer = self.layer2_2_relu_1_post_act_fake_quantizer(layer2_2_relu_1);  layer2_2_relu_1 = None
    return layer2_2_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_2_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_3_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_2_relu_1_post_act_fake_quantizer, layer2_3_conv1, layer2_3_bn1, layer2_3_relu, layer2_3_relu_post_act_fake_quantizer, layer2_3_conv2, layer2_3_bn2, add_12, layer2_3_relu_1, layer2_3_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_2_relu_1_post_act_fake_quantizer):
    layer2_3_conv1 = getattr(self.layer2, "3").conv1(layer2_2_relu_1_post_act_fake_quantizer)
    layer2_3_bn1 = getattr(self.layer2, "3").bn1(layer2_3_conv1);  layer2_3_conv1 = None
    layer2_3_relu = getattr(self.layer2, "3").relu(layer2_3_bn1);  layer2_3_bn1 = None
    layer2_3_relu_post_act_fake_quantizer = self.layer2_3_relu_post_act_fake_quantizer(layer2_3_relu);  layer2_3_relu = None
    layer2_3_conv2 = getattr(self.layer2, "3").conv2(layer2_3_relu_post_act_fake_quantizer);  layer2_3_relu_post_act_fake_quantizer = None
    layer2_3_bn2 = getattr(self.layer2, "3").bn2(layer2_3_conv2);  layer2_3_conv2 = None
    add_12 = layer2_3_bn2 + layer2_2_relu_1_post_act_fake_quantizer;  layer2_3_bn2 = layer2_2_relu_1_post_act_fake_quantizer = None
    layer2_3_relu_1 = getattr(self.layer2, "3").relu_dup1(add_12);  add_12 = None
    layer2_3_relu_1_post_act_fake_quantizer = self.layer2_3_relu_1_post_act_fake_quantizer(layer2_3_relu_1);  layer2_3_relu_1 = None
    return layer2_3_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_3_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_4_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_3_relu_1_post_act_fake_quantizer, layer2_4_conv1, layer2_4_bn1, layer2_4_relu, layer2_4_relu_post_act_fake_quantizer, layer2_4_conv2, layer2_4_bn2, add_13, layer2_4_relu_1, layer2_4_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_3_relu_1_post_act_fake_quantizer):
    layer2_4_conv1 = getattr(self.layer2, "4").conv1(layer2_3_relu_1_post_act_fake_quantizer)
    layer2_4_bn1 = getattr(self.layer2, "4").bn1(layer2_4_conv1);  layer2_4_conv1 = None
    layer2_4_relu = getattr(self.layer2, "4").relu(layer2_4_bn1);  layer2_4_bn1 = None
    layer2_4_relu_post_act_fake_quantizer = self.layer2_4_relu_post_act_fake_quantizer(layer2_4_relu);  layer2_4_relu = None
    layer2_4_conv2 = getattr(self.layer2, "4").conv2(layer2_4_relu_post_act_fake_quantizer);  layer2_4_relu_post_act_fake_quantizer = None
    layer2_4_bn2 = getattr(self.layer2, "4").bn2(layer2_4_conv2);  layer2_4_conv2 = None
    add_13 = layer2_4_bn2 + layer2_3_relu_1_post_act_fake_quantizer;  layer2_4_bn2 = layer2_3_relu_1_post_act_fake_quantizer = None
    layer2_4_relu_1 = getattr(self.layer2, "4").relu_dup1(add_13);  add_13 = None
    layer2_4_relu_1_post_act_fake_quantizer = self.layer2_4_relu_1_post_act_fake_quantizer(layer2_4_relu_1);  layer2_4_relu_1 = None
    return layer2_4_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_4_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_5_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_4_relu_1_post_act_fake_quantizer, layer2_5_conv1, layer2_5_bn1, layer2_5_relu, layer2_5_relu_post_act_fake_quantizer, layer2_5_conv2, layer2_5_bn2, add_14, layer2_5_relu_1, layer2_5_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_4_relu_1_post_act_fake_quantizer):
    layer2_5_conv1 = getattr(self.layer2, "5").conv1(layer2_4_relu_1_post_act_fake_quantizer)
    layer2_5_bn1 = getattr(self.layer2, "5").bn1(layer2_5_conv1);  layer2_5_conv1 = None
    layer2_5_relu = getattr(self.layer2, "5").relu(layer2_5_bn1);  layer2_5_bn1 = None
    layer2_5_relu_post_act_fake_quantizer = self.layer2_5_relu_post_act_fake_quantizer(layer2_5_relu);  layer2_5_relu = None
    layer2_5_conv2 = getattr(self.layer2, "5").conv2(layer2_5_relu_post_act_fake_quantizer);  layer2_5_relu_post_act_fake_quantizer = None
    layer2_5_bn2 = getattr(self.layer2, "5").bn2(layer2_5_conv2);  layer2_5_conv2 = None
    add_14 = layer2_5_bn2 + layer2_4_relu_1_post_act_fake_quantizer;  layer2_5_bn2 = layer2_4_relu_1_post_act_fake_quantizer = None
    layer2_5_relu_1 = getattr(self.layer2, "5").relu_dup1(add_14);  add_14 = None
    layer2_5_relu_1_post_act_fake_quantizer = self.layer2_5_relu_1_post_act_fake_quantizer(layer2_5_relu_1);  layer2_5_relu_1 = None
    return layer2_5_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_5_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_6_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_5_relu_1_post_act_fake_quantizer, layer2_6_conv1, layer2_6_bn1, layer2_6_relu, layer2_6_relu_post_act_fake_quantizer, layer2_6_conv2, layer2_6_bn2, add_15, layer2_6_relu_1, layer2_6_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_5_relu_1_post_act_fake_quantizer):
    layer2_6_conv1 = getattr(self.layer2, "6").conv1(layer2_5_relu_1_post_act_fake_quantizer)
    layer2_6_bn1 = getattr(self.layer2, "6").bn1(layer2_6_conv1);  layer2_6_conv1 = None
    layer2_6_relu = getattr(self.layer2, "6").relu(layer2_6_bn1);  layer2_6_bn1 = None
    layer2_6_relu_post_act_fake_quantizer = self.layer2_6_relu_post_act_fake_quantizer(layer2_6_relu);  layer2_6_relu = None
    layer2_6_conv2 = getattr(self.layer2, "6").conv2(layer2_6_relu_post_act_fake_quantizer);  layer2_6_relu_post_act_fake_quantizer = None
    layer2_6_bn2 = getattr(self.layer2, "6").bn2(layer2_6_conv2);  layer2_6_conv2 = None
    add_15 = layer2_6_bn2 + layer2_5_relu_1_post_act_fake_quantizer;  layer2_6_bn2 = layer2_5_relu_1_post_act_fake_quantizer = None
    layer2_6_relu_1 = getattr(self.layer2, "6").relu_dup1(add_15);  add_15 = None
    layer2_6_relu_1_post_act_fake_quantizer = self.layer2_6_relu_1_post_act_fake_quantizer(layer2_6_relu_1);  layer2_6_relu_1 = None
    return layer2_6_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_6_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_7_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_6_relu_1_post_act_fake_quantizer, layer2_7_conv1, layer2_7_bn1, layer2_7_relu, layer2_7_relu_post_act_fake_quantizer, layer2_7_conv2, layer2_7_bn2, add_16, layer2_7_relu_1, layer2_7_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_6_relu_1_post_act_fake_quantizer):
    layer2_7_conv1 = getattr(self.layer2, "7").conv1(layer2_6_relu_1_post_act_fake_quantizer)
    layer2_7_bn1 = getattr(self.layer2, "7").bn1(layer2_7_conv1);  layer2_7_conv1 = None
    layer2_7_relu = getattr(self.layer2, "7").relu(layer2_7_bn1);  layer2_7_bn1 = None
    layer2_7_relu_post_act_fake_quantizer = self.layer2_7_relu_post_act_fake_quantizer(layer2_7_relu);  layer2_7_relu = None
    layer2_7_conv2 = getattr(self.layer2, "7").conv2(layer2_7_relu_post_act_fake_quantizer);  layer2_7_relu_post_act_fake_quantizer = None
    layer2_7_bn2 = getattr(self.layer2, "7").bn2(layer2_7_conv2);  layer2_7_conv2 = None
    add_16 = layer2_7_bn2 + layer2_6_relu_1_post_act_fake_quantizer;  layer2_7_bn2 = layer2_6_relu_1_post_act_fake_quantizer = None
    layer2_7_relu_1 = getattr(self.layer2, "7").relu_dup1(add_16);  add_16 = None
    layer2_7_relu_1_post_act_fake_quantizer = self.layer2_7_relu_1_post_act_fake_quantizer(layer2_7_relu_1);  layer2_7_relu_1 = None
    return layer2_7_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_7_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_8_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_7_relu_1_post_act_fake_quantizer, layer2_8_conv1, layer2_8_bn1, layer2_8_relu, layer2_8_relu_post_act_fake_quantizer, layer2_8_conv2, layer2_8_bn2, add_17, layer2_8_relu_1, layer2_8_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_7_relu_1_post_act_fake_quantizer):
    layer2_8_conv1 = getattr(self.layer2, "8").conv1(layer2_7_relu_1_post_act_fake_quantizer)
    layer2_8_bn1 = getattr(self.layer2, "8").bn1(layer2_8_conv1);  layer2_8_conv1 = None
    layer2_8_relu = getattr(self.layer2, "8").relu(layer2_8_bn1);  layer2_8_bn1 = None
    layer2_8_relu_post_act_fake_quantizer = self.layer2_8_relu_post_act_fake_quantizer(layer2_8_relu);  layer2_8_relu = None
    layer2_8_conv2 = getattr(self.layer2, "8").conv2(layer2_8_relu_post_act_fake_quantizer);  layer2_8_relu_post_act_fake_quantizer = None
    layer2_8_bn2 = getattr(self.layer2, "8").bn2(layer2_8_conv2);  layer2_8_conv2 = None
    add_17 = layer2_8_bn2 + layer2_7_relu_1_post_act_fake_quantizer;  layer2_8_bn2 = layer2_7_relu_1_post_act_fake_quantizer = None
    layer2_8_relu_1 = getattr(self.layer2, "8").relu_dup1(add_17);  add_17 = None
    layer2_8_relu_1_post_act_fake_quantizer = self.layer2_8_relu_1_post_act_fake_quantizer(layer2_8_relu_1);  layer2_8_relu_1 = None
    return layer2_8_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_8_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_0_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_8_relu_1_post_act_fake_quantizer, layer3_0_conv1, layer3_0_bn1, layer3_0_relu, layer3_0_relu_post_act_fake_quantizer, layer3_0_conv2, layer3_0_bn2, layer3_0_downsample_0, layer3_0_downsample_1, add_18, layer3_0_relu_1, layer3_0_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_8_relu_1_post_act_fake_quantizer):
    layer3_0_conv1 = getattr(self.layer3, "0").conv1(layer2_8_relu_1_post_act_fake_quantizer)
    layer3_0_bn1 = getattr(self.layer3, "0").bn1(layer3_0_conv1);  layer3_0_conv1 = None
    layer3_0_relu = getattr(self.layer3, "0").relu(layer3_0_bn1);  layer3_0_bn1 = None
    layer3_0_relu_post_act_fake_quantizer = self.layer3_0_relu_post_act_fake_quantizer(layer3_0_relu);  layer3_0_relu = None
    layer3_0_conv2 = getattr(self.layer3, "0").conv2(layer3_0_relu_post_act_fake_quantizer);  layer3_0_relu_post_act_fake_quantizer = None
    layer3_0_bn2 = getattr(self.layer3, "0").bn2(layer3_0_conv2);  layer3_0_conv2 = None
    layer3_0_downsample_0 = getattr(getattr(self.layer3, "0").downsample, "0")(layer2_8_relu_1_post_act_fake_quantizer);  layer2_8_relu_1_post_act_fake_quantizer = None
    layer3_0_downsample_1 = getattr(getattr(self.layer3, "0").downsample, "1")(layer3_0_downsample_0);  layer3_0_downsample_0 = None
    add_18 = layer3_0_bn2 + layer3_0_downsample_1;  layer3_0_bn2 = layer3_0_downsample_1 = None
    layer3_0_relu_1 = getattr(self.layer3, "0").relu_dup1(add_18);  add_18 = None
    layer3_0_relu_1_post_act_fake_quantizer = self.layer3_0_relu_1_post_act_fake_quantizer(layer3_0_relu_1);  layer3_0_relu_1 = None
    return layer3_0_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_0_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_1_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_0_relu_1_post_act_fake_quantizer, layer3_1_conv1, layer3_1_bn1, layer3_1_relu, layer3_1_relu_post_act_fake_quantizer, layer3_1_conv2, layer3_1_bn2, add_19, layer3_1_relu_1, layer3_1_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_0_relu_1_post_act_fake_quantizer):
    layer3_1_conv1 = getattr(self.layer3, "1").conv1(layer3_0_relu_1_post_act_fake_quantizer)
    layer3_1_bn1 = getattr(self.layer3, "1").bn1(layer3_1_conv1);  layer3_1_conv1 = None
    layer3_1_relu = getattr(self.layer3, "1").relu(layer3_1_bn1);  layer3_1_bn1 = None
    layer3_1_relu_post_act_fake_quantizer = self.layer3_1_relu_post_act_fake_quantizer(layer3_1_relu);  layer3_1_relu = None
    layer3_1_conv2 = getattr(self.layer3, "1").conv2(layer3_1_relu_post_act_fake_quantizer);  layer3_1_relu_post_act_fake_quantizer = None
    layer3_1_bn2 = getattr(self.layer3, "1").bn2(layer3_1_conv2);  layer3_1_conv2 = None
    add_19 = layer3_1_bn2 + layer3_0_relu_1_post_act_fake_quantizer;  layer3_1_bn2 = layer3_0_relu_1_post_act_fake_quantizer = None
    layer3_1_relu_1 = getattr(self.layer3, "1").relu_dup1(add_19);  add_19 = None
    layer3_1_relu_1_post_act_fake_quantizer = self.layer3_1_relu_1_post_act_fake_quantizer(layer3_1_relu_1);  layer3_1_relu_1 = None
    return layer3_1_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_1_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_2_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_1_relu_1_post_act_fake_quantizer, layer3_2_conv1, layer3_2_bn1, layer3_2_relu, layer3_2_relu_post_act_fake_quantizer, layer3_2_conv2, layer3_2_bn2, add_20, layer3_2_relu_1, layer3_2_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_1_relu_1_post_act_fake_quantizer):
    layer3_2_conv1 = getattr(self.layer3, "2").conv1(layer3_1_relu_1_post_act_fake_quantizer)
    layer3_2_bn1 = getattr(self.layer3, "2").bn1(layer3_2_conv1);  layer3_2_conv1 = None
    layer3_2_relu = getattr(self.layer3, "2").relu(layer3_2_bn1);  layer3_2_bn1 = None
    layer3_2_relu_post_act_fake_quantizer = self.layer3_2_relu_post_act_fake_quantizer(layer3_2_relu);  layer3_2_relu = None
    layer3_2_conv2 = getattr(self.layer3, "2").conv2(layer3_2_relu_post_act_fake_quantizer);  layer3_2_relu_post_act_fake_quantizer = None
    layer3_2_bn2 = getattr(self.layer3, "2").bn2(layer3_2_conv2);  layer3_2_conv2 = None
    add_20 = layer3_2_bn2 + layer3_1_relu_1_post_act_fake_quantizer;  layer3_2_bn2 = layer3_1_relu_1_post_act_fake_quantizer = None
    layer3_2_relu_1 = getattr(self.layer3, "2").relu_dup1(add_20);  add_20 = None
    layer3_2_relu_1_post_act_fake_quantizer = self.layer3_2_relu_1_post_act_fake_quantizer(layer3_2_relu_1);  layer3_2_relu_1 = None
    return layer3_2_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_2_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_3_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_2_relu_1_post_act_fake_quantizer, layer3_3_conv1, layer3_3_bn1, layer3_3_relu, layer3_3_relu_post_act_fake_quantizer, layer3_3_conv2, layer3_3_bn2, add_21, layer3_3_relu_1, layer3_3_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_2_relu_1_post_act_fake_quantizer):
    layer3_3_conv1 = getattr(self.layer3, "3").conv1(layer3_2_relu_1_post_act_fake_quantizer)
    layer3_3_bn1 = getattr(self.layer3, "3").bn1(layer3_3_conv1);  layer3_3_conv1 = None
    layer3_3_relu = getattr(self.layer3, "3").relu(layer3_3_bn1);  layer3_3_bn1 = None
    layer3_3_relu_post_act_fake_quantizer = self.layer3_3_relu_post_act_fake_quantizer(layer3_3_relu);  layer3_3_relu = None
    layer3_3_conv2 = getattr(self.layer3, "3").conv2(layer3_3_relu_post_act_fake_quantizer);  layer3_3_relu_post_act_fake_quantizer = None
    layer3_3_bn2 = getattr(self.layer3, "3").bn2(layer3_3_conv2);  layer3_3_conv2 = None
    add_21 = layer3_3_bn2 + layer3_2_relu_1_post_act_fake_quantizer;  layer3_3_bn2 = layer3_2_relu_1_post_act_fake_quantizer = None
    layer3_3_relu_1 = getattr(self.layer3, "3").relu_dup1(add_21);  add_21 = None
    layer3_3_relu_1_post_act_fake_quantizer = self.layer3_3_relu_1_post_act_fake_quantizer(layer3_3_relu_1);  layer3_3_relu_1 = None
    return layer3_3_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_3_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_4_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_3_relu_1_post_act_fake_quantizer, layer3_4_conv1, layer3_4_bn1, layer3_4_relu, layer3_4_relu_post_act_fake_quantizer, layer3_4_conv2, layer3_4_bn2, add_22, layer3_4_relu_1, layer3_4_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_3_relu_1_post_act_fake_quantizer):
    layer3_4_conv1 = getattr(self.layer3, "4").conv1(layer3_3_relu_1_post_act_fake_quantizer)
    layer3_4_bn1 = getattr(self.layer3, "4").bn1(layer3_4_conv1);  layer3_4_conv1 = None
    layer3_4_relu = getattr(self.layer3, "4").relu(layer3_4_bn1);  layer3_4_bn1 = None
    layer3_4_relu_post_act_fake_quantizer = self.layer3_4_relu_post_act_fake_quantizer(layer3_4_relu);  layer3_4_relu = None
    layer3_4_conv2 = getattr(self.layer3, "4").conv2(layer3_4_relu_post_act_fake_quantizer);  layer3_4_relu_post_act_fake_quantizer = None
    layer3_4_bn2 = getattr(self.layer3, "4").bn2(layer3_4_conv2);  layer3_4_conv2 = None
    add_22 = layer3_4_bn2 + layer3_3_relu_1_post_act_fake_quantizer;  layer3_4_bn2 = layer3_3_relu_1_post_act_fake_quantizer = None
    layer3_4_relu_1 = getattr(self.layer3, "4").relu_dup1(add_22);  add_22 = None
    layer3_4_relu_1_post_act_fake_quantizer = self.layer3_4_relu_1_post_act_fake_quantizer(layer3_4_relu_1);  layer3_4_relu_1 = None
    return layer3_4_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_4_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_5_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_4_relu_1_post_act_fake_quantizer, layer3_5_conv1, layer3_5_bn1, layer3_5_relu, layer3_5_relu_post_act_fake_quantizer, layer3_5_conv2, layer3_5_bn2, add_23, layer3_5_relu_1, layer3_5_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_4_relu_1_post_act_fake_quantizer):
    layer3_5_conv1 = getattr(self.layer3, "5").conv1(layer3_4_relu_1_post_act_fake_quantizer)
    layer3_5_bn1 = getattr(self.layer3, "5").bn1(layer3_5_conv1);  layer3_5_conv1 = None
    layer3_5_relu = getattr(self.layer3, "5").relu(layer3_5_bn1);  layer3_5_bn1 = None
    layer3_5_relu_post_act_fake_quantizer = self.layer3_5_relu_post_act_fake_quantizer(layer3_5_relu);  layer3_5_relu = None
    layer3_5_conv2 = getattr(self.layer3, "5").conv2(layer3_5_relu_post_act_fake_quantizer);  layer3_5_relu_post_act_fake_quantizer = None
    layer3_5_bn2 = getattr(self.layer3, "5").bn2(layer3_5_conv2);  layer3_5_conv2 = None
    add_23 = layer3_5_bn2 + layer3_4_relu_1_post_act_fake_quantizer;  layer3_5_bn2 = layer3_4_relu_1_post_act_fake_quantizer = None
    layer3_5_relu_1 = getattr(self.layer3, "5").relu_dup1(add_23);  add_23 = None
    layer3_5_relu_1_post_act_fake_quantizer = self.layer3_5_relu_1_post_act_fake_quantizer(layer3_5_relu_1);  layer3_5_relu_1 = None
    return layer3_5_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_5_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_6_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_5_relu_1_post_act_fake_quantizer, layer3_6_conv1, layer3_6_bn1, layer3_6_relu, layer3_6_relu_post_act_fake_quantizer, layer3_6_conv2, layer3_6_bn2, add_24, layer3_6_relu_1, layer3_6_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_5_relu_1_post_act_fake_quantizer):
    layer3_6_conv1 = getattr(self.layer3, "6").conv1(layer3_5_relu_1_post_act_fake_quantizer)
    layer3_6_bn1 = getattr(self.layer3, "6").bn1(layer3_6_conv1);  layer3_6_conv1 = None
    layer3_6_relu = getattr(self.layer3, "6").relu(layer3_6_bn1);  layer3_6_bn1 = None
    layer3_6_relu_post_act_fake_quantizer = self.layer3_6_relu_post_act_fake_quantizer(layer3_6_relu);  layer3_6_relu = None
    layer3_6_conv2 = getattr(self.layer3, "6").conv2(layer3_6_relu_post_act_fake_quantizer);  layer3_6_relu_post_act_fake_quantizer = None
    layer3_6_bn2 = getattr(self.layer3, "6").bn2(layer3_6_conv2);  layer3_6_conv2 = None
    add_24 = layer3_6_bn2 + layer3_5_relu_1_post_act_fake_quantizer;  layer3_6_bn2 = layer3_5_relu_1_post_act_fake_quantizer = None
    layer3_6_relu_1 = getattr(self.layer3, "6").relu_dup1(add_24);  add_24 = None
    layer3_6_relu_1_post_act_fake_quantizer = self.layer3_6_relu_1_post_act_fake_quantizer(layer3_6_relu_1);  layer3_6_relu_1 = None
    return layer3_6_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_6_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_7_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_6_relu_1_post_act_fake_quantizer, layer3_7_conv1, layer3_7_bn1, layer3_7_relu, layer3_7_relu_post_act_fake_quantizer, layer3_7_conv2, layer3_7_bn2, add_25, layer3_7_relu_1, layer3_7_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_6_relu_1_post_act_fake_quantizer):
    layer3_7_conv1 = getattr(self.layer3, "7").conv1(layer3_6_relu_1_post_act_fake_quantizer)
    layer3_7_bn1 = getattr(self.layer3, "7").bn1(layer3_7_conv1);  layer3_7_conv1 = None
    layer3_7_relu = getattr(self.layer3, "7").relu(layer3_7_bn1);  layer3_7_bn1 = None
    layer3_7_relu_post_act_fake_quantizer = self.layer3_7_relu_post_act_fake_quantizer(layer3_7_relu);  layer3_7_relu = None
    layer3_7_conv2 = getattr(self.layer3, "7").conv2(layer3_7_relu_post_act_fake_quantizer);  layer3_7_relu_post_act_fake_quantizer = None
    layer3_7_bn2 = getattr(self.layer3, "7").bn2(layer3_7_conv2);  layer3_7_conv2 = None
    add_25 = layer3_7_bn2 + layer3_6_relu_1_post_act_fake_quantizer;  layer3_7_bn2 = layer3_6_relu_1_post_act_fake_quantizer = None
    layer3_7_relu_1 = getattr(self.layer3, "7").relu_dup1(add_25);  add_25 = None
    layer3_7_relu_1_post_act_fake_quantizer = self.layer3_7_relu_1_post_act_fake_quantizer(layer3_7_relu_1);  layer3_7_relu_1 = None
    return layer3_7_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_7_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_8_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_7_relu_1_post_act_fake_quantizer, layer3_8_conv1, layer3_8_bn1, layer3_8_relu, layer3_8_relu_post_act_fake_quantizer, layer3_8_conv2, layer3_8_bn2, add_26, layer3_8_relu_1, avgpool, size, view, view_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_7_relu_1_post_act_fake_quantizer):
    layer3_8_conv1 = getattr(self.layer3, "8").conv1(layer3_7_relu_1_post_act_fake_quantizer)
    layer3_8_bn1 = getattr(self.layer3, "8").bn1(layer3_8_conv1);  layer3_8_conv1 = None
    layer3_8_relu = getattr(self.layer3, "8").relu(layer3_8_bn1);  layer3_8_bn1 = None
    layer3_8_relu_post_act_fake_quantizer = self.layer3_8_relu_post_act_fake_quantizer(layer3_8_relu);  layer3_8_relu = None
    layer3_8_conv2 = getattr(self.layer3, "8").conv2(layer3_8_relu_post_act_fake_quantizer);  layer3_8_relu_post_act_fake_quantizer = None
    layer3_8_bn2 = getattr(self.layer3, "8").bn2(layer3_8_conv2);  layer3_8_conv2 = None
    add_26 = layer3_8_bn2 + layer3_7_relu_1_post_act_fake_quantizer;  layer3_8_bn2 = layer3_7_relu_1_post_act_fake_quantizer = None
    layer3_8_relu_1 = getattr(self.layer3, "8").relu_dup1(add_26);  add_26 = None
    avgpool = self.avgpool(layer3_8_relu_1);  layer3_8_relu_1 = None
    size = avgpool.size(0)
    view = avgpool.view(size, -1);  avgpool = size = None
    view_post_act_fake_quantizer = self.view_post_act_fake_quantizer(view);  view = None
    return view_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for view_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for fc
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [view_post_act_fake_quantizer, fc]
[MQBENCH] INFO: 


def forward(self, view_post_act_fake_quantizer):
    fc = self.fc(view_post_act_fake_quantizer);  view_post_act_fake_quantizer = None
    return fc
    
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node x_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_0_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_0_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_1_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_1_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_2_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_2_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_3_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_3_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_4_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_4_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_5_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_5_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_6_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_6_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_7_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_7_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_8_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_8_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_0_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.downsample.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.downsample.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_0_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_1_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_1_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_2_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_2_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_3_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_3_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_4_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_4_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_5_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_5_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_6_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_6_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_7_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_7_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_8_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_8_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_0_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.downsample.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.downsample.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_0_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_1_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_1_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_2_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_2_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_3_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_3_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_4_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_4_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_5_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_5_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_6_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_6_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_7_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_7_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_8_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node avgpool in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node view_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node fc in quant
QDROP model already saved, now loading QDROP_2bits_cifar100_resnet56.pt
Prepare 4bits model using MQBench
[MQBENCH] INFO: Quantize model Scheme: BackendType.Academic Mode: Eval
[MQBENCH] INFO: Weight Qconfig:
    FakeQuantize: AdaRoundFakeQuantize Params: {}
    Oberver:      MSEObserver Params: Symmetric: True / Bitwidth: 4 / Per channel: False / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Activation Qconfig:
    FakeQuantize: QDropFakeQuantize Params: {}
    Oberver:      EMAMSEObserver Params: Symmetric: False / Bitwidth: 4 / Per channel: False / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Replace module to qat module.
[MQBENCH] INFO: Set layer conv1 to 8 bit.
[MQBENCH] INFO: Set layer fc to 8 bit.
dbg node_to_quantize_output
 odict_keys([x, relu, layer1_0_relu, layer1_0_relu_1, layer1_1_relu, layer1_1_relu_1, layer1_2_relu, layer1_2_relu_1, layer1_3_relu, layer1_3_relu_1, layer1_4_relu, layer1_4_relu_1, layer1_5_relu, layer1_5_relu_1, layer1_6_relu, layer1_6_relu_1, layer1_7_relu, layer1_7_relu_1, layer1_8_relu, layer1_8_relu_1, layer2_0_relu, layer2_0_relu_1, layer2_1_relu, layer2_1_relu_1, layer2_2_relu, layer2_2_relu_1, layer2_3_relu, layer2_3_relu_1, layer2_4_relu, layer2_4_relu_1, layer2_5_relu, layer2_5_relu_1, layer2_6_relu, layer2_6_relu_1, layer2_7_relu, layer2_7_relu_1, layer2_8_relu, layer2_8_relu_1, layer3_0_relu, layer3_0_relu_1, layer3_1_relu, layer3_1_relu_1, layer3_2_relu, layer3_2_relu_1, layer3_3_relu, layer3_3_relu_1, layer3_4_relu, layer3_4_relu_1, layer3_5_relu, layer3_5_relu_1, layer3_6_relu, layer3_6_relu_1, layer3_7_relu, layer3_7_relu_1, layer3_8_relu, view])
[MQBENCH] INFO: Set x post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant x_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_4_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_5_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_6_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_7_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_8_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_4_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_5_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_6_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_7_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_8_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_4_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_5_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_6_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_7_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: Set view post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant view_post_act_fake_quantizer
[MQBENCH] INFO: Enable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: prepare block reconstruction for conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [x_post_act_fake_quantizer, conv1, bn1, relu, relu_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, x_post_act_fake_quantizer):
    conv1 = self.conv1(x_post_act_fake_quantizer);  x_post_act_fake_quantizer = None
    bn1 = self.bn1(conv1);  conv1 = None
    relu = self.relu(bn1);  bn1 = None
    relu_post_act_fake_quantizer = self.relu_post_act_fake_quantizer(relu);  relu = None
    return relu_post_act_fake_quantizer
    
Init alpha to be FP32
[MQBENCH] INFO: learn the scale for relu_post_act_fake_quantizer
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_0_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [relu_post_act_fake_quantizer, layer1_0_conv1, layer1_0_bn1, layer1_0_relu, layer1_0_relu_post_act_fake_quantizer, layer1_0_conv2, layer1_0_bn2, add, layer1_0_relu_1, layer1_0_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, relu_post_act_fake_quantizer):
    layer1_0_conv1 = getattr(self.layer1, "0").conv1(relu_post_act_fake_quantizer)
    layer1_0_bn1 = getattr(self.layer1, "0").bn1(layer1_0_conv1);  layer1_0_conv1 = None
    layer1_0_relu = getattr(self.layer1, "0").relu(layer1_0_bn1);  layer1_0_bn1 = None
    layer1_0_relu_post_act_fake_quantizer = self.layer1_0_relu_post_act_fake_quantizer(layer1_0_relu);  layer1_0_relu = None
    layer1_0_conv2 = getattr(self.layer1, "0").conv2(layer1_0_relu_post_act_fake_quantizer);  layer1_0_relu_post_act_fake_quantizer = None
    layer1_0_bn2 = getattr(self.layer1, "0").bn2(layer1_0_conv2);  layer1_0_conv2 = None
    add = layer1_0_bn2 + relu_post_act_fake_quantizer;  layer1_0_bn2 = relu_post_act_fake_quantizer = None
    layer1_0_relu_1 = getattr(self.layer1, "0").relu_dup1(add);  add = None
    layer1_0_relu_1_post_act_fake_quantizer = self.layer1_0_relu_1_post_act_fake_quantizer(layer1_0_relu_1);  layer1_0_relu_1 = None
    return layer1_0_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_0_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_1_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_0_relu_1_post_act_fake_quantizer, layer1_1_conv1, layer1_1_bn1, layer1_1_relu, layer1_1_relu_post_act_fake_quantizer, layer1_1_conv2, layer1_1_bn2, add_1, layer1_1_relu_1, layer1_1_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_0_relu_1_post_act_fake_quantizer):
    layer1_1_conv1 = getattr(self.layer1, "1").conv1(layer1_0_relu_1_post_act_fake_quantizer)
    layer1_1_bn1 = getattr(self.layer1, "1").bn1(layer1_1_conv1);  layer1_1_conv1 = None
    layer1_1_relu = getattr(self.layer1, "1").relu(layer1_1_bn1);  layer1_1_bn1 = None
    layer1_1_relu_post_act_fake_quantizer = self.layer1_1_relu_post_act_fake_quantizer(layer1_1_relu);  layer1_1_relu = None
    layer1_1_conv2 = getattr(self.layer1, "1").conv2(layer1_1_relu_post_act_fake_quantizer);  layer1_1_relu_post_act_fake_quantizer = None
    layer1_1_bn2 = getattr(self.layer1, "1").bn2(layer1_1_conv2);  layer1_1_conv2 = None
    add_1 = layer1_1_bn2 + layer1_0_relu_1_post_act_fake_quantizer;  layer1_1_bn2 = layer1_0_relu_1_post_act_fake_quantizer = None
    layer1_1_relu_1 = getattr(self.layer1, "1").relu_dup1(add_1);  add_1 = None
    layer1_1_relu_1_post_act_fake_quantizer = self.layer1_1_relu_1_post_act_fake_quantizer(layer1_1_relu_1);  layer1_1_relu_1 = None
    return layer1_1_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_1_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_2_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_1_relu_1_post_act_fake_quantizer, layer1_2_conv1, layer1_2_bn1, layer1_2_relu, layer1_2_relu_post_act_fake_quantizer, layer1_2_conv2, layer1_2_bn2, add_2, layer1_2_relu_1, layer1_2_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_1_relu_1_post_act_fake_quantizer):
    layer1_2_conv1 = getattr(self.layer1, "2").conv1(layer1_1_relu_1_post_act_fake_quantizer)
    layer1_2_bn1 = getattr(self.layer1, "2").bn1(layer1_2_conv1);  layer1_2_conv1 = None
    layer1_2_relu = getattr(self.layer1, "2").relu(layer1_2_bn1);  layer1_2_bn1 = None
    layer1_2_relu_post_act_fake_quantizer = self.layer1_2_relu_post_act_fake_quantizer(layer1_2_relu);  layer1_2_relu = None
    layer1_2_conv2 = getattr(self.layer1, "2").conv2(layer1_2_relu_post_act_fake_quantizer);  layer1_2_relu_post_act_fake_quantizer = None
    layer1_2_bn2 = getattr(self.layer1, "2").bn2(layer1_2_conv2);  layer1_2_conv2 = None
    add_2 = layer1_2_bn2 + layer1_1_relu_1_post_act_fake_quantizer;  layer1_2_bn2 = layer1_1_relu_1_post_act_fake_quantizer = None
    layer1_2_relu_1 = getattr(self.layer1, "2").relu_dup1(add_2);  add_2 = None
    layer1_2_relu_1_post_act_fake_quantizer = self.layer1_2_relu_1_post_act_fake_quantizer(layer1_2_relu_1);  layer1_2_relu_1 = None
    return layer1_2_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_2_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_3_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_2_relu_1_post_act_fake_quantizer, layer1_3_conv1, layer1_3_bn1, layer1_3_relu, layer1_3_relu_post_act_fake_quantizer, layer1_3_conv2, layer1_3_bn2, add_3, layer1_3_relu_1, layer1_3_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_2_relu_1_post_act_fake_quantizer):
    layer1_3_conv1 = getattr(self.layer1, "3").conv1(layer1_2_relu_1_post_act_fake_quantizer)
    layer1_3_bn1 = getattr(self.layer1, "3").bn1(layer1_3_conv1);  layer1_3_conv1 = None
    layer1_3_relu = getattr(self.layer1, "3").relu(layer1_3_bn1);  layer1_3_bn1 = None
    layer1_3_relu_post_act_fake_quantizer = self.layer1_3_relu_post_act_fake_quantizer(layer1_3_relu);  layer1_3_relu = None
    layer1_3_conv2 = getattr(self.layer1, "3").conv2(layer1_3_relu_post_act_fake_quantizer);  layer1_3_relu_post_act_fake_quantizer = None
    layer1_3_bn2 = getattr(self.layer1, "3").bn2(layer1_3_conv2);  layer1_3_conv2 = None
    add_3 = layer1_3_bn2 + layer1_2_relu_1_post_act_fake_quantizer;  layer1_3_bn2 = layer1_2_relu_1_post_act_fake_quantizer = None
    layer1_3_relu_1 = getattr(self.layer1, "3").relu_dup1(add_3);  add_3 = None
    layer1_3_relu_1_post_act_fake_quantizer = self.layer1_3_relu_1_post_act_fake_quantizer(layer1_3_relu_1);  layer1_3_relu_1 = None
    return layer1_3_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_3_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_4_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_3_relu_1_post_act_fake_quantizer, layer1_4_conv1, layer1_4_bn1, layer1_4_relu, layer1_4_relu_post_act_fake_quantizer, layer1_4_conv2, layer1_4_bn2, add_4, layer1_4_relu_1, layer1_4_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_3_relu_1_post_act_fake_quantizer):
    layer1_4_conv1 = getattr(self.layer1, "4").conv1(layer1_3_relu_1_post_act_fake_quantizer)
    layer1_4_bn1 = getattr(self.layer1, "4").bn1(layer1_4_conv1);  layer1_4_conv1 = None
    layer1_4_relu = getattr(self.layer1, "4").relu(layer1_4_bn1);  layer1_4_bn1 = None
    layer1_4_relu_post_act_fake_quantizer = self.layer1_4_relu_post_act_fake_quantizer(layer1_4_relu);  layer1_4_relu = None
    layer1_4_conv2 = getattr(self.layer1, "4").conv2(layer1_4_relu_post_act_fake_quantizer);  layer1_4_relu_post_act_fake_quantizer = None
    layer1_4_bn2 = getattr(self.layer1, "4").bn2(layer1_4_conv2);  layer1_4_conv2 = None
    add_4 = layer1_4_bn2 + layer1_3_relu_1_post_act_fake_quantizer;  layer1_4_bn2 = layer1_3_relu_1_post_act_fake_quantizer = None
    layer1_4_relu_1 = getattr(self.layer1, "4").relu_dup1(add_4);  add_4 = None
    layer1_4_relu_1_post_act_fake_quantizer = self.layer1_4_relu_1_post_act_fake_quantizer(layer1_4_relu_1);  layer1_4_relu_1 = None
    return layer1_4_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_4_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_5_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_4_relu_1_post_act_fake_quantizer, layer1_5_conv1, layer1_5_bn1, layer1_5_relu, layer1_5_relu_post_act_fake_quantizer, layer1_5_conv2, layer1_5_bn2, add_5, layer1_5_relu_1, layer1_5_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_4_relu_1_post_act_fake_quantizer):
    layer1_5_conv1 = getattr(self.layer1, "5").conv1(layer1_4_relu_1_post_act_fake_quantizer)
    layer1_5_bn1 = getattr(self.layer1, "5").bn1(layer1_5_conv1);  layer1_5_conv1 = None
    layer1_5_relu = getattr(self.layer1, "5").relu(layer1_5_bn1);  layer1_5_bn1 = None
    layer1_5_relu_post_act_fake_quantizer = self.layer1_5_relu_post_act_fake_quantizer(layer1_5_relu);  layer1_5_relu = None
    layer1_5_conv2 = getattr(self.layer1, "5").conv2(layer1_5_relu_post_act_fake_quantizer);  layer1_5_relu_post_act_fake_quantizer = None
    layer1_5_bn2 = getattr(self.layer1, "5").bn2(layer1_5_conv2);  layer1_5_conv2 = None
    add_5 = layer1_5_bn2 + layer1_4_relu_1_post_act_fake_quantizer;  layer1_5_bn2 = layer1_4_relu_1_post_act_fake_quantizer = None
    layer1_5_relu_1 = getattr(self.layer1, "5").relu_dup1(add_5);  add_5 = None
    layer1_5_relu_1_post_act_fake_quantizer = self.layer1_5_relu_1_post_act_fake_quantizer(layer1_5_relu_1);  layer1_5_relu_1 = None
    return layer1_5_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_5_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_6_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_5_relu_1_post_act_fake_quantizer, layer1_6_conv1, layer1_6_bn1, layer1_6_relu, layer1_6_relu_post_act_fake_quantizer, layer1_6_conv2, layer1_6_bn2, add_6, layer1_6_relu_1, layer1_6_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_5_relu_1_post_act_fake_quantizer):
    layer1_6_conv1 = getattr(self.layer1, "6").conv1(layer1_5_relu_1_post_act_fake_quantizer)
    layer1_6_bn1 = getattr(self.layer1, "6").bn1(layer1_6_conv1);  layer1_6_conv1 = None
    layer1_6_relu = getattr(self.layer1, "6").relu(layer1_6_bn1);  layer1_6_bn1 = None
    layer1_6_relu_post_act_fake_quantizer = self.layer1_6_relu_post_act_fake_quantizer(layer1_6_relu);  layer1_6_relu = None
    layer1_6_conv2 = getattr(self.layer1, "6").conv2(layer1_6_relu_post_act_fake_quantizer);  layer1_6_relu_post_act_fake_quantizer = None
    layer1_6_bn2 = getattr(self.layer1, "6").bn2(layer1_6_conv2);  layer1_6_conv2 = None
    add_6 = layer1_6_bn2 + layer1_5_relu_1_post_act_fake_quantizer;  layer1_6_bn2 = layer1_5_relu_1_post_act_fake_quantizer = None
    layer1_6_relu_1 = getattr(self.layer1, "6").relu_dup1(add_6);  add_6 = None
    layer1_6_relu_1_post_act_fake_quantizer = self.layer1_6_relu_1_post_act_fake_quantizer(layer1_6_relu_1);  layer1_6_relu_1 = None
    return layer1_6_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_6_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_7_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_6_relu_1_post_act_fake_quantizer, layer1_7_conv1, layer1_7_bn1, layer1_7_relu, layer1_7_relu_post_act_fake_quantizer, layer1_7_conv2, layer1_7_bn2, add_7, layer1_7_relu_1, layer1_7_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_6_relu_1_post_act_fake_quantizer):
    layer1_7_conv1 = getattr(self.layer1, "7").conv1(layer1_6_relu_1_post_act_fake_quantizer)
    layer1_7_bn1 = getattr(self.layer1, "7").bn1(layer1_7_conv1);  layer1_7_conv1 = None
    layer1_7_relu = getattr(self.layer1, "7").relu(layer1_7_bn1);  layer1_7_bn1 = None
    layer1_7_relu_post_act_fake_quantizer = self.layer1_7_relu_post_act_fake_quantizer(layer1_7_relu);  layer1_7_relu = None
    layer1_7_conv2 = getattr(self.layer1, "7").conv2(layer1_7_relu_post_act_fake_quantizer);  layer1_7_relu_post_act_fake_quantizer = None
    layer1_7_bn2 = getattr(self.layer1, "7").bn2(layer1_7_conv2);  layer1_7_conv2 = None
    add_7 = layer1_7_bn2 + layer1_6_relu_1_post_act_fake_quantizer;  layer1_7_bn2 = layer1_6_relu_1_post_act_fake_quantizer = None
    layer1_7_relu_1 = getattr(self.layer1, "7").relu_dup1(add_7);  add_7 = None
    layer1_7_relu_1_post_act_fake_quantizer = self.layer1_7_relu_1_post_act_fake_quantizer(layer1_7_relu_1);  layer1_7_relu_1 = None
    return layer1_7_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_7_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_8_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_7_relu_1_post_act_fake_quantizer, layer1_8_conv1, layer1_8_bn1, layer1_8_relu, layer1_8_relu_post_act_fake_quantizer, layer1_8_conv2, layer1_8_bn2, add_8, layer1_8_relu_1, layer1_8_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_7_relu_1_post_act_fake_quantizer):
    layer1_8_conv1 = getattr(self.layer1, "8").conv1(layer1_7_relu_1_post_act_fake_quantizer)
    layer1_8_bn1 = getattr(self.layer1, "8").bn1(layer1_8_conv1);  layer1_8_conv1 = None
    layer1_8_relu = getattr(self.layer1, "8").relu(layer1_8_bn1);  layer1_8_bn1 = None
    layer1_8_relu_post_act_fake_quantizer = self.layer1_8_relu_post_act_fake_quantizer(layer1_8_relu);  layer1_8_relu = None
    layer1_8_conv2 = getattr(self.layer1, "8").conv2(layer1_8_relu_post_act_fake_quantizer);  layer1_8_relu_post_act_fake_quantizer = None
    layer1_8_bn2 = getattr(self.layer1, "8").bn2(layer1_8_conv2);  layer1_8_conv2 = None
    add_8 = layer1_8_bn2 + layer1_7_relu_1_post_act_fake_quantizer;  layer1_8_bn2 = layer1_7_relu_1_post_act_fake_quantizer = None
    layer1_8_relu_1 = getattr(self.layer1, "8").relu_dup1(add_8);  add_8 = None
    layer1_8_relu_1_post_act_fake_quantizer = self.layer1_8_relu_1_post_act_fake_quantizer(layer1_8_relu_1);  layer1_8_relu_1 = None
    return layer1_8_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_8_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_0_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_8_relu_1_post_act_fake_quantizer, layer2_0_conv1, layer2_0_bn1, layer2_0_relu, layer2_0_relu_post_act_fake_quantizer, layer2_0_conv2, layer2_0_bn2, layer2_0_downsample_0, layer2_0_downsample_1, add_9, layer2_0_relu_1, layer2_0_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_8_relu_1_post_act_fake_quantizer):
    layer2_0_conv1 = getattr(self.layer2, "0").conv1(layer1_8_relu_1_post_act_fake_quantizer)
    layer2_0_bn1 = getattr(self.layer2, "0").bn1(layer2_0_conv1);  layer2_0_conv1 = None
    layer2_0_relu = getattr(self.layer2, "0").relu(layer2_0_bn1);  layer2_0_bn1 = None
    layer2_0_relu_post_act_fake_quantizer = self.layer2_0_relu_post_act_fake_quantizer(layer2_0_relu);  layer2_0_relu = None
    layer2_0_conv2 = getattr(self.layer2, "0").conv2(layer2_0_relu_post_act_fake_quantizer);  layer2_0_relu_post_act_fake_quantizer = None
    layer2_0_bn2 = getattr(self.layer2, "0").bn2(layer2_0_conv2);  layer2_0_conv2 = None
    layer2_0_downsample_0 = getattr(getattr(self.layer2, "0").downsample, "0")(layer1_8_relu_1_post_act_fake_quantizer);  layer1_8_relu_1_post_act_fake_quantizer = None
    layer2_0_downsample_1 = getattr(getattr(self.layer2, "0").downsample, "1")(layer2_0_downsample_0);  layer2_0_downsample_0 = None
    add_9 = layer2_0_bn2 + layer2_0_downsample_1;  layer2_0_bn2 = layer2_0_downsample_1 = None
    layer2_0_relu_1 = getattr(self.layer2, "0").relu_dup1(add_9);  add_9 = None
    layer2_0_relu_1_post_act_fake_quantizer = self.layer2_0_relu_1_post_act_fake_quantizer(layer2_0_relu_1);  layer2_0_relu_1 = None
    return layer2_0_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_0_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_1_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_0_relu_1_post_act_fake_quantizer, layer2_1_conv1, layer2_1_bn1, layer2_1_relu, layer2_1_relu_post_act_fake_quantizer, layer2_1_conv2, layer2_1_bn2, add_10, layer2_1_relu_1, layer2_1_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_0_relu_1_post_act_fake_quantizer):
    layer2_1_conv1 = getattr(self.layer2, "1").conv1(layer2_0_relu_1_post_act_fake_quantizer)
    layer2_1_bn1 = getattr(self.layer2, "1").bn1(layer2_1_conv1);  layer2_1_conv1 = None
    layer2_1_relu = getattr(self.layer2, "1").relu(layer2_1_bn1);  layer2_1_bn1 = None
    layer2_1_relu_post_act_fake_quantizer = self.layer2_1_relu_post_act_fake_quantizer(layer2_1_relu);  layer2_1_relu = None
    layer2_1_conv2 = getattr(self.layer2, "1").conv2(layer2_1_relu_post_act_fake_quantizer);  layer2_1_relu_post_act_fake_quantizer = None
    layer2_1_bn2 = getattr(self.layer2, "1").bn2(layer2_1_conv2);  layer2_1_conv2 = None
    add_10 = layer2_1_bn2 + layer2_0_relu_1_post_act_fake_quantizer;  layer2_1_bn2 = layer2_0_relu_1_post_act_fake_quantizer = None
    layer2_1_relu_1 = getattr(self.layer2, "1").relu_dup1(add_10);  add_10 = None
    layer2_1_relu_1_post_act_fake_quantizer = self.layer2_1_relu_1_post_act_fake_quantizer(layer2_1_relu_1);  layer2_1_relu_1 = None
    return layer2_1_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_1_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_2_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_1_relu_1_post_act_fake_quantizer, layer2_2_conv1, layer2_2_bn1, layer2_2_relu, layer2_2_relu_post_act_fake_quantizer, layer2_2_conv2, layer2_2_bn2, add_11, layer2_2_relu_1, layer2_2_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_1_relu_1_post_act_fake_quantizer):
    layer2_2_conv1 = getattr(self.layer2, "2").conv1(layer2_1_relu_1_post_act_fake_quantizer)
    layer2_2_bn1 = getattr(self.layer2, "2").bn1(layer2_2_conv1);  layer2_2_conv1 = None
    layer2_2_relu = getattr(self.layer2, "2").relu(layer2_2_bn1);  layer2_2_bn1 = None
    layer2_2_relu_post_act_fake_quantizer = self.layer2_2_relu_post_act_fake_quantizer(layer2_2_relu);  layer2_2_relu = None
    layer2_2_conv2 = getattr(self.layer2, "2").conv2(layer2_2_relu_post_act_fake_quantizer);  layer2_2_relu_post_act_fake_quantizer = None
    layer2_2_bn2 = getattr(self.layer2, "2").bn2(layer2_2_conv2);  layer2_2_conv2 = None
    add_11 = layer2_2_bn2 + layer2_1_relu_1_post_act_fake_quantizer;  layer2_2_bn2 = layer2_1_relu_1_post_act_fake_quantizer = None
    layer2_2_relu_1 = getattr(self.layer2, "2").relu_dup1(add_11);  add_11 = None
    layer2_2_relu_1_post_act_fake_quantizer = self.layer2_2_relu_1_post_act_fake_quantizer(layer2_2_relu_1);  layer2_2_relu_1 = None
    return layer2_2_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_2_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_3_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_2_relu_1_post_act_fake_quantizer, layer2_3_conv1, layer2_3_bn1, layer2_3_relu, layer2_3_relu_post_act_fake_quantizer, layer2_3_conv2, layer2_3_bn2, add_12, layer2_3_relu_1, layer2_3_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_2_relu_1_post_act_fake_quantizer):
    layer2_3_conv1 = getattr(self.layer2, "3").conv1(layer2_2_relu_1_post_act_fake_quantizer)
    layer2_3_bn1 = getattr(self.layer2, "3").bn1(layer2_3_conv1);  layer2_3_conv1 = None
    layer2_3_relu = getattr(self.layer2, "3").relu(layer2_3_bn1);  layer2_3_bn1 = None
    layer2_3_relu_post_act_fake_quantizer = self.layer2_3_relu_post_act_fake_quantizer(layer2_3_relu);  layer2_3_relu = None
    layer2_3_conv2 = getattr(self.layer2, "3").conv2(layer2_3_relu_post_act_fake_quantizer);  layer2_3_relu_post_act_fake_quantizer = None
    layer2_3_bn2 = getattr(self.layer2, "3").bn2(layer2_3_conv2);  layer2_3_conv2 = None
    add_12 = layer2_3_bn2 + layer2_2_relu_1_post_act_fake_quantizer;  layer2_3_bn2 = layer2_2_relu_1_post_act_fake_quantizer = None
    layer2_3_relu_1 = getattr(self.layer2, "3").relu_dup1(add_12);  add_12 = None
    layer2_3_relu_1_post_act_fake_quantizer = self.layer2_3_relu_1_post_act_fake_quantizer(layer2_3_relu_1);  layer2_3_relu_1 = None
    return layer2_3_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_3_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_4_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_3_relu_1_post_act_fake_quantizer, layer2_4_conv1, layer2_4_bn1, layer2_4_relu, layer2_4_relu_post_act_fake_quantizer, layer2_4_conv2, layer2_4_bn2, add_13, layer2_4_relu_1, layer2_4_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_3_relu_1_post_act_fake_quantizer):
    layer2_4_conv1 = getattr(self.layer2, "4").conv1(layer2_3_relu_1_post_act_fake_quantizer)
    layer2_4_bn1 = getattr(self.layer2, "4").bn1(layer2_4_conv1);  layer2_4_conv1 = None
    layer2_4_relu = getattr(self.layer2, "4").relu(layer2_4_bn1);  layer2_4_bn1 = None
    layer2_4_relu_post_act_fake_quantizer = self.layer2_4_relu_post_act_fake_quantizer(layer2_4_relu);  layer2_4_relu = None
    layer2_4_conv2 = getattr(self.layer2, "4").conv2(layer2_4_relu_post_act_fake_quantizer);  layer2_4_relu_post_act_fake_quantizer = None
    layer2_4_bn2 = getattr(self.layer2, "4").bn2(layer2_4_conv2);  layer2_4_conv2 = None
    add_13 = layer2_4_bn2 + layer2_3_relu_1_post_act_fake_quantizer;  layer2_4_bn2 = layer2_3_relu_1_post_act_fake_quantizer = None
    layer2_4_relu_1 = getattr(self.layer2, "4").relu_dup1(add_13);  add_13 = None
    layer2_4_relu_1_post_act_fake_quantizer = self.layer2_4_relu_1_post_act_fake_quantizer(layer2_4_relu_1);  layer2_4_relu_1 = None
    return layer2_4_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_4_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_5_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_4_relu_1_post_act_fake_quantizer, layer2_5_conv1, layer2_5_bn1, layer2_5_relu, layer2_5_relu_post_act_fake_quantizer, layer2_5_conv2, layer2_5_bn2, add_14, layer2_5_relu_1, layer2_5_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_4_relu_1_post_act_fake_quantizer):
    layer2_5_conv1 = getattr(self.layer2, "5").conv1(layer2_4_relu_1_post_act_fake_quantizer)
    layer2_5_bn1 = getattr(self.layer2, "5").bn1(layer2_5_conv1);  layer2_5_conv1 = None
    layer2_5_relu = getattr(self.layer2, "5").relu(layer2_5_bn1);  layer2_5_bn1 = None
    layer2_5_relu_post_act_fake_quantizer = self.layer2_5_relu_post_act_fake_quantizer(layer2_5_relu);  layer2_5_relu = None
    layer2_5_conv2 = getattr(self.layer2, "5").conv2(layer2_5_relu_post_act_fake_quantizer);  layer2_5_relu_post_act_fake_quantizer = None
    layer2_5_bn2 = getattr(self.layer2, "5").bn2(layer2_5_conv2);  layer2_5_conv2 = None
    add_14 = layer2_5_bn2 + layer2_4_relu_1_post_act_fake_quantizer;  layer2_5_bn2 = layer2_4_relu_1_post_act_fake_quantizer = None
    layer2_5_relu_1 = getattr(self.layer2, "5").relu_dup1(add_14);  add_14 = None
    layer2_5_relu_1_post_act_fake_quantizer = self.layer2_5_relu_1_post_act_fake_quantizer(layer2_5_relu_1);  layer2_5_relu_1 = None
    return layer2_5_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_5_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_6_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_5_relu_1_post_act_fake_quantizer, layer2_6_conv1, layer2_6_bn1, layer2_6_relu, layer2_6_relu_post_act_fake_quantizer, layer2_6_conv2, layer2_6_bn2, add_15, layer2_6_relu_1, layer2_6_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_5_relu_1_post_act_fake_quantizer):
    layer2_6_conv1 = getattr(self.layer2, "6").conv1(layer2_5_relu_1_post_act_fake_quantizer)
    layer2_6_bn1 = getattr(self.layer2, "6").bn1(layer2_6_conv1);  layer2_6_conv1 = None
    layer2_6_relu = getattr(self.layer2, "6").relu(layer2_6_bn1);  layer2_6_bn1 = None
    layer2_6_relu_post_act_fake_quantizer = self.layer2_6_relu_post_act_fake_quantizer(layer2_6_relu);  layer2_6_relu = None
    layer2_6_conv2 = getattr(self.layer2, "6").conv2(layer2_6_relu_post_act_fake_quantizer);  layer2_6_relu_post_act_fake_quantizer = None
    layer2_6_bn2 = getattr(self.layer2, "6").bn2(layer2_6_conv2);  layer2_6_conv2 = None
    add_15 = layer2_6_bn2 + layer2_5_relu_1_post_act_fake_quantizer;  layer2_6_bn2 = layer2_5_relu_1_post_act_fake_quantizer = None
    layer2_6_relu_1 = getattr(self.layer2, "6").relu_dup1(add_15);  add_15 = None
    layer2_6_relu_1_post_act_fake_quantizer = self.layer2_6_relu_1_post_act_fake_quantizer(layer2_6_relu_1);  layer2_6_relu_1 = None
    return layer2_6_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_6_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_7_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_6_relu_1_post_act_fake_quantizer, layer2_7_conv1, layer2_7_bn1, layer2_7_relu, layer2_7_relu_post_act_fake_quantizer, layer2_7_conv2, layer2_7_bn2, add_16, layer2_7_relu_1, layer2_7_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_6_relu_1_post_act_fake_quantizer):
    layer2_7_conv1 = getattr(self.layer2, "7").conv1(layer2_6_relu_1_post_act_fake_quantizer)
    layer2_7_bn1 = getattr(self.layer2, "7").bn1(layer2_7_conv1);  layer2_7_conv1 = None
    layer2_7_relu = getattr(self.layer2, "7").relu(layer2_7_bn1);  layer2_7_bn1 = None
    layer2_7_relu_post_act_fake_quantizer = self.layer2_7_relu_post_act_fake_quantizer(layer2_7_relu);  layer2_7_relu = None
    layer2_7_conv2 = getattr(self.layer2, "7").conv2(layer2_7_relu_post_act_fake_quantizer);  layer2_7_relu_post_act_fake_quantizer = None
    layer2_7_bn2 = getattr(self.layer2, "7").bn2(layer2_7_conv2);  layer2_7_conv2 = None
    add_16 = layer2_7_bn2 + layer2_6_relu_1_post_act_fake_quantizer;  layer2_7_bn2 = layer2_6_relu_1_post_act_fake_quantizer = None
    layer2_7_relu_1 = getattr(self.layer2, "7").relu_dup1(add_16);  add_16 = None
    layer2_7_relu_1_post_act_fake_quantizer = self.layer2_7_relu_1_post_act_fake_quantizer(layer2_7_relu_1);  layer2_7_relu_1 = None
    return layer2_7_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_7_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_8_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_7_relu_1_post_act_fake_quantizer, layer2_8_conv1, layer2_8_bn1, layer2_8_relu, layer2_8_relu_post_act_fake_quantizer, layer2_8_conv2, layer2_8_bn2, add_17, layer2_8_relu_1, layer2_8_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_7_relu_1_post_act_fake_quantizer):
    layer2_8_conv1 = getattr(self.layer2, "8").conv1(layer2_7_relu_1_post_act_fake_quantizer)
    layer2_8_bn1 = getattr(self.layer2, "8").bn1(layer2_8_conv1);  layer2_8_conv1 = None
    layer2_8_relu = getattr(self.layer2, "8").relu(layer2_8_bn1);  layer2_8_bn1 = None
    layer2_8_relu_post_act_fake_quantizer = self.layer2_8_relu_post_act_fake_quantizer(layer2_8_relu);  layer2_8_relu = None
    layer2_8_conv2 = getattr(self.layer2, "8").conv2(layer2_8_relu_post_act_fake_quantizer);  layer2_8_relu_post_act_fake_quantizer = None
    layer2_8_bn2 = getattr(self.layer2, "8").bn2(layer2_8_conv2);  layer2_8_conv2 = None
    add_17 = layer2_8_bn2 + layer2_7_relu_1_post_act_fake_quantizer;  layer2_8_bn2 = layer2_7_relu_1_post_act_fake_quantizer = None
    layer2_8_relu_1 = getattr(self.layer2, "8").relu_dup1(add_17);  add_17 = None
    layer2_8_relu_1_post_act_fake_quantizer = self.layer2_8_relu_1_post_act_fake_quantizer(layer2_8_relu_1);  layer2_8_relu_1 = None
    return layer2_8_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_8_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_0_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_8_relu_1_post_act_fake_quantizer, layer3_0_conv1, layer3_0_bn1, layer3_0_relu, layer3_0_relu_post_act_fake_quantizer, layer3_0_conv2, layer3_0_bn2, layer3_0_downsample_0, layer3_0_downsample_1, add_18, layer3_0_relu_1, layer3_0_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_8_relu_1_post_act_fake_quantizer):
    layer3_0_conv1 = getattr(self.layer3, "0").conv1(layer2_8_relu_1_post_act_fake_quantizer)
    layer3_0_bn1 = getattr(self.layer3, "0").bn1(layer3_0_conv1);  layer3_0_conv1 = None
    layer3_0_relu = getattr(self.layer3, "0").relu(layer3_0_bn1);  layer3_0_bn1 = None
    layer3_0_relu_post_act_fake_quantizer = self.layer3_0_relu_post_act_fake_quantizer(layer3_0_relu);  layer3_0_relu = None
    layer3_0_conv2 = getattr(self.layer3, "0").conv2(layer3_0_relu_post_act_fake_quantizer);  layer3_0_relu_post_act_fake_quantizer = None
    layer3_0_bn2 = getattr(self.layer3, "0").bn2(layer3_0_conv2);  layer3_0_conv2 = None
    layer3_0_downsample_0 = getattr(getattr(self.layer3, "0").downsample, "0")(layer2_8_relu_1_post_act_fake_quantizer);  layer2_8_relu_1_post_act_fake_quantizer = None
    layer3_0_downsample_1 = getattr(getattr(self.layer3, "0").downsample, "1")(layer3_0_downsample_0);  layer3_0_downsample_0 = None
    add_18 = layer3_0_bn2 + layer3_0_downsample_1;  layer3_0_bn2 = layer3_0_downsample_1 = None
    layer3_0_relu_1 = getattr(self.layer3, "0").relu_dup1(add_18);  add_18 = None
    layer3_0_relu_1_post_act_fake_quantizer = self.layer3_0_relu_1_post_act_fake_quantizer(layer3_0_relu_1);  layer3_0_relu_1 = None
    return layer3_0_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_0_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_1_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_0_relu_1_post_act_fake_quantizer, layer3_1_conv1, layer3_1_bn1, layer3_1_relu, layer3_1_relu_post_act_fake_quantizer, layer3_1_conv2, layer3_1_bn2, add_19, layer3_1_relu_1, layer3_1_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_0_relu_1_post_act_fake_quantizer):
    layer3_1_conv1 = getattr(self.layer3, "1").conv1(layer3_0_relu_1_post_act_fake_quantizer)
    layer3_1_bn1 = getattr(self.layer3, "1").bn1(layer3_1_conv1);  layer3_1_conv1 = None
    layer3_1_relu = getattr(self.layer3, "1").relu(layer3_1_bn1);  layer3_1_bn1 = None
    layer3_1_relu_post_act_fake_quantizer = self.layer3_1_relu_post_act_fake_quantizer(layer3_1_relu);  layer3_1_relu = None
    layer3_1_conv2 = getattr(self.layer3, "1").conv2(layer3_1_relu_post_act_fake_quantizer);  layer3_1_relu_post_act_fake_quantizer = None
    layer3_1_bn2 = getattr(self.layer3, "1").bn2(layer3_1_conv2);  layer3_1_conv2 = None
    add_19 = layer3_1_bn2 + layer3_0_relu_1_post_act_fake_quantizer;  layer3_1_bn2 = layer3_0_relu_1_post_act_fake_quantizer = None
    layer3_1_relu_1 = getattr(self.layer3, "1").relu_dup1(add_19);  add_19 = None
    layer3_1_relu_1_post_act_fake_quantizer = self.layer3_1_relu_1_post_act_fake_quantizer(layer3_1_relu_1);  layer3_1_relu_1 = None
    return layer3_1_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_1_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_2_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_1_relu_1_post_act_fake_quantizer, layer3_2_conv1, layer3_2_bn1, layer3_2_relu, layer3_2_relu_post_act_fake_quantizer, layer3_2_conv2, layer3_2_bn2, add_20, layer3_2_relu_1, layer3_2_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_1_relu_1_post_act_fake_quantizer):
    layer3_2_conv1 = getattr(self.layer3, "2").conv1(layer3_1_relu_1_post_act_fake_quantizer)
    layer3_2_bn1 = getattr(self.layer3, "2").bn1(layer3_2_conv1);  layer3_2_conv1 = None
    layer3_2_relu = getattr(self.layer3, "2").relu(layer3_2_bn1);  layer3_2_bn1 = None
    layer3_2_relu_post_act_fake_quantizer = self.layer3_2_relu_post_act_fake_quantizer(layer3_2_relu);  layer3_2_relu = None
    layer3_2_conv2 = getattr(self.layer3, "2").conv2(layer3_2_relu_post_act_fake_quantizer);  layer3_2_relu_post_act_fake_quantizer = None
    layer3_2_bn2 = getattr(self.layer3, "2").bn2(layer3_2_conv2);  layer3_2_conv2 = None
    add_20 = layer3_2_bn2 + layer3_1_relu_1_post_act_fake_quantizer;  layer3_2_bn2 = layer3_1_relu_1_post_act_fake_quantizer = None
    layer3_2_relu_1 = getattr(self.layer3, "2").relu_dup1(add_20);  add_20 = None
    layer3_2_relu_1_post_act_fake_quantizer = self.layer3_2_relu_1_post_act_fake_quantizer(layer3_2_relu_1);  layer3_2_relu_1 = None
    return layer3_2_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_2_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_3_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_2_relu_1_post_act_fake_quantizer, layer3_3_conv1, layer3_3_bn1, layer3_3_relu, layer3_3_relu_post_act_fake_quantizer, layer3_3_conv2, layer3_3_bn2, add_21, layer3_3_relu_1, layer3_3_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_2_relu_1_post_act_fake_quantizer):
    layer3_3_conv1 = getattr(self.layer3, "3").conv1(layer3_2_relu_1_post_act_fake_quantizer)
    layer3_3_bn1 = getattr(self.layer3, "3").bn1(layer3_3_conv1);  layer3_3_conv1 = None
    layer3_3_relu = getattr(self.layer3, "3").relu(layer3_3_bn1);  layer3_3_bn1 = None
    layer3_3_relu_post_act_fake_quantizer = self.layer3_3_relu_post_act_fake_quantizer(layer3_3_relu);  layer3_3_relu = None
    layer3_3_conv2 = getattr(self.layer3, "3").conv2(layer3_3_relu_post_act_fake_quantizer);  layer3_3_relu_post_act_fake_quantizer = None
    layer3_3_bn2 = getattr(self.layer3, "3").bn2(layer3_3_conv2);  layer3_3_conv2 = None
    add_21 = layer3_3_bn2 + layer3_2_relu_1_post_act_fake_quantizer;  layer3_3_bn2 = layer3_2_relu_1_post_act_fake_quantizer = None
    layer3_3_relu_1 = getattr(self.layer3, "3").relu_dup1(add_21);  add_21 = None
    layer3_3_relu_1_post_act_fake_quantizer = self.layer3_3_relu_1_post_act_fake_quantizer(layer3_3_relu_1);  layer3_3_relu_1 = None
    return layer3_3_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_3_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_4_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_3_relu_1_post_act_fake_quantizer, layer3_4_conv1, layer3_4_bn1, layer3_4_relu, layer3_4_relu_post_act_fake_quantizer, layer3_4_conv2, layer3_4_bn2, add_22, layer3_4_relu_1, layer3_4_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_3_relu_1_post_act_fake_quantizer):
    layer3_4_conv1 = getattr(self.layer3, "4").conv1(layer3_3_relu_1_post_act_fake_quantizer)
    layer3_4_bn1 = getattr(self.layer3, "4").bn1(layer3_4_conv1);  layer3_4_conv1 = None
    layer3_4_relu = getattr(self.layer3, "4").relu(layer3_4_bn1);  layer3_4_bn1 = None
    layer3_4_relu_post_act_fake_quantizer = self.layer3_4_relu_post_act_fake_quantizer(layer3_4_relu);  layer3_4_relu = None
    layer3_4_conv2 = getattr(self.layer3, "4").conv2(layer3_4_relu_post_act_fake_quantizer);  layer3_4_relu_post_act_fake_quantizer = None
    layer3_4_bn2 = getattr(self.layer3, "4").bn2(layer3_4_conv2);  layer3_4_conv2 = None
    add_22 = layer3_4_bn2 + layer3_3_relu_1_post_act_fake_quantizer;  layer3_4_bn2 = layer3_3_relu_1_post_act_fake_quantizer = None
    layer3_4_relu_1 = getattr(self.layer3, "4").relu_dup1(add_22);  add_22 = None
    layer3_4_relu_1_post_act_fake_quantizer = self.layer3_4_relu_1_post_act_fake_quantizer(layer3_4_relu_1);  layer3_4_relu_1 = None
    return layer3_4_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_4_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_5_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_4_relu_1_post_act_fake_quantizer, layer3_5_conv1, layer3_5_bn1, layer3_5_relu, layer3_5_relu_post_act_fake_quantizer, layer3_5_conv2, layer3_5_bn2, add_23, layer3_5_relu_1, layer3_5_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_4_relu_1_post_act_fake_quantizer):
    layer3_5_conv1 = getattr(self.layer3, "5").conv1(layer3_4_relu_1_post_act_fake_quantizer)
    layer3_5_bn1 = getattr(self.layer3, "5").bn1(layer3_5_conv1);  layer3_5_conv1 = None
    layer3_5_relu = getattr(self.layer3, "5").relu(layer3_5_bn1);  layer3_5_bn1 = None
    layer3_5_relu_post_act_fake_quantizer = self.layer3_5_relu_post_act_fake_quantizer(layer3_5_relu);  layer3_5_relu = None
    layer3_5_conv2 = getattr(self.layer3, "5").conv2(layer3_5_relu_post_act_fake_quantizer);  layer3_5_relu_post_act_fake_quantizer = None
    layer3_5_bn2 = getattr(self.layer3, "5").bn2(layer3_5_conv2);  layer3_5_conv2 = None
    add_23 = layer3_5_bn2 + layer3_4_relu_1_post_act_fake_quantizer;  layer3_5_bn2 = layer3_4_relu_1_post_act_fake_quantizer = None
    layer3_5_relu_1 = getattr(self.layer3, "5").relu_dup1(add_23);  add_23 = None
    layer3_5_relu_1_post_act_fake_quantizer = self.layer3_5_relu_1_post_act_fake_quantizer(layer3_5_relu_1);  layer3_5_relu_1 = None
    return layer3_5_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_5_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_6_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_5_relu_1_post_act_fake_quantizer, layer3_6_conv1, layer3_6_bn1, layer3_6_relu, layer3_6_relu_post_act_fake_quantizer, layer3_6_conv2, layer3_6_bn2, add_24, layer3_6_relu_1, layer3_6_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_5_relu_1_post_act_fake_quantizer):
    layer3_6_conv1 = getattr(self.layer3, "6").conv1(layer3_5_relu_1_post_act_fake_quantizer)
    layer3_6_bn1 = getattr(self.layer3, "6").bn1(layer3_6_conv1);  layer3_6_conv1 = None
    layer3_6_relu = getattr(self.layer3, "6").relu(layer3_6_bn1);  layer3_6_bn1 = None
    layer3_6_relu_post_act_fake_quantizer = self.layer3_6_relu_post_act_fake_quantizer(layer3_6_relu);  layer3_6_relu = None
    layer3_6_conv2 = getattr(self.layer3, "6").conv2(layer3_6_relu_post_act_fake_quantizer);  layer3_6_relu_post_act_fake_quantizer = None
    layer3_6_bn2 = getattr(self.layer3, "6").bn2(layer3_6_conv2);  layer3_6_conv2 = None
    add_24 = layer3_6_bn2 + layer3_5_relu_1_post_act_fake_quantizer;  layer3_6_bn2 = layer3_5_relu_1_post_act_fake_quantizer = None
    layer3_6_relu_1 = getattr(self.layer3, "6").relu_dup1(add_24);  add_24 = None
    layer3_6_relu_1_post_act_fake_quantizer = self.layer3_6_relu_1_post_act_fake_quantizer(layer3_6_relu_1);  layer3_6_relu_1 = None
    return layer3_6_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_6_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_7_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_6_relu_1_post_act_fake_quantizer, layer3_7_conv1, layer3_7_bn1, layer3_7_relu, layer3_7_relu_post_act_fake_quantizer, layer3_7_conv2, layer3_7_bn2, add_25, layer3_7_relu_1, layer3_7_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_6_relu_1_post_act_fake_quantizer):
    layer3_7_conv1 = getattr(self.layer3, "7").conv1(layer3_6_relu_1_post_act_fake_quantizer)
    layer3_7_bn1 = getattr(self.layer3, "7").bn1(layer3_7_conv1);  layer3_7_conv1 = None
    layer3_7_relu = getattr(self.layer3, "7").relu(layer3_7_bn1);  layer3_7_bn1 = None
    layer3_7_relu_post_act_fake_quantizer = self.layer3_7_relu_post_act_fake_quantizer(layer3_7_relu);  layer3_7_relu = None
    layer3_7_conv2 = getattr(self.layer3, "7").conv2(layer3_7_relu_post_act_fake_quantizer);  layer3_7_relu_post_act_fake_quantizer = None
    layer3_7_bn2 = getattr(self.layer3, "7").bn2(layer3_7_conv2);  layer3_7_conv2 = None
    add_25 = layer3_7_bn2 + layer3_6_relu_1_post_act_fake_quantizer;  layer3_7_bn2 = layer3_6_relu_1_post_act_fake_quantizer = None
    layer3_7_relu_1 = getattr(self.layer3, "7").relu_dup1(add_25);  add_25 = None
    layer3_7_relu_1_post_act_fake_quantizer = self.layer3_7_relu_1_post_act_fake_quantizer(layer3_7_relu_1);  layer3_7_relu_1 = None
    return layer3_7_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_7_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_8_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_7_relu_1_post_act_fake_quantizer, layer3_8_conv1, layer3_8_bn1, layer3_8_relu, layer3_8_relu_post_act_fake_quantizer, layer3_8_conv2, layer3_8_bn2, add_26, layer3_8_relu_1, avgpool, size, view, view_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_7_relu_1_post_act_fake_quantizer):
    layer3_8_conv1 = getattr(self.layer3, "8").conv1(layer3_7_relu_1_post_act_fake_quantizer)
    layer3_8_bn1 = getattr(self.layer3, "8").bn1(layer3_8_conv1);  layer3_8_conv1 = None
    layer3_8_relu = getattr(self.layer3, "8").relu(layer3_8_bn1);  layer3_8_bn1 = None
    layer3_8_relu_post_act_fake_quantizer = self.layer3_8_relu_post_act_fake_quantizer(layer3_8_relu);  layer3_8_relu = None
    layer3_8_conv2 = getattr(self.layer3, "8").conv2(layer3_8_relu_post_act_fake_quantizer);  layer3_8_relu_post_act_fake_quantizer = None
    layer3_8_bn2 = getattr(self.layer3, "8").bn2(layer3_8_conv2);  layer3_8_conv2 = None
    add_26 = layer3_8_bn2 + layer3_7_relu_1_post_act_fake_quantizer;  layer3_8_bn2 = layer3_7_relu_1_post_act_fake_quantizer = None
    layer3_8_relu_1 = getattr(self.layer3, "8").relu_dup1(add_26);  add_26 = None
    avgpool = self.avgpool(layer3_8_relu_1);  layer3_8_relu_1 = None
    size = avgpool.size(0)
    view = avgpool.view(size, -1);  avgpool = size = None
    view_post_act_fake_quantizer = self.view_post_act_fake_quantizer(view);  view = None
    return view_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for view_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for fc
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [view_post_act_fake_quantizer, fc]
[MQBENCH] INFO: 


def forward(self, view_post_act_fake_quantizer):
    fc = self.fc(view_post_act_fake_quantizer);  view_post_act_fake_quantizer = None
    return fc
    
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node x_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_0_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_0_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_1_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_1_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_2_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_2_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_3_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_3_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_4_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_4_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_5_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_5_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_6_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_6_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_7_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_7_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_8_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_8_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_0_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.downsample.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.downsample.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_0_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_1_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_1_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_2_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_2_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_3_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_3_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_4_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_4_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_5_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_5_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_6_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_6_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_7_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_7_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_8_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_8_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_0_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.downsample.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.downsample.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_0_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_1_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_1_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_2_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_2_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_3_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_3_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_4_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_4_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_5_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_5_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_6_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_6_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_7_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_7_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_8_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node avgpool in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node view_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node fc in quant
QDROP model already saved, now loading QDROP_4bits_cifar100_resnet56.pt
Prepare 8bits model using MQBench
[MQBENCH] INFO: Quantize model Scheme: BackendType.Academic Mode: Eval
[MQBENCH] INFO: Weight Qconfig:
    FakeQuantize: AdaRoundFakeQuantize Params: {}
    Oberver:      MSEObserver Params: Symmetric: True / Bitwidth: 8 / Per channel: False / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Activation Qconfig:
    FakeQuantize: QDropFakeQuantize Params: {}
    Oberver:      EMAMSEObserver Params: Symmetric: False / Bitwidth: 8 / Per channel: False / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Replace module to qat module.
[MQBENCH] INFO: Set layer conv1 to 8 bit.
[MQBENCH] INFO: Set layer fc to 8 bit.
dbg node_to_quantize_output
 odict_keys([x, relu, layer1_0_relu, layer1_0_relu_1, layer1_1_relu, layer1_1_relu_1, layer1_2_relu, layer1_2_relu_1, layer1_3_relu, layer1_3_relu_1, layer1_4_relu, layer1_4_relu_1, layer1_5_relu, layer1_5_relu_1, layer1_6_relu, layer1_6_relu_1, layer1_7_relu, layer1_7_relu_1, layer1_8_relu, layer1_8_relu_1, layer2_0_relu, layer2_0_relu_1, layer2_1_relu, layer2_1_relu_1, layer2_2_relu, layer2_2_relu_1, layer2_3_relu, layer2_3_relu_1, layer2_4_relu, layer2_4_relu_1, layer2_5_relu, layer2_5_relu_1, layer2_6_relu, layer2_6_relu_1, layer2_7_relu, layer2_7_relu_1, layer2_8_relu, layer2_8_relu_1, layer3_0_relu, layer3_0_relu_1, layer3_1_relu, layer3_1_relu_1, layer3_2_relu, layer3_2_relu_1, layer3_3_relu, layer3_3_relu_1, layer3_4_relu, layer3_4_relu_1, layer3_5_relu, layer3_5_relu_1, layer3_6_relu, layer3_6_relu_1, layer3_7_relu, layer3_7_relu_1, layer3_8_relu, view])
[MQBENCH] INFO: Set x post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant x_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_4_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_5_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_6_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_7_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_8_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_4_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_5_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_6_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_7_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_8_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_4_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_5_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_6_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_7_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: Set view post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant view_post_act_fake_quantizer
[MQBENCH] INFO: Enable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: prepare block reconstruction for conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [x_post_act_fake_quantizer, conv1, bn1, relu, relu_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, x_post_act_fake_quantizer):
    conv1 = self.conv1(x_post_act_fake_quantizer);  x_post_act_fake_quantizer = None
    bn1 = self.bn1(conv1);  conv1 = None
    relu = self.relu(bn1);  bn1 = None
    relu_post_act_fake_quantizer = self.relu_post_act_fake_quantizer(relu);  relu = None
    return relu_post_act_fake_quantizer
    
Init alpha to be FP32
[MQBENCH] INFO: learn the scale for relu_post_act_fake_quantizer
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_0_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [relu_post_act_fake_quantizer, layer1_0_conv1, layer1_0_bn1, layer1_0_relu, layer1_0_relu_post_act_fake_quantizer, layer1_0_conv2, layer1_0_bn2, add, layer1_0_relu_1, layer1_0_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, relu_post_act_fake_quantizer):
    layer1_0_conv1 = getattr(self.layer1, "0").conv1(relu_post_act_fake_quantizer)
    layer1_0_bn1 = getattr(self.layer1, "0").bn1(layer1_0_conv1);  layer1_0_conv1 = None
    layer1_0_relu = getattr(self.layer1, "0").relu(layer1_0_bn1);  layer1_0_bn1 = None
    layer1_0_relu_post_act_fake_quantizer = self.layer1_0_relu_post_act_fake_quantizer(layer1_0_relu);  layer1_0_relu = None
    layer1_0_conv2 = getattr(self.layer1, "0").conv2(layer1_0_relu_post_act_fake_quantizer);  layer1_0_relu_post_act_fake_quantizer = None
    layer1_0_bn2 = getattr(self.layer1, "0").bn2(layer1_0_conv2);  layer1_0_conv2 = None
    add = layer1_0_bn2 + relu_post_act_fake_quantizer;  layer1_0_bn2 = relu_post_act_fake_quantizer = None
    layer1_0_relu_1 = getattr(self.layer1, "0").relu_dup1(add);  add = None
    layer1_0_relu_1_post_act_fake_quantizer = self.layer1_0_relu_1_post_act_fake_quantizer(layer1_0_relu_1);  layer1_0_relu_1 = None
    return layer1_0_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_0_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_1_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_0_relu_1_post_act_fake_quantizer, layer1_1_conv1, layer1_1_bn1, layer1_1_relu, layer1_1_relu_post_act_fake_quantizer, layer1_1_conv2, layer1_1_bn2, add_1, layer1_1_relu_1, layer1_1_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_0_relu_1_post_act_fake_quantizer):
    layer1_1_conv1 = getattr(self.layer1, "1").conv1(layer1_0_relu_1_post_act_fake_quantizer)
    layer1_1_bn1 = getattr(self.layer1, "1").bn1(layer1_1_conv1);  layer1_1_conv1 = None
    layer1_1_relu = getattr(self.layer1, "1").relu(layer1_1_bn1);  layer1_1_bn1 = None
    layer1_1_relu_post_act_fake_quantizer = self.layer1_1_relu_post_act_fake_quantizer(layer1_1_relu);  layer1_1_relu = None
    layer1_1_conv2 = getattr(self.layer1, "1").conv2(layer1_1_relu_post_act_fake_quantizer);  layer1_1_relu_post_act_fake_quantizer = None
    layer1_1_bn2 = getattr(self.layer1, "1").bn2(layer1_1_conv2);  layer1_1_conv2 = None
    add_1 = layer1_1_bn2 + layer1_0_relu_1_post_act_fake_quantizer;  layer1_1_bn2 = layer1_0_relu_1_post_act_fake_quantizer = None
    layer1_1_relu_1 = getattr(self.layer1, "1").relu_dup1(add_1);  add_1 = None
    layer1_1_relu_1_post_act_fake_quantizer = self.layer1_1_relu_1_post_act_fake_quantizer(layer1_1_relu_1);  layer1_1_relu_1 = None
    return layer1_1_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_1_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_2_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_1_relu_1_post_act_fake_quantizer, layer1_2_conv1, layer1_2_bn1, layer1_2_relu, layer1_2_relu_post_act_fake_quantizer, layer1_2_conv2, layer1_2_bn2, add_2, layer1_2_relu_1, layer1_2_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_1_relu_1_post_act_fake_quantizer):
    layer1_2_conv1 = getattr(self.layer1, "2").conv1(layer1_1_relu_1_post_act_fake_quantizer)
    layer1_2_bn1 = getattr(self.layer1, "2").bn1(layer1_2_conv1);  layer1_2_conv1 = None
    layer1_2_relu = getattr(self.layer1, "2").relu(layer1_2_bn1);  layer1_2_bn1 = None
    layer1_2_relu_post_act_fake_quantizer = self.layer1_2_relu_post_act_fake_quantizer(layer1_2_relu);  layer1_2_relu = None
    layer1_2_conv2 = getattr(self.layer1, "2").conv2(layer1_2_relu_post_act_fake_quantizer);  layer1_2_relu_post_act_fake_quantizer = None
    layer1_2_bn2 = getattr(self.layer1, "2").bn2(layer1_2_conv2);  layer1_2_conv2 = None
    add_2 = layer1_2_bn2 + layer1_1_relu_1_post_act_fake_quantizer;  layer1_2_bn2 = layer1_1_relu_1_post_act_fake_quantizer = None
    layer1_2_relu_1 = getattr(self.layer1, "2").relu_dup1(add_2);  add_2 = None
    layer1_2_relu_1_post_act_fake_quantizer = self.layer1_2_relu_1_post_act_fake_quantizer(layer1_2_relu_1);  layer1_2_relu_1 = None
    return layer1_2_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_2_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_3_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_2_relu_1_post_act_fake_quantizer, layer1_3_conv1, layer1_3_bn1, layer1_3_relu, layer1_3_relu_post_act_fake_quantizer, layer1_3_conv2, layer1_3_bn2, add_3, layer1_3_relu_1, layer1_3_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_2_relu_1_post_act_fake_quantizer):
    layer1_3_conv1 = getattr(self.layer1, "3").conv1(layer1_2_relu_1_post_act_fake_quantizer)
    layer1_3_bn1 = getattr(self.layer1, "3").bn1(layer1_3_conv1);  layer1_3_conv1 = None
    layer1_3_relu = getattr(self.layer1, "3").relu(layer1_3_bn1);  layer1_3_bn1 = None
    layer1_3_relu_post_act_fake_quantizer = self.layer1_3_relu_post_act_fake_quantizer(layer1_3_relu);  layer1_3_relu = None
    layer1_3_conv2 = getattr(self.layer1, "3").conv2(layer1_3_relu_post_act_fake_quantizer);  layer1_3_relu_post_act_fake_quantizer = None
    layer1_3_bn2 = getattr(self.layer1, "3").bn2(layer1_3_conv2);  layer1_3_conv2 = None
    add_3 = layer1_3_bn2 + layer1_2_relu_1_post_act_fake_quantizer;  layer1_3_bn2 = layer1_2_relu_1_post_act_fake_quantizer = None
    layer1_3_relu_1 = getattr(self.layer1, "3").relu_dup1(add_3);  add_3 = None
    layer1_3_relu_1_post_act_fake_quantizer = self.layer1_3_relu_1_post_act_fake_quantizer(layer1_3_relu_1);  layer1_3_relu_1 = None
    return layer1_3_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_3_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_4_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_3_relu_1_post_act_fake_quantizer, layer1_4_conv1, layer1_4_bn1, layer1_4_relu, layer1_4_relu_post_act_fake_quantizer, layer1_4_conv2, layer1_4_bn2, add_4, layer1_4_relu_1, layer1_4_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_3_relu_1_post_act_fake_quantizer):
    layer1_4_conv1 = getattr(self.layer1, "4").conv1(layer1_3_relu_1_post_act_fake_quantizer)
    layer1_4_bn1 = getattr(self.layer1, "4").bn1(layer1_4_conv1);  layer1_4_conv1 = None
    layer1_4_relu = getattr(self.layer1, "4").relu(layer1_4_bn1);  layer1_4_bn1 = None
    layer1_4_relu_post_act_fake_quantizer = self.layer1_4_relu_post_act_fake_quantizer(layer1_4_relu);  layer1_4_relu = None
    layer1_4_conv2 = getattr(self.layer1, "4").conv2(layer1_4_relu_post_act_fake_quantizer);  layer1_4_relu_post_act_fake_quantizer = None
    layer1_4_bn2 = getattr(self.layer1, "4").bn2(layer1_4_conv2);  layer1_4_conv2 = None
    add_4 = layer1_4_bn2 + layer1_3_relu_1_post_act_fake_quantizer;  layer1_4_bn2 = layer1_3_relu_1_post_act_fake_quantizer = None
    layer1_4_relu_1 = getattr(self.layer1, "4").relu_dup1(add_4);  add_4 = None
    layer1_4_relu_1_post_act_fake_quantizer = self.layer1_4_relu_1_post_act_fake_quantizer(layer1_4_relu_1);  layer1_4_relu_1 = None
    return layer1_4_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_4_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_5_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_4_relu_1_post_act_fake_quantizer, layer1_5_conv1, layer1_5_bn1, layer1_5_relu, layer1_5_relu_post_act_fake_quantizer, layer1_5_conv2, layer1_5_bn2, add_5, layer1_5_relu_1, layer1_5_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_4_relu_1_post_act_fake_quantizer):
    layer1_5_conv1 = getattr(self.layer1, "5").conv1(layer1_4_relu_1_post_act_fake_quantizer)
    layer1_5_bn1 = getattr(self.layer1, "5").bn1(layer1_5_conv1);  layer1_5_conv1 = None
    layer1_5_relu = getattr(self.layer1, "5").relu(layer1_5_bn1);  layer1_5_bn1 = None
    layer1_5_relu_post_act_fake_quantizer = self.layer1_5_relu_post_act_fake_quantizer(layer1_5_relu);  layer1_5_relu = None
    layer1_5_conv2 = getattr(self.layer1, "5").conv2(layer1_5_relu_post_act_fake_quantizer);  layer1_5_relu_post_act_fake_quantizer = None
    layer1_5_bn2 = getattr(self.layer1, "5").bn2(layer1_5_conv2);  layer1_5_conv2 = None
    add_5 = layer1_5_bn2 + layer1_4_relu_1_post_act_fake_quantizer;  layer1_5_bn2 = layer1_4_relu_1_post_act_fake_quantizer = None
    layer1_5_relu_1 = getattr(self.layer1, "5").relu_dup1(add_5);  add_5 = None
    layer1_5_relu_1_post_act_fake_quantizer = self.layer1_5_relu_1_post_act_fake_quantizer(layer1_5_relu_1);  layer1_5_relu_1 = None
    return layer1_5_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_5_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_6_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_5_relu_1_post_act_fake_quantizer, layer1_6_conv1, layer1_6_bn1, layer1_6_relu, layer1_6_relu_post_act_fake_quantizer, layer1_6_conv2, layer1_6_bn2, add_6, layer1_6_relu_1, layer1_6_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_5_relu_1_post_act_fake_quantizer):
    layer1_6_conv1 = getattr(self.layer1, "6").conv1(layer1_5_relu_1_post_act_fake_quantizer)
    layer1_6_bn1 = getattr(self.layer1, "6").bn1(layer1_6_conv1);  layer1_6_conv1 = None
    layer1_6_relu = getattr(self.layer1, "6").relu(layer1_6_bn1);  layer1_6_bn1 = None
    layer1_6_relu_post_act_fake_quantizer = self.layer1_6_relu_post_act_fake_quantizer(layer1_6_relu);  layer1_6_relu = None
    layer1_6_conv2 = getattr(self.layer1, "6").conv2(layer1_6_relu_post_act_fake_quantizer);  layer1_6_relu_post_act_fake_quantizer = None
    layer1_6_bn2 = getattr(self.layer1, "6").bn2(layer1_6_conv2);  layer1_6_conv2 = None
    add_6 = layer1_6_bn2 + layer1_5_relu_1_post_act_fake_quantizer;  layer1_6_bn2 = layer1_5_relu_1_post_act_fake_quantizer = None
    layer1_6_relu_1 = getattr(self.layer1, "6").relu_dup1(add_6);  add_6 = None
    layer1_6_relu_1_post_act_fake_quantizer = self.layer1_6_relu_1_post_act_fake_quantizer(layer1_6_relu_1);  layer1_6_relu_1 = None
    return layer1_6_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_6_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_7_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_6_relu_1_post_act_fake_quantizer, layer1_7_conv1, layer1_7_bn1, layer1_7_relu, layer1_7_relu_post_act_fake_quantizer, layer1_7_conv2, layer1_7_bn2, add_7, layer1_7_relu_1, layer1_7_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_6_relu_1_post_act_fake_quantizer):
    layer1_7_conv1 = getattr(self.layer1, "7").conv1(layer1_6_relu_1_post_act_fake_quantizer)
    layer1_7_bn1 = getattr(self.layer1, "7").bn1(layer1_7_conv1);  layer1_7_conv1 = None
    layer1_7_relu = getattr(self.layer1, "7").relu(layer1_7_bn1);  layer1_7_bn1 = None
    layer1_7_relu_post_act_fake_quantizer = self.layer1_7_relu_post_act_fake_quantizer(layer1_7_relu);  layer1_7_relu = None
    layer1_7_conv2 = getattr(self.layer1, "7").conv2(layer1_7_relu_post_act_fake_quantizer);  layer1_7_relu_post_act_fake_quantizer = None
    layer1_7_bn2 = getattr(self.layer1, "7").bn2(layer1_7_conv2);  layer1_7_conv2 = None
    add_7 = layer1_7_bn2 + layer1_6_relu_1_post_act_fake_quantizer;  layer1_7_bn2 = layer1_6_relu_1_post_act_fake_quantizer = None
    layer1_7_relu_1 = getattr(self.layer1, "7").relu_dup1(add_7);  add_7 = None
    layer1_7_relu_1_post_act_fake_quantizer = self.layer1_7_relu_1_post_act_fake_quantizer(layer1_7_relu_1);  layer1_7_relu_1 = None
    return layer1_7_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_7_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_8_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_7_relu_1_post_act_fake_quantizer, layer1_8_conv1, layer1_8_bn1, layer1_8_relu, layer1_8_relu_post_act_fake_quantizer, layer1_8_conv2, layer1_8_bn2, add_8, layer1_8_relu_1, layer1_8_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_7_relu_1_post_act_fake_quantizer):
    layer1_8_conv1 = getattr(self.layer1, "8").conv1(layer1_7_relu_1_post_act_fake_quantizer)
    layer1_8_bn1 = getattr(self.layer1, "8").bn1(layer1_8_conv1);  layer1_8_conv1 = None
    layer1_8_relu = getattr(self.layer1, "8").relu(layer1_8_bn1);  layer1_8_bn1 = None
    layer1_8_relu_post_act_fake_quantizer = self.layer1_8_relu_post_act_fake_quantizer(layer1_8_relu);  layer1_8_relu = None
    layer1_8_conv2 = getattr(self.layer1, "8").conv2(layer1_8_relu_post_act_fake_quantizer);  layer1_8_relu_post_act_fake_quantizer = None
    layer1_8_bn2 = getattr(self.layer1, "8").bn2(layer1_8_conv2);  layer1_8_conv2 = None
    add_8 = layer1_8_bn2 + layer1_7_relu_1_post_act_fake_quantizer;  layer1_8_bn2 = layer1_7_relu_1_post_act_fake_quantizer = None
    layer1_8_relu_1 = getattr(self.layer1, "8").relu_dup1(add_8);  add_8 = None
    layer1_8_relu_1_post_act_fake_quantizer = self.layer1_8_relu_1_post_act_fake_quantizer(layer1_8_relu_1);  layer1_8_relu_1 = None
    return layer1_8_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_8_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_0_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_8_relu_1_post_act_fake_quantizer, layer2_0_conv1, layer2_0_bn1, layer2_0_relu, layer2_0_relu_post_act_fake_quantizer, layer2_0_conv2, layer2_0_bn2, layer2_0_downsample_0, layer2_0_downsample_1, add_9, layer2_0_relu_1, layer2_0_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_8_relu_1_post_act_fake_quantizer):
    layer2_0_conv1 = getattr(self.layer2, "0").conv1(layer1_8_relu_1_post_act_fake_quantizer)
    layer2_0_bn1 = getattr(self.layer2, "0").bn1(layer2_0_conv1);  layer2_0_conv1 = None
    layer2_0_relu = getattr(self.layer2, "0").relu(layer2_0_bn1);  layer2_0_bn1 = None
    layer2_0_relu_post_act_fake_quantizer = self.layer2_0_relu_post_act_fake_quantizer(layer2_0_relu);  layer2_0_relu = None
    layer2_0_conv2 = getattr(self.layer2, "0").conv2(layer2_0_relu_post_act_fake_quantizer);  layer2_0_relu_post_act_fake_quantizer = None
    layer2_0_bn2 = getattr(self.layer2, "0").bn2(layer2_0_conv2);  layer2_0_conv2 = None
    layer2_0_downsample_0 = getattr(getattr(self.layer2, "0").downsample, "0")(layer1_8_relu_1_post_act_fake_quantizer);  layer1_8_relu_1_post_act_fake_quantizer = None
    layer2_0_downsample_1 = getattr(getattr(self.layer2, "0").downsample, "1")(layer2_0_downsample_0);  layer2_0_downsample_0 = None
    add_9 = layer2_0_bn2 + layer2_0_downsample_1;  layer2_0_bn2 = layer2_0_downsample_1 = None
    layer2_0_relu_1 = getattr(self.layer2, "0").relu_dup1(add_9);  add_9 = None
    layer2_0_relu_1_post_act_fake_quantizer = self.layer2_0_relu_1_post_act_fake_quantizer(layer2_0_relu_1);  layer2_0_relu_1 = None
    return layer2_0_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_0_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_1_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_0_relu_1_post_act_fake_quantizer, layer2_1_conv1, layer2_1_bn1, layer2_1_relu, layer2_1_relu_post_act_fake_quantizer, layer2_1_conv2, layer2_1_bn2, add_10, layer2_1_relu_1, layer2_1_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_0_relu_1_post_act_fake_quantizer):
    layer2_1_conv1 = getattr(self.layer2, "1").conv1(layer2_0_relu_1_post_act_fake_quantizer)
    layer2_1_bn1 = getattr(self.layer2, "1").bn1(layer2_1_conv1);  layer2_1_conv1 = None
    layer2_1_relu = getattr(self.layer2, "1").relu(layer2_1_bn1);  layer2_1_bn1 = None
    layer2_1_relu_post_act_fake_quantizer = self.layer2_1_relu_post_act_fake_quantizer(layer2_1_relu);  layer2_1_relu = None
    layer2_1_conv2 = getattr(self.layer2, "1").conv2(layer2_1_relu_post_act_fake_quantizer);  layer2_1_relu_post_act_fake_quantizer = None
    layer2_1_bn2 = getattr(self.layer2, "1").bn2(layer2_1_conv2);  layer2_1_conv2 = None
    add_10 = layer2_1_bn2 + layer2_0_relu_1_post_act_fake_quantizer;  layer2_1_bn2 = layer2_0_relu_1_post_act_fake_quantizer = None
    layer2_1_relu_1 = getattr(self.layer2, "1").relu_dup1(add_10);  add_10 = None
    layer2_1_relu_1_post_act_fake_quantizer = self.layer2_1_relu_1_post_act_fake_quantizer(layer2_1_relu_1);  layer2_1_relu_1 = None
    return layer2_1_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_1_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_2_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_1_relu_1_post_act_fake_quantizer, layer2_2_conv1, layer2_2_bn1, layer2_2_relu, layer2_2_relu_post_act_fake_quantizer, layer2_2_conv2, layer2_2_bn2, add_11, layer2_2_relu_1, layer2_2_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_1_relu_1_post_act_fake_quantizer):
    layer2_2_conv1 = getattr(self.layer2, "2").conv1(layer2_1_relu_1_post_act_fake_quantizer)
    layer2_2_bn1 = getattr(self.layer2, "2").bn1(layer2_2_conv1);  layer2_2_conv1 = None
    layer2_2_relu = getattr(self.layer2, "2").relu(layer2_2_bn1);  layer2_2_bn1 = None
    layer2_2_relu_post_act_fake_quantizer = self.layer2_2_relu_post_act_fake_quantizer(layer2_2_relu);  layer2_2_relu = None
    layer2_2_conv2 = getattr(self.layer2, "2").conv2(layer2_2_relu_post_act_fake_quantizer);  layer2_2_relu_post_act_fake_quantizer = None
    layer2_2_bn2 = getattr(self.layer2, "2").bn2(layer2_2_conv2);  layer2_2_conv2 = None
    add_11 = layer2_2_bn2 + layer2_1_relu_1_post_act_fake_quantizer;  layer2_2_bn2 = layer2_1_relu_1_post_act_fake_quantizer = None
    layer2_2_relu_1 = getattr(self.layer2, "2").relu_dup1(add_11);  add_11 = None
    layer2_2_relu_1_post_act_fake_quantizer = self.layer2_2_relu_1_post_act_fake_quantizer(layer2_2_relu_1);  layer2_2_relu_1 = None
    return layer2_2_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_2_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_3_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_2_relu_1_post_act_fake_quantizer, layer2_3_conv1, layer2_3_bn1, layer2_3_relu, layer2_3_relu_post_act_fake_quantizer, layer2_3_conv2, layer2_3_bn2, add_12, layer2_3_relu_1, layer2_3_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_2_relu_1_post_act_fake_quantizer):
    layer2_3_conv1 = getattr(self.layer2, "3").conv1(layer2_2_relu_1_post_act_fake_quantizer)
    layer2_3_bn1 = getattr(self.layer2, "3").bn1(layer2_3_conv1);  layer2_3_conv1 = None
    layer2_3_relu = getattr(self.layer2, "3").relu(layer2_3_bn1);  layer2_3_bn1 = None
    layer2_3_relu_post_act_fake_quantizer = self.layer2_3_relu_post_act_fake_quantizer(layer2_3_relu);  layer2_3_relu = None
    layer2_3_conv2 = getattr(self.layer2, "3").conv2(layer2_3_relu_post_act_fake_quantizer);  layer2_3_relu_post_act_fake_quantizer = None
    layer2_3_bn2 = getattr(self.layer2, "3").bn2(layer2_3_conv2);  layer2_3_conv2 = None
    add_12 = layer2_3_bn2 + layer2_2_relu_1_post_act_fake_quantizer;  layer2_3_bn2 = layer2_2_relu_1_post_act_fake_quantizer = None
    layer2_3_relu_1 = getattr(self.layer2, "3").relu_dup1(add_12);  add_12 = None
    layer2_3_relu_1_post_act_fake_quantizer = self.layer2_3_relu_1_post_act_fake_quantizer(layer2_3_relu_1);  layer2_3_relu_1 = None
    return layer2_3_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_3_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_4_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_3_relu_1_post_act_fake_quantizer, layer2_4_conv1, layer2_4_bn1, layer2_4_relu, layer2_4_relu_post_act_fake_quantizer, layer2_4_conv2, layer2_4_bn2, add_13, layer2_4_relu_1, layer2_4_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_3_relu_1_post_act_fake_quantizer):
    layer2_4_conv1 = getattr(self.layer2, "4").conv1(layer2_3_relu_1_post_act_fake_quantizer)
    layer2_4_bn1 = getattr(self.layer2, "4").bn1(layer2_4_conv1);  layer2_4_conv1 = None
    layer2_4_relu = getattr(self.layer2, "4").relu(layer2_4_bn1);  layer2_4_bn1 = None
    layer2_4_relu_post_act_fake_quantizer = self.layer2_4_relu_post_act_fake_quantizer(layer2_4_relu);  layer2_4_relu = None
    layer2_4_conv2 = getattr(self.layer2, "4").conv2(layer2_4_relu_post_act_fake_quantizer);  layer2_4_relu_post_act_fake_quantizer = None
    layer2_4_bn2 = getattr(self.layer2, "4").bn2(layer2_4_conv2);  layer2_4_conv2 = None
    add_13 = layer2_4_bn2 + layer2_3_relu_1_post_act_fake_quantizer;  layer2_4_bn2 = layer2_3_relu_1_post_act_fake_quantizer = None
    layer2_4_relu_1 = getattr(self.layer2, "4").relu_dup1(add_13);  add_13 = None
    layer2_4_relu_1_post_act_fake_quantizer = self.layer2_4_relu_1_post_act_fake_quantizer(layer2_4_relu_1);  layer2_4_relu_1 = None
    return layer2_4_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_4_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_5_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_4_relu_1_post_act_fake_quantizer, layer2_5_conv1, layer2_5_bn1, layer2_5_relu, layer2_5_relu_post_act_fake_quantizer, layer2_5_conv2, layer2_5_bn2, add_14, layer2_5_relu_1, layer2_5_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_4_relu_1_post_act_fake_quantizer):
    layer2_5_conv1 = getattr(self.layer2, "5").conv1(layer2_4_relu_1_post_act_fake_quantizer)
    layer2_5_bn1 = getattr(self.layer2, "5").bn1(layer2_5_conv1);  layer2_5_conv1 = None
    layer2_5_relu = getattr(self.layer2, "5").relu(layer2_5_bn1);  layer2_5_bn1 = None
    layer2_5_relu_post_act_fake_quantizer = self.layer2_5_relu_post_act_fake_quantizer(layer2_5_relu);  layer2_5_relu = None
    layer2_5_conv2 = getattr(self.layer2, "5").conv2(layer2_5_relu_post_act_fake_quantizer);  layer2_5_relu_post_act_fake_quantizer = None
    layer2_5_bn2 = getattr(self.layer2, "5").bn2(layer2_5_conv2);  layer2_5_conv2 = None
    add_14 = layer2_5_bn2 + layer2_4_relu_1_post_act_fake_quantizer;  layer2_5_bn2 = layer2_4_relu_1_post_act_fake_quantizer = None
    layer2_5_relu_1 = getattr(self.layer2, "5").relu_dup1(add_14);  add_14 = None
    layer2_5_relu_1_post_act_fake_quantizer = self.layer2_5_relu_1_post_act_fake_quantizer(layer2_5_relu_1);  layer2_5_relu_1 = None
    return layer2_5_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_5_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_6_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_5_relu_1_post_act_fake_quantizer, layer2_6_conv1, layer2_6_bn1, layer2_6_relu, layer2_6_relu_post_act_fake_quantizer, layer2_6_conv2, layer2_6_bn2, add_15, layer2_6_relu_1, layer2_6_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_5_relu_1_post_act_fake_quantizer):
    layer2_6_conv1 = getattr(self.layer2, "6").conv1(layer2_5_relu_1_post_act_fake_quantizer)
    layer2_6_bn1 = getattr(self.layer2, "6").bn1(layer2_6_conv1);  layer2_6_conv1 = None
    layer2_6_relu = getattr(self.layer2, "6").relu(layer2_6_bn1);  layer2_6_bn1 = None
    layer2_6_relu_post_act_fake_quantizer = self.layer2_6_relu_post_act_fake_quantizer(layer2_6_relu);  layer2_6_relu = None
    layer2_6_conv2 = getattr(self.layer2, "6").conv2(layer2_6_relu_post_act_fake_quantizer);  layer2_6_relu_post_act_fake_quantizer = None
    layer2_6_bn2 = getattr(self.layer2, "6").bn2(layer2_6_conv2);  layer2_6_conv2 = None
    add_15 = layer2_6_bn2 + layer2_5_relu_1_post_act_fake_quantizer;  layer2_6_bn2 = layer2_5_relu_1_post_act_fake_quantizer = None
    layer2_6_relu_1 = getattr(self.layer2, "6").relu_dup1(add_15);  add_15 = None
    layer2_6_relu_1_post_act_fake_quantizer = self.layer2_6_relu_1_post_act_fake_quantizer(layer2_6_relu_1);  layer2_6_relu_1 = None
    return layer2_6_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_6_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_7_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_6_relu_1_post_act_fake_quantizer, layer2_7_conv1, layer2_7_bn1, layer2_7_relu, layer2_7_relu_post_act_fake_quantizer, layer2_7_conv2, layer2_7_bn2, add_16, layer2_7_relu_1, layer2_7_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_6_relu_1_post_act_fake_quantizer):
    layer2_7_conv1 = getattr(self.layer2, "7").conv1(layer2_6_relu_1_post_act_fake_quantizer)
    layer2_7_bn1 = getattr(self.layer2, "7").bn1(layer2_7_conv1);  layer2_7_conv1 = None
    layer2_7_relu = getattr(self.layer2, "7").relu(layer2_7_bn1);  layer2_7_bn1 = None
    layer2_7_relu_post_act_fake_quantizer = self.layer2_7_relu_post_act_fake_quantizer(layer2_7_relu);  layer2_7_relu = None
    layer2_7_conv2 = getattr(self.layer2, "7").conv2(layer2_7_relu_post_act_fake_quantizer);  layer2_7_relu_post_act_fake_quantizer = None
    layer2_7_bn2 = getattr(self.layer2, "7").bn2(layer2_7_conv2);  layer2_7_conv2 = None
    add_16 = layer2_7_bn2 + layer2_6_relu_1_post_act_fake_quantizer;  layer2_7_bn2 = layer2_6_relu_1_post_act_fake_quantizer = None
    layer2_7_relu_1 = getattr(self.layer2, "7").relu_dup1(add_16);  add_16 = None
    layer2_7_relu_1_post_act_fake_quantizer = self.layer2_7_relu_1_post_act_fake_quantizer(layer2_7_relu_1);  layer2_7_relu_1 = None
    return layer2_7_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_7_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_8_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_7_relu_1_post_act_fake_quantizer, layer2_8_conv1, layer2_8_bn1, layer2_8_relu, layer2_8_relu_post_act_fake_quantizer, layer2_8_conv2, layer2_8_bn2, add_17, layer2_8_relu_1, layer2_8_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_7_relu_1_post_act_fake_quantizer):
    layer2_8_conv1 = getattr(self.layer2, "8").conv1(layer2_7_relu_1_post_act_fake_quantizer)
    layer2_8_bn1 = getattr(self.layer2, "8").bn1(layer2_8_conv1);  layer2_8_conv1 = None
    layer2_8_relu = getattr(self.layer2, "8").relu(layer2_8_bn1);  layer2_8_bn1 = None
    layer2_8_relu_post_act_fake_quantizer = self.layer2_8_relu_post_act_fake_quantizer(layer2_8_relu);  layer2_8_relu = None
    layer2_8_conv2 = getattr(self.layer2, "8").conv2(layer2_8_relu_post_act_fake_quantizer);  layer2_8_relu_post_act_fake_quantizer = None
    layer2_8_bn2 = getattr(self.layer2, "8").bn2(layer2_8_conv2);  layer2_8_conv2 = None
    add_17 = layer2_8_bn2 + layer2_7_relu_1_post_act_fake_quantizer;  layer2_8_bn2 = layer2_7_relu_1_post_act_fake_quantizer = None
    layer2_8_relu_1 = getattr(self.layer2, "8").relu_dup1(add_17);  add_17 = None
    layer2_8_relu_1_post_act_fake_quantizer = self.layer2_8_relu_1_post_act_fake_quantizer(layer2_8_relu_1);  layer2_8_relu_1 = None
    return layer2_8_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_8_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_0_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_8_relu_1_post_act_fake_quantizer, layer3_0_conv1, layer3_0_bn1, layer3_0_relu, layer3_0_relu_post_act_fake_quantizer, layer3_0_conv2, layer3_0_bn2, layer3_0_downsample_0, layer3_0_downsample_1, add_18, layer3_0_relu_1, layer3_0_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_8_relu_1_post_act_fake_quantizer):
    layer3_0_conv1 = getattr(self.layer3, "0").conv1(layer2_8_relu_1_post_act_fake_quantizer)
    layer3_0_bn1 = getattr(self.layer3, "0").bn1(layer3_0_conv1);  layer3_0_conv1 = None
    layer3_0_relu = getattr(self.layer3, "0").relu(layer3_0_bn1);  layer3_0_bn1 = None
    layer3_0_relu_post_act_fake_quantizer = self.layer3_0_relu_post_act_fake_quantizer(layer3_0_relu);  layer3_0_relu = None
    layer3_0_conv2 = getattr(self.layer3, "0").conv2(layer3_0_relu_post_act_fake_quantizer);  layer3_0_relu_post_act_fake_quantizer = None
    layer3_0_bn2 = getattr(self.layer3, "0").bn2(layer3_0_conv2);  layer3_0_conv2 = None
    layer3_0_downsample_0 = getattr(getattr(self.layer3, "0").downsample, "0")(layer2_8_relu_1_post_act_fake_quantizer);  layer2_8_relu_1_post_act_fake_quantizer = None
    layer3_0_downsample_1 = getattr(getattr(self.layer3, "0").downsample, "1")(layer3_0_downsample_0);  layer3_0_downsample_0 = None
    add_18 = layer3_0_bn2 + layer3_0_downsample_1;  layer3_0_bn2 = layer3_0_downsample_1 = None
    layer3_0_relu_1 = getattr(self.layer3, "0").relu_dup1(add_18);  add_18 = None
    layer3_0_relu_1_post_act_fake_quantizer = self.layer3_0_relu_1_post_act_fake_quantizer(layer3_0_relu_1);  layer3_0_relu_1 = None
    return layer3_0_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_0_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_1_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_0_relu_1_post_act_fake_quantizer, layer3_1_conv1, layer3_1_bn1, layer3_1_relu, layer3_1_relu_post_act_fake_quantizer, layer3_1_conv2, layer3_1_bn2, add_19, layer3_1_relu_1, layer3_1_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_0_relu_1_post_act_fake_quantizer):
    layer3_1_conv1 = getattr(self.layer3, "1").conv1(layer3_0_relu_1_post_act_fake_quantizer)
    layer3_1_bn1 = getattr(self.layer3, "1").bn1(layer3_1_conv1);  layer3_1_conv1 = None
    layer3_1_relu = getattr(self.layer3, "1").relu(layer3_1_bn1);  layer3_1_bn1 = None
    layer3_1_relu_post_act_fake_quantizer = self.layer3_1_relu_post_act_fake_quantizer(layer3_1_relu);  layer3_1_relu = None
    layer3_1_conv2 = getattr(self.layer3, "1").conv2(layer3_1_relu_post_act_fake_quantizer);  layer3_1_relu_post_act_fake_quantizer = None
    layer3_1_bn2 = getattr(self.layer3, "1").bn2(layer3_1_conv2);  layer3_1_conv2 = None
    add_19 = layer3_1_bn2 + layer3_0_relu_1_post_act_fake_quantizer;  layer3_1_bn2 = layer3_0_relu_1_post_act_fake_quantizer = None
    layer3_1_relu_1 = getattr(self.layer3, "1").relu_dup1(add_19);  add_19 = None
    layer3_1_relu_1_post_act_fake_quantizer = self.layer3_1_relu_1_post_act_fake_quantizer(layer3_1_relu_1);  layer3_1_relu_1 = None
    return layer3_1_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_1_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_2_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_1_relu_1_post_act_fake_quantizer, layer3_2_conv1, layer3_2_bn1, layer3_2_relu, layer3_2_relu_post_act_fake_quantizer, layer3_2_conv2, layer3_2_bn2, add_20, layer3_2_relu_1, layer3_2_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_1_relu_1_post_act_fake_quantizer):
    layer3_2_conv1 = getattr(self.layer3, "2").conv1(layer3_1_relu_1_post_act_fake_quantizer)
    layer3_2_bn1 = getattr(self.layer3, "2").bn1(layer3_2_conv1);  layer3_2_conv1 = None
    layer3_2_relu = getattr(self.layer3, "2").relu(layer3_2_bn1);  layer3_2_bn1 = None
    layer3_2_relu_post_act_fake_quantizer = self.layer3_2_relu_post_act_fake_quantizer(layer3_2_relu);  layer3_2_relu = None
    layer3_2_conv2 = getattr(self.layer3, "2").conv2(layer3_2_relu_post_act_fake_quantizer);  layer3_2_relu_post_act_fake_quantizer = None
    layer3_2_bn2 = getattr(self.layer3, "2").bn2(layer3_2_conv2);  layer3_2_conv2 = None
    add_20 = layer3_2_bn2 + layer3_1_relu_1_post_act_fake_quantizer;  layer3_2_bn2 = layer3_1_relu_1_post_act_fake_quantizer = None
    layer3_2_relu_1 = getattr(self.layer3, "2").relu_dup1(add_20);  add_20 = None
    layer3_2_relu_1_post_act_fake_quantizer = self.layer3_2_relu_1_post_act_fake_quantizer(layer3_2_relu_1);  layer3_2_relu_1 = None
    return layer3_2_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_2_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_3_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_2_relu_1_post_act_fake_quantizer, layer3_3_conv1, layer3_3_bn1, layer3_3_relu, layer3_3_relu_post_act_fake_quantizer, layer3_3_conv2, layer3_3_bn2, add_21, layer3_3_relu_1, layer3_3_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_2_relu_1_post_act_fake_quantizer):
    layer3_3_conv1 = getattr(self.layer3, "3").conv1(layer3_2_relu_1_post_act_fake_quantizer)
    layer3_3_bn1 = getattr(self.layer3, "3").bn1(layer3_3_conv1);  layer3_3_conv1 = None
    layer3_3_relu = getattr(self.layer3, "3").relu(layer3_3_bn1);  layer3_3_bn1 = None
    layer3_3_relu_post_act_fake_quantizer = self.layer3_3_relu_post_act_fake_quantizer(layer3_3_relu);  layer3_3_relu = None
    layer3_3_conv2 = getattr(self.layer3, "3").conv2(layer3_3_relu_post_act_fake_quantizer);  layer3_3_relu_post_act_fake_quantizer = None
    layer3_3_bn2 = getattr(self.layer3, "3").bn2(layer3_3_conv2);  layer3_3_conv2 = None
    add_21 = layer3_3_bn2 + layer3_2_relu_1_post_act_fake_quantizer;  layer3_3_bn2 = layer3_2_relu_1_post_act_fake_quantizer = None
    layer3_3_relu_1 = getattr(self.layer3, "3").relu_dup1(add_21);  add_21 = None
    layer3_3_relu_1_post_act_fake_quantizer = self.layer3_3_relu_1_post_act_fake_quantizer(layer3_3_relu_1);  layer3_3_relu_1 = None
    return layer3_3_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_3_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_4_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_3_relu_1_post_act_fake_quantizer, layer3_4_conv1, layer3_4_bn1, layer3_4_relu, layer3_4_relu_post_act_fake_quantizer, layer3_4_conv2, layer3_4_bn2, add_22, layer3_4_relu_1, layer3_4_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_3_relu_1_post_act_fake_quantizer):
    layer3_4_conv1 = getattr(self.layer3, "4").conv1(layer3_3_relu_1_post_act_fake_quantizer)
    layer3_4_bn1 = getattr(self.layer3, "4").bn1(layer3_4_conv1);  layer3_4_conv1 = None
    layer3_4_relu = getattr(self.layer3, "4").relu(layer3_4_bn1);  layer3_4_bn1 = None
    layer3_4_relu_post_act_fake_quantizer = self.layer3_4_relu_post_act_fake_quantizer(layer3_4_relu);  layer3_4_relu = None
    layer3_4_conv2 = getattr(self.layer3, "4").conv2(layer3_4_relu_post_act_fake_quantizer);  layer3_4_relu_post_act_fake_quantizer = None
    layer3_4_bn2 = getattr(self.layer3, "4").bn2(layer3_4_conv2);  layer3_4_conv2 = None
    add_22 = layer3_4_bn2 + layer3_3_relu_1_post_act_fake_quantizer;  layer3_4_bn2 = layer3_3_relu_1_post_act_fake_quantizer = None
    layer3_4_relu_1 = getattr(self.layer3, "4").relu_dup1(add_22);  add_22 = None
    layer3_4_relu_1_post_act_fake_quantizer = self.layer3_4_relu_1_post_act_fake_quantizer(layer3_4_relu_1);  layer3_4_relu_1 = None
    return layer3_4_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_4_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_5_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_4_relu_1_post_act_fake_quantizer, layer3_5_conv1, layer3_5_bn1, layer3_5_relu, layer3_5_relu_post_act_fake_quantizer, layer3_5_conv2, layer3_5_bn2, add_23, layer3_5_relu_1, layer3_5_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_4_relu_1_post_act_fake_quantizer):
    layer3_5_conv1 = getattr(self.layer3, "5").conv1(layer3_4_relu_1_post_act_fake_quantizer)
    layer3_5_bn1 = getattr(self.layer3, "5").bn1(layer3_5_conv1);  layer3_5_conv1 = None
    layer3_5_relu = getattr(self.layer3, "5").relu(layer3_5_bn1);  layer3_5_bn1 = None
    layer3_5_relu_post_act_fake_quantizer = self.layer3_5_relu_post_act_fake_quantizer(layer3_5_relu);  layer3_5_relu = None
    layer3_5_conv2 = getattr(self.layer3, "5").conv2(layer3_5_relu_post_act_fake_quantizer);  layer3_5_relu_post_act_fake_quantizer = None
    layer3_5_bn2 = getattr(self.layer3, "5").bn2(layer3_5_conv2);  layer3_5_conv2 = None
    add_23 = layer3_5_bn2 + layer3_4_relu_1_post_act_fake_quantizer;  layer3_5_bn2 = layer3_4_relu_1_post_act_fake_quantizer = None
    layer3_5_relu_1 = getattr(self.layer3, "5").relu_dup1(add_23);  add_23 = None
    layer3_5_relu_1_post_act_fake_quantizer = self.layer3_5_relu_1_post_act_fake_quantizer(layer3_5_relu_1);  layer3_5_relu_1 = None
    return layer3_5_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_5_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_6_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_5_relu_1_post_act_fake_quantizer, layer3_6_conv1, layer3_6_bn1, layer3_6_relu, layer3_6_relu_post_act_fake_quantizer, layer3_6_conv2, layer3_6_bn2, add_24, layer3_6_relu_1, layer3_6_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_5_relu_1_post_act_fake_quantizer):
    layer3_6_conv1 = getattr(self.layer3, "6").conv1(layer3_5_relu_1_post_act_fake_quantizer)
    layer3_6_bn1 = getattr(self.layer3, "6").bn1(layer3_6_conv1);  layer3_6_conv1 = None
    layer3_6_relu = getattr(self.layer3, "6").relu(layer3_6_bn1);  layer3_6_bn1 = None
    layer3_6_relu_post_act_fake_quantizer = self.layer3_6_relu_post_act_fake_quantizer(layer3_6_relu);  layer3_6_relu = None
    layer3_6_conv2 = getattr(self.layer3, "6").conv2(layer3_6_relu_post_act_fake_quantizer);  layer3_6_relu_post_act_fake_quantizer = None
    layer3_6_bn2 = getattr(self.layer3, "6").bn2(layer3_6_conv2);  layer3_6_conv2 = None
    add_24 = layer3_6_bn2 + layer3_5_relu_1_post_act_fake_quantizer;  layer3_6_bn2 = layer3_5_relu_1_post_act_fake_quantizer = None
    layer3_6_relu_1 = getattr(self.layer3, "6").relu_dup1(add_24);  add_24 = None
    layer3_6_relu_1_post_act_fake_quantizer = self.layer3_6_relu_1_post_act_fake_quantizer(layer3_6_relu_1);  layer3_6_relu_1 = None
    return layer3_6_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_6_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_7_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_6_relu_1_post_act_fake_quantizer, layer3_7_conv1, layer3_7_bn1, layer3_7_relu, layer3_7_relu_post_act_fake_quantizer, layer3_7_conv2, layer3_7_bn2, add_25, layer3_7_relu_1, layer3_7_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_6_relu_1_post_act_fake_quantizer):
    layer3_7_conv1 = getattr(self.layer3, "7").conv1(layer3_6_relu_1_post_act_fake_quantizer)
    layer3_7_bn1 = getattr(self.layer3, "7").bn1(layer3_7_conv1);  layer3_7_conv1 = None
    layer3_7_relu = getattr(self.layer3, "7").relu(layer3_7_bn1);  layer3_7_bn1 = None
    layer3_7_relu_post_act_fake_quantizer = self.layer3_7_relu_post_act_fake_quantizer(layer3_7_relu);  layer3_7_relu = None
    layer3_7_conv2 = getattr(self.layer3, "7").conv2(layer3_7_relu_post_act_fake_quantizer);  layer3_7_relu_post_act_fake_quantizer = None
    layer3_7_bn2 = getattr(self.layer3, "7").bn2(layer3_7_conv2);  layer3_7_conv2 = None
    add_25 = layer3_7_bn2 + layer3_6_relu_1_post_act_fake_quantizer;  layer3_7_bn2 = layer3_6_relu_1_post_act_fake_quantizer = None
    layer3_7_relu_1 = getattr(self.layer3, "7").relu_dup1(add_25);  add_25 = None
    layer3_7_relu_1_post_act_fake_quantizer = self.layer3_7_relu_1_post_act_fake_quantizer(layer3_7_relu_1);  layer3_7_relu_1 = None
    return layer3_7_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_7_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_8_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_7_relu_1_post_act_fake_quantizer, layer3_8_conv1, layer3_8_bn1, layer3_8_relu, layer3_8_relu_post_act_fake_quantizer, layer3_8_conv2, layer3_8_bn2, add_26, layer3_8_relu_1, avgpool, size, view, view_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_7_relu_1_post_act_fake_quantizer):
    layer3_8_conv1 = getattr(self.layer3, "8").conv1(layer3_7_relu_1_post_act_fake_quantizer)
    layer3_8_bn1 = getattr(self.layer3, "8").bn1(layer3_8_conv1);  layer3_8_conv1 = None
    layer3_8_relu = getattr(self.layer3, "8").relu(layer3_8_bn1);  layer3_8_bn1 = None
    layer3_8_relu_post_act_fake_quantizer = self.layer3_8_relu_post_act_fake_quantizer(layer3_8_relu);  layer3_8_relu = None
    layer3_8_conv2 = getattr(self.layer3, "8").conv2(layer3_8_relu_post_act_fake_quantizer);  layer3_8_relu_post_act_fake_quantizer = None
    layer3_8_bn2 = getattr(self.layer3, "8").bn2(layer3_8_conv2);  layer3_8_conv2 = None
    add_26 = layer3_8_bn2 + layer3_7_relu_1_post_act_fake_quantizer;  layer3_8_bn2 = layer3_7_relu_1_post_act_fake_quantizer = None
    layer3_8_relu_1 = getattr(self.layer3, "8").relu_dup1(add_26);  add_26 = None
    avgpool = self.avgpool(layer3_8_relu_1);  layer3_8_relu_1 = None
    size = avgpool.size(0)
    view = avgpool.view(size, -1);  avgpool = size = None
    view_post_act_fake_quantizer = self.view_post_act_fake_quantizer(view);  view = None
    return view_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for view_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for fc
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [view_post_act_fake_quantizer, fc]
[MQBENCH] INFO: 


def forward(self, view_post_act_fake_quantizer):
    fc = self.fc(view_post_act_fake_quantizer);  view_post_act_fake_quantizer = None
    return fc
    
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node x_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_0_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_0_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_1_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_1_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_2_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_2_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_3_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_3_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_4_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_4_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_5_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_5_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_6_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_6_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_7_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_7_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_8_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_8_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_0_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.downsample.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.downsample.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_0_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_1_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_1_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_2_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_2_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_3_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_3_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_4_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_4_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_5_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_5_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_6_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_6_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_7_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_7_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_8_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_8_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_0_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.downsample.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.downsample.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_0_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_1_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_1_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_2_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_2_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_3_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_3_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_4_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_4_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_5_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_5_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_6_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_6_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_7_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_7_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_8_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node avgpool in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node view_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node fc in quant
QDROP model already saved, now loading QDROP_8bits_cifar100_resnet56.pt
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
evaluate mqb quantized model
{'mean_acc': 0.3691, 'qtl_acc': 0.3691, 'mean_loss': 2.938320410402515, 'qtl_loss': 2.938320410402515, 'test time': 4.501008987426758, 'acc_list': array([0.3691]), 'loss_list': array([2.93832041])}
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
evaluate mqb quantized model
{'mean_acc': 0.6835, 'qtl_acc': 0.6835, 'mean_loss': 1.4832341354104537, 'qtl_loss': 1.4832341354104537, 'test time': 4.501655101776123, 'acc_list': array([0.6835]), 'loss_list': array([1.48323414])}
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
evaluate mqb quantized model
{'mean_acc': 0.7235, 'qtl_acc': 0.7235, 'mean_loss': 1.2972044356261627, 'qtl_loss': 1.2972044356261627, 'test time': 4.453027248382568, 'acc_list': array([0.7235]), 'loss_list': array([1.29720444])}
[MQBENCH] INFO: Disable observer and Disable quantize.
input of  conv1  is  x_post_act_fake_quantizer
input of  layer1.0.conv1  is  relu_post_act_fake_quantizer
input of  layer1.0.conv2  is  layer1_0_relu_post_act_fake_quantizer
input of  layer1.1.conv1  is  layer1_0_relu_1_post_act_fake_quantizer
input of  layer1.1.conv2  is  layer1_1_relu_post_act_fake_quantizer
input of  layer1.2.conv1  is  layer1_1_relu_1_post_act_fake_quantizer
input of  layer1.2.conv2  is  layer1_2_relu_post_act_fake_quantizer
input of  layer1.3.conv1  is  layer1_2_relu_1_post_act_fake_quantizer
input of  layer1.3.conv2  is  layer1_3_relu_post_act_fake_quantizer
input of  layer1.4.conv1  is  layer1_3_relu_1_post_act_fake_quantizer
input of  layer1.4.conv2  is  layer1_4_relu_post_act_fake_quantizer
input of  layer1.5.conv1  is  layer1_4_relu_1_post_act_fake_quantizer
input of  layer1.5.conv2  is  layer1_5_relu_post_act_fake_quantizer
input of  layer1.6.conv1  is  layer1_5_relu_1_post_act_fake_quantizer
input of  layer1.6.conv2  is  layer1_6_relu_post_act_fake_quantizer
input of  layer1.7.conv1  is  layer1_6_relu_1_post_act_fake_quantizer
input of  layer1.7.conv2  is  layer1_7_relu_post_act_fake_quantizer
input of  layer1.8.conv1  is  layer1_7_relu_1_post_act_fake_quantizer
input of  layer1.8.conv2  is  layer1_8_relu_post_act_fake_quantizer
input of  layer2.0.conv1  is  layer1_8_relu_1_post_act_fake_quantizer
input of  layer2.0.conv2  is  layer2_0_relu_post_act_fake_quantizer
input of  layer2.0.downsample.0  is  layer1_8_relu_1_post_act_fake_quantizer
input of  layer2.1.conv1  is  layer2_0_relu_1_post_act_fake_quantizer
input of  layer2.1.conv2  is  layer2_1_relu_post_act_fake_quantizer
input of  layer2.2.conv1  is  layer2_1_relu_1_post_act_fake_quantizer
input of  layer2.2.conv2  is  layer2_2_relu_post_act_fake_quantizer
input of  layer2.3.conv1  is  layer2_2_relu_1_post_act_fake_quantizer
input of  layer2.3.conv2  is  layer2_3_relu_post_act_fake_quantizer
input of  layer2.4.conv1  is  layer2_3_relu_1_post_act_fake_quantizer
input of  layer2.4.conv2  is  layer2_4_relu_post_act_fake_quantizer
input of  layer2.5.conv1  is  layer2_4_relu_1_post_act_fake_quantizer
input of  layer2.5.conv2  is  layer2_5_relu_post_act_fake_quantizer
input of  layer2.6.conv1  is  layer2_5_relu_1_post_act_fake_quantizer
input of  layer2.6.conv2  is  layer2_6_relu_post_act_fake_quantizer
input of  layer2.7.conv1  is  layer2_6_relu_1_post_act_fake_quantizer
input of  layer2.7.conv2  is  layer2_7_relu_post_act_fake_quantizer
input of  layer2.8.conv1  is  layer2_7_relu_1_post_act_fake_quantizer
input of  layer2.8.conv2  is  layer2_8_relu_post_act_fake_quantizer
input of  layer3.0.conv1  is  layer2_8_relu_1_post_act_fake_quantizer
input of  layer3.0.conv2  is  layer3_0_relu_post_act_fake_quantizer
input of  layer3.0.downsample.0  is  layer2_8_relu_1_post_act_fake_quantizer
input of  layer3.1.conv1  is  layer3_0_relu_1_post_act_fake_quantizer
input of  layer3.1.conv2  is  layer3_1_relu_post_act_fake_quantizer
input of  layer3.2.conv1  is  layer3_1_relu_1_post_act_fake_quantizer
input of  layer3.2.conv2  is  layer3_2_relu_post_act_fake_quantizer
input of  layer3.3.conv1  is  layer3_2_relu_1_post_act_fake_quantizer
input of  layer3.3.conv2  is  layer3_3_relu_post_act_fake_quantizer
input of  layer3.4.conv1  is  layer3_3_relu_1_post_act_fake_quantizer
input of  layer3.4.conv2  is  layer3_4_relu_post_act_fake_quantizer
input of  layer3.5.conv1  is  layer3_4_relu_1_post_act_fake_quantizer
input of  layer3.5.conv2  is  layer3_5_relu_post_act_fake_quantizer
input of  layer3.6.conv1  is  layer3_5_relu_1_post_act_fake_quantizer
input of  layer3.6.conv2  is  layer3_6_relu_post_act_fake_quantizer
input of  layer3.7.conv1  is  layer3_6_relu_1_post_act_fake_quantizer
input of  layer3.7.conv2  is  layer3_7_relu_post_act_fake_quantizer
input of  layer3.8.conv1  is  layer3_7_relu_1_post_act_fake_quantizer
input of  layer3.8.conv2  is  layer3_8_relu_post_act_fake_quantizer
input of  fc  is  view_post_act_fake_quantizer
----------Gradient Caching----------
perturb layer layer1.0.conv1 to A2W2 p=2.0677347034215927
perturb layer layer1.0.conv1 to A2W4 p=1.2532820105552673
perturb layer layer1.0.conv1 to A2W8 p=1.2261190861463547
perturb layer layer1.0.conv1 to A4W2 p=0.9557577967643738
perturb layer layer1.0.conv1 to A4W4 p=0.1982286050915718
perturb layer layer1.0.conv1 to A4W8 p=0.21228326112031937
perturb layer layer1.0.conv1 to A8W2 p=0.7373148500919342
perturb layer layer1.0.conv1 to A8W4 p=-0.01243128627538681
perturb layer layer1.0.conv1 to A8W8 p=0.0032554790377616882
perturb layer layer1.0.conv1 to A2W2 and layer layer1.0.conv2 to A2W2 p=2.721445605158806
perturb layer layer1.0.conv1 to A2W2 and layer layer1.0.conv2 to A2W4 p=2.2810195833444595
perturb layer layer1.0.conv1 to A2W2 and layer layer1.0.conv2 to A2W8 p=2.381844088435173
perturb layer layer1.0.conv1 to A2W2 and layer layer1.0.conv2 to A4W2 p=2.4829693883657455
perturb layer layer1.0.conv1 to A2W2 and layer layer1.0.conv2 to A4W4 p=2.018380895256996
perturb layer layer1.0.conv1 to A2W2 and layer layer1.0.conv2 to A4W8 p=2.106217637658119
perturb layer layer1.0.conv1 to A2W2 and layer layer1.0.conv2 to A8W2 p=2.4513194113969803
perturb layer layer1.0.conv1 to A2W2 and layer layer1.0.conv2 to A8W4 p=1.9813603609800339
perturb layer layer1.0.conv1 to A2W2 and layer layer1.0.conv2 to A8W8 p=2.0661797672510147
perturb layer layer1.0.conv1 to A2W4 and layer layer1.0.conv2 to A2W2 p=1.867751196026802
perturb layer layer1.0.conv1 to A2W4 and layer layer1.0.conv2 to A2W4 p=1.5295309871435165
perturb layer layer1.0.conv1 to A2W4 and layer layer1.0.conv2 to A2W8 p=1.5955729335546494
perturb layer layer1.0.conv1 to A2W4 and layer layer1.0.conv2 to A4W2 p=1.6538758426904678
perturb layer layer1.0.conv1 to A2W4 and layer layer1.0.conv2 to A4W4 p=1.2519487142562866
Traceback (most recent call last):
  File "/homes/zdeng/sum2022/CLADO_MPQ/clado_gc.py", line 258, in <module>
    p = perturb_loss({n:naw,m:maw},ref_metric,calib_data,KL=KL)
  File "/homes/zdeng/sum2022/CLADO_MPQ/clado_gc.py", line 210, in perturb_loss
    res = evaluate(eval_data,mqb_mix_model)
  File "/homes/zdeng/sum2022/CLADO_MPQ/utils/util.py", line 97, in evaluate
    t_outputs = model(t_images)
  File "/homes/zdeng/.conda/envs/mltls/lib/python3.9/site-packages/torch/fx/graph_module.py", line 608, in wrapped_call
    return super(type(self), self).__call__(*args, **kwargs)
  File "/homes/zdeng/.conda/envs/mltls/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "<eval_with_key>.123", line 239, in forward
  File "/homes/zdeng/.conda/envs/mltls/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/homes/zdeng/.conda/envs/mltls/lib/python3.9/site-packages/torch/nn/qat/modules/conv.py", line 37, in forward
    return self._conv_forward(input, self.weight_fake_quant(self.weight), self.bias)
  File "/homes/zdeng/.conda/envs/mltls/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/homes/zdeng/.conda/envs/mltls/lib/python3.9/site-packages/MQBench-0.0.6-py3.9.egg/mqbench/fake_quantize/adaround_quantizer.py", line 108, in forward
    if self.fake_quant_enabled[0] == 1:
KeyboardInterrupt
