Using cache found in /homes/zdeng/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master
Files already downloaded and verified
Files already downloaded and verified
fp32 model: {'mean_acc': 0.7263, 'qtl_acc': 0.7263, 'mean_loss': 1.2951814660543128, 'qtl_loss': 1.2951814660543128, 'test time': 2.2678604125976562, 'acc_list': array([0.7263]), 'loss_list': array([1.29518147])}
Prepare 2bits model using MQBench
[MQBENCH] INFO: Quantize model Scheme: BackendType.Academic Mode: Eval
[MQBENCH] INFO: Weight Qconfig:
    FakeQuantize: AdaRoundFakeQuantize Params: {}
    Oberver:      MSEObserver Params: Symmetric: True / Bitwidth: 2 / Per channel: False / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Activation Qconfig:
    FakeQuantize: QDropFakeQuantize Params: {}
    Oberver:      EMAMSEObserver Params: Symmetric: False / Bitwidth: 2 / Per channel: False / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Replace module to qat module.
[MQBENCH] INFO: Set layer conv1 to 8 bit.
[MQBENCH] INFO: Set layer fc to 8 bit.
dbg node_to_quantize_output
 odict_keys([x, relu, layer1_0_relu, layer1_0_relu_1, layer1_1_relu, layer1_1_relu_1, layer1_2_relu, layer1_2_relu_1, layer1_3_relu, layer1_3_relu_1, layer1_4_relu, layer1_4_relu_1, layer1_5_relu, layer1_5_relu_1, layer1_6_relu, layer1_6_relu_1, layer1_7_relu, layer1_7_relu_1, layer1_8_relu, layer1_8_relu_1, layer2_0_relu, layer2_0_relu_1, layer2_1_relu, layer2_1_relu_1, layer2_2_relu, layer2_2_relu_1, layer2_3_relu, layer2_3_relu_1, layer2_4_relu, layer2_4_relu_1, layer2_5_relu, layer2_5_relu_1, layer2_6_relu, layer2_6_relu_1, layer2_7_relu, layer2_7_relu_1, layer2_8_relu, layer2_8_relu_1, layer3_0_relu, layer3_0_relu_1, layer3_1_relu, layer3_1_relu_1, layer3_2_relu, layer3_2_relu_1, layer3_3_relu, layer3_3_relu_1, layer3_4_relu, layer3_4_relu_1, layer3_5_relu, layer3_5_relu_1, layer3_6_relu, layer3_6_relu_1, layer3_7_relu, layer3_7_relu_1, layer3_8_relu, view])
[MQBENCH] INFO: Set x post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant x_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_4_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_5_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_6_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_7_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_8_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_4_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_5_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_6_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_7_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_8_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_4_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_5_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_6_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_7_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: Set view post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant view_post_act_fake_quantizer
[MQBENCH] INFO: Enable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: prepare block reconstruction for conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [x_post_act_fake_quantizer, conv1, bn1, relu, relu_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, x_post_act_fake_quantizer):
    conv1 = self.conv1(x_post_act_fake_quantizer);  x_post_act_fake_quantizer = None
    bn1 = self.bn1(conv1);  conv1 = None
    relu = self.relu(bn1);  bn1 = None
    relu_post_act_fake_quantizer = self.relu_post_act_fake_quantizer(relu);  relu = None
    return relu_post_act_fake_quantizer
    
Init alpha to be FP32
[MQBENCH] INFO: learn the scale for relu_post_act_fake_quantizer
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_0_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [relu_post_act_fake_quantizer, layer1_0_conv1, layer1_0_bn1, layer1_0_relu, layer1_0_relu_post_act_fake_quantizer, layer1_0_conv2, layer1_0_bn2, add, layer1_0_relu_1, layer1_0_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, relu_post_act_fake_quantizer):
    layer1_0_conv1 = getattr(self.layer1, "0").conv1(relu_post_act_fake_quantizer)
    layer1_0_bn1 = getattr(self.layer1, "0").bn1(layer1_0_conv1);  layer1_0_conv1 = None
    layer1_0_relu = getattr(self.layer1, "0").relu(layer1_0_bn1);  layer1_0_bn1 = None
    layer1_0_relu_post_act_fake_quantizer = self.layer1_0_relu_post_act_fake_quantizer(layer1_0_relu);  layer1_0_relu = None
    layer1_0_conv2 = getattr(self.layer1, "0").conv2(layer1_0_relu_post_act_fake_quantizer);  layer1_0_relu_post_act_fake_quantizer = None
    layer1_0_bn2 = getattr(self.layer1, "0").bn2(layer1_0_conv2);  layer1_0_conv2 = None
    add = layer1_0_bn2 + relu_post_act_fake_quantizer;  layer1_0_bn2 = relu_post_act_fake_quantizer = None
    layer1_0_relu_1 = getattr(self.layer1, "0").relu_dup1(add);  add = None
    layer1_0_relu_1_post_act_fake_quantizer = self.layer1_0_relu_1_post_act_fake_quantizer(layer1_0_relu_1);  layer1_0_relu_1 = None
    return layer1_0_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_0_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_1_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_0_relu_1_post_act_fake_quantizer, layer1_1_conv1, layer1_1_bn1, layer1_1_relu, layer1_1_relu_post_act_fake_quantizer, layer1_1_conv2, layer1_1_bn2, add_1, layer1_1_relu_1, layer1_1_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_0_relu_1_post_act_fake_quantizer):
    layer1_1_conv1 = getattr(self.layer1, "1").conv1(layer1_0_relu_1_post_act_fake_quantizer)
    layer1_1_bn1 = getattr(self.layer1, "1").bn1(layer1_1_conv1);  layer1_1_conv1 = None
    layer1_1_relu = getattr(self.layer1, "1").relu(layer1_1_bn1);  layer1_1_bn1 = None
    layer1_1_relu_post_act_fake_quantizer = self.layer1_1_relu_post_act_fake_quantizer(layer1_1_relu);  layer1_1_relu = None
    layer1_1_conv2 = getattr(self.layer1, "1").conv2(layer1_1_relu_post_act_fake_quantizer);  layer1_1_relu_post_act_fake_quantizer = None
    layer1_1_bn2 = getattr(self.layer1, "1").bn2(layer1_1_conv2);  layer1_1_conv2 = None
    add_1 = layer1_1_bn2 + layer1_0_relu_1_post_act_fake_quantizer;  layer1_1_bn2 = layer1_0_relu_1_post_act_fake_quantizer = None
    layer1_1_relu_1 = getattr(self.layer1, "1").relu_dup1(add_1);  add_1 = None
    layer1_1_relu_1_post_act_fake_quantizer = self.layer1_1_relu_1_post_act_fake_quantizer(layer1_1_relu_1);  layer1_1_relu_1 = None
    return layer1_1_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_1_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_2_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_1_relu_1_post_act_fake_quantizer, layer1_2_conv1, layer1_2_bn1, layer1_2_relu, layer1_2_relu_post_act_fake_quantizer, layer1_2_conv2, layer1_2_bn2, add_2, layer1_2_relu_1, layer1_2_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_1_relu_1_post_act_fake_quantizer):
    layer1_2_conv1 = getattr(self.layer1, "2").conv1(layer1_1_relu_1_post_act_fake_quantizer)
    layer1_2_bn1 = getattr(self.layer1, "2").bn1(layer1_2_conv1);  layer1_2_conv1 = None
    layer1_2_relu = getattr(self.layer1, "2").relu(layer1_2_bn1);  layer1_2_bn1 = None
    layer1_2_relu_post_act_fake_quantizer = self.layer1_2_relu_post_act_fake_quantizer(layer1_2_relu);  layer1_2_relu = None
    layer1_2_conv2 = getattr(self.layer1, "2").conv2(layer1_2_relu_post_act_fake_quantizer);  layer1_2_relu_post_act_fake_quantizer = None
    layer1_2_bn2 = getattr(self.layer1, "2").bn2(layer1_2_conv2);  layer1_2_conv2 = None
    add_2 = layer1_2_bn2 + layer1_1_relu_1_post_act_fake_quantizer;  layer1_2_bn2 = layer1_1_relu_1_post_act_fake_quantizer = None
    layer1_2_relu_1 = getattr(self.layer1, "2").relu_dup1(add_2);  add_2 = None
    layer1_2_relu_1_post_act_fake_quantizer = self.layer1_2_relu_1_post_act_fake_quantizer(layer1_2_relu_1);  layer1_2_relu_1 = None
    return layer1_2_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_2_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_3_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_2_relu_1_post_act_fake_quantizer, layer1_3_conv1, layer1_3_bn1, layer1_3_relu, layer1_3_relu_post_act_fake_quantizer, layer1_3_conv2, layer1_3_bn2, add_3, layer1_3_relu_1, layer1_3_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_2_relu_1_post_act_fake_quantizer):
    layer1_3_conv1 = getattr(self.layer1, "3").conv1(layer1_2_relu_1_post_act_fake_quantizer)
    layer1_3_bn1 = getattr(self.layer1, "3").bn1(layer1_3_conv1);  layer1_3_conv1 = None
    layer1_3_relu = getattr(self.layer1, "3").relu(layer1_3_bn1);  layer1_3_bn1 = None
    layer1_3_relu_post_act_fake_quantizer = self.layer1_3_relu_post_act_fake_quantizer(layer1_3_relu);  layer1_3_relu = None
    layer1_3_conv2 = getattr(self.layer1, "3").conv2(layer1_3_relu_post_act_fake_quantizer);  layer1_3_relu_post_act_fake_quantizer = None
    layer1_3_bn2 = getattr(self.layer1, "3").bn2(layer1_3_conv2);  layer1_3_conv2 = None
    add_3 = layer1_3_bn2 + layer1_2_relu_1_post_act_fake_quantizer;  layer1_3_bn2 = layer1_2_relu_1_post_act_fake_quantizer = None
    layer1_3_relu_1 = getattr(self.layer1, "3").relu_dup1(add_3);  add_3 = None
    layer1_3_relu_1_post_act_fake_quantizer = self.layer1_3_relu_1_post_act_fake_quantizer(layer1_3_relu_1);  layer1_3_relu_1 = None
    return layer1_3_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_3_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_4_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_3_relu_1_post_act_fake_quantizer, layer1_4_conv1, layer1_4_bn1, layer1_4_relu, layer1_4_relu_post_act_fake_quantizer, layer1_4_conv2, layer1_4_bn2, add_4, layer1_4_relu_1, layer1_4_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_3_relu_1_post_act_fake_quantizer):
    layer1_4_conv1 = getattr(self.layer1, "4").conv1(layer1_3_relu_1_post_act_fake_quantizer)
    layer1_4_bn1 = getattr(self.layer1, "4").bn1(layer1_4_conv1);  layer1_4_conv1 = None
    layer1_4_relu = getattr(self.layer1, "4").relu(layer1_4_bn1);  layer1_4_bn1 = None
    layer1_4_relu_post_act_fake_quantizer = self.layer1_4_relu_post_act_fake_quantizer(layer1_4_relu);  layer1_4_relu = None
    layer1_4_conv2 = getattr(self.layer1, "4").conv2(layer1_4_relu_post_act_fake_quantizer);  layer1_4_relu_post_act_fake_quantizer = None
    layer1_4_bn2 = getattr(self.layer1, "4").bn2(layer1_4_conv2);  layer1_4_conv2 = None
    add_4 = layer1_4_bn2 + layer1_3_relu_1_post_act_fake_quantizer;  layer1_4_bn2 = layer1_3_relu_1_post_act_fake_quantizer = None
    layer1_4_relu_1 = getattr(self.layer1, "4").relu_dup1(add_4);  add_4 = None
    layer1_4_relu_1_post_act_fake_quantizer = self.layer1_4_relu_1_post_act_fake_quantizer(layer1_4_relu_1);  layer1_4_relu_1 = None
    return layer1_4_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_4_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_5_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_4_relu_1_post_act_fake_quantizer, layer1_5_conv1, layer1_5_bn1, layer1_5_relu, layer1_5_relu_post_act_fake_quantizer, layer1_5_conv2, layer1_5_bn2, add_5, layer1_5_relu_1, layer1_5_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_4_relu_1_post_act_fake_quantizer):
    layer1_5_conv1 = getattr(self.layer1, "5").conv1(layer1_4_relu_1_post_act_fake_quantizer)
    layer1_5_bn1 = getattr(self.layer1, "5").bn1(layer1_5_conv1);  layer1_5_conv1 = None
    layer1_5_relu = getattr(self.layer1, "5").relu(layer1_5_bn1);  layer1_5_bn1 = None
    layer1_5_relu_post_act_fake_quantizer = self.layer1_5_relu_post_act_fake_quantizer(layer1_5_relu);  layer1_5_relu = None
    layer1_5_conv2 = getattr(self.layer1, "5").conv2(layer1_5_relu_post_act_fake_quantizer);  layer1_5_relu_post_act_fake_quantizer = None
    layer1_5_bn2 = getattr(self.layer1, "5").bn2(layer1_5_conv2);  layer1_5_conv2 = None
    add_5 = layer1_5_bn2 + layer1_4_relu_1_post_act_fake_quantizer;  layer1_5_bn2 = layer1_4_relu_1_post_act_fake_quantizer = None
    layer1_5_relu_1 = getattr(self.layer1, "5").relu_dup1(add_5);  add_5 = None
    layer1_5_relu_1_post_act_fake_quantizer = self.layer1_5_relu_1_post_act_fake_quantizer(layer1_5_relu_1);  layer1_5_relu_1 = None
    return layer1_5_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_5_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_6_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_5_relu_1_post_act_fake_quantizer, layer1_6_conv1, layer1_6_bn1, layer1_6_relu, layer1_6_relu_post_act_fake_quantizer, layer1_6_conv2, layer1_6_bn2, add_6, layer1_6_relu_1, layer1_6_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_5_relu_1_post_act_fake_quantizer):
    layer1_6_conv1 = getattr(self.layer1, "6").conv1(layer1_5_relu_1_post_act_fake_quantizer)
    layer1_6_bn1 = getattr(self.layer1, "6").bn1(layer1_6_conv1);  layer1_6_conv1 = None
    layer1_6_relu = getattr(self.layer1, "6").relu(layer1_6_bn1);  layer1_6_bn1 = None
    layer1_6_relu_post_act_fake_quantizer = self.layer1_6_relu_post_act_fake_quantizer(layer1_6_relu);  layer1_6_relu = None
    layer1_6_conv2 = getattr(self.layer1, "6").conv2(layer1_6_relu_post_act_fake_quantizer);  layer1_6_relu_post_act_fake_quantizer = None
    layer1_6_bn2 = getattr(self.layer1, "6").bn2(layer1_6_conv2);  layer1_6_conv2 = None
    add_6 = layer1_6_bn2 + layer1_5_relu_1_post_act_fake_quantizer;  layer1_6_bn2 = layer1_5_relu_1_post_act_fake_quantizer = None
    layer1_6_relu_1 = getattr(self.layer1, "6").relu_dup1(add_6);  add_6 = None
    layer1_6_relu_1_post_act_fake_quantizer = self.layer1_6_relu_1_post_act_fake_quantizer(layer1_6_relu_1);  layer1_6_relu_1 = None
    return layer1_6_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_6_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_7_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_6_relu_1_post_act_fake_quantizer, layer1_7_conv1, layer1_7_bn1, layer1_7_relu, layer1_7_relu_post_act_fake_quantizer, layer1_7_conv2, layer1_7_bn2, add_7, layer1_7_relu_1, layer1_7_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_6_relu_1_post_act_fake_quantizer):
    layer1_7_conv1 = getattr(self.layer1, "7").conv1(layer1_6_relu_1_post_act_fake_quantizer)
    layer1_7_bn1 = getattr(self.layer1, "7").bn1(layer1_7_conv1);  layer1_7_conv1 = None
    layer1_7_relu = getattr(self.layer1, "7").relu(layer1_7_bn1);  layer1_7_bn1 = None
    layer1_7_relu_post_act_fake_quantizer = self.layer1_7_relu_post_act_fake_quantizer(layer1_7_relu);  layer1_7_relu = None
    layer1_7_conv2 = getattr(self.layer1, "7").conv2(layer1_7_relu_post_act_fake_quantizer);  layer1_7_relu_post_act_fake_quantizer = None
    layer1_7_bn2 = getattr(self.layer1, "7").bn2(layer1_7_conv2);  layer1_7_conv2 = None
    add_7 = layer1_7_bn2 + layer1_6_relu_1_post_act_fake_quantizer;  layer1_7_bn2 = layer1_6_relu_1_post_act_fake_quantizer = None
    layer1_7_relu_1 = getattr(self.layer1, "7").relu_dup1(add_7);  add_7 = None
    layer1_7_relu_1_post_act_fake_quantizer = self.layer1_7_relu_1_post_act_fake_quantizer(layer1_7_relu_1);  layer1_7_relu_1 = None
    return layer1_7_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_7_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_8_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_7_relu_1_post_act_fake_quantizer, layer1_8_conv1, layer1_8_bn1, layer1_8_relu, layer1_8_relu_post_act_fake_quantizer, layer1_8_conv2, layer1_8_bn2, add_8, layer1_8_relu_1, layer1_8_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_7_relu_1_post_act_fake_quantizer):
    layer1_8_conv1 = getattr(self.layer1, "8").conv1(layer1_7_relu_1_post_act_fake_quantizer)
    layer1_8_bn1 = getattr(self.layer1, "8").bn1(layer1_8_conv1);  layer1_8_conv1 = None
    layer1_8_relu = getattr(self.layer1, "8").relu(layer1_8_bn1);  layer1_8_bn1 = None
    layer1_8_relu_post_act_fake_quantizer = self.layer1_8_relu_post_act_fake_quantizer(layer1_8_relu);  layer1_8_relu = None
    layer1_8_conv2 = getattr(self.layer1, "8").conv2(layer1_8_relu_post_act_fake_quantizer);  layer1_8_relu_post_act_fake_quantizer = None
    layer1_8_bn2 = getattr(self.layer1, "8").bn2(layer1_8_conv2);  layer1_8_conv2 = None
    add_8 = layer1_8_bn2 + layer1_7_relu_1_post_act_fake_quantizer;  layer1_8_bn2 = layer1_7_relu_1_post_act_fake_quantizer = None
    layer1_8_relu_1 = getattr(self.layer1, "8").relu_dup1(add_8);  add_8 = None
    layer1_8_relu_1_post_act_fake_quantizer = self.layer1_8_relu_1_post_act_fake_quantizer(layer1_8_relu_1);  layer1_8_relu_1 = None
    return layer1_8_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_8_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_0_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_8_relu_1_post_act_fake_quantizer, layer2_0_conv1, layer2_0_bn1, layer2_0_relu, layer2_0_relu_post_act_fake_quantizer, layer2_0_conv2, layer2_0_bn2, layer2_0_downsample_0, layer2_0_downsample_1, add_9, layer2_0_relu_1, layer2_0_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_8_relu_1_post_act_fake_quantizer):
    layer2_0_conv1 = getattr(self.layer2, "0").conv1(layer1_8_relu_1_post_act_fake_quantizer)
    layer2_0_bn1 = getattr(self.layer2, "0").bn1(layer2_0_conv1);  layer2_0_conv1 = None
    layer2_0_relu = getattr(self.layer2, "0").relu(layer2_0_bn1);  layer2_0_bn1 = None
    layer2_0_relu_post_act_fake_quantizer = self.layer2_0_relu_post_act_fake_quantizer(layer2_0_relu);  layer2_0_relu = None
    layer2_0_conv2 = getattr(self.layer2, "0").conv2(layer2_0_relu_post_act_fake_quantizer);  layer2_0_relu_post_act_fake_quantizer = None
    layer2_0_bn2 = getattr(self.layer2, "0").bn2(layer2_0_conv2);  layer2_0_conv2 = None
    layer2_0_downsample_0 = getattr(getattr(self.layer2, "0").downsample, "0")(layer1_8_relu_1_post_act_fake_quantizer);  layer1_8_relu_1_post_act_fake_quantizer = None
    layer2_0_downsample_1 = getattr(getattr(self.layer2, "0").downsample, "1")(layer2_0_downsample_0);  layer2_0_downsample_0 = None
    add_9 = layer2_0_bn2 + layer2_0_downsample_1;  layer2_0_bn2 = layer2_0_downsample_1 = None
    layer2_0_relu_1 = getattr(self.layer2, "0").relu_dup1(add_9);  add_9 = None
    layer2_0_relu_1_post_act_fake_quantizer = self.layer2_0_relu_1_post_act_fake_quantizer(layer2_0_relu_1);  layer2_0_relu_1 = None
    return layer2_0_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_0_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_1_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_0_relu_1_post_act_fake_quantizer, layer2_1_conv1, layer2_1_bn1, layer2_1_relu, layer2_1_relu_post_act_fake_quantizer, layer2_1_conv2, layer2_1_bn2, add_10, layer2_1_relu_1, layer2_1_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_0_relu_1_post_act_fake_quantizer):
    layer2_1_conv1 = getattr(self.layer2, "1").conv1(layer2_0_relu_1_post_act_fake_quantizer)
    layer2_1_bn1 = getattr(self.layer2, "1").bn1(layer2_1_conv1);  layer2_1_conv1 = None
    layer2_1_relu = getattr(self.layer2, "1").relu(layer2_1_bn1);  layer2_1_bn1 = None
    layer2_1_relu_post_act_fake_quantizer = self.layer2_1_relu_post_act_fake_quantizer(layer2_1_relu);  layer2_1_relu = None
    layer2_1_conv2 = getattr(self.layer2, "1").conv2(layer2_1_relu_post_act_fake_quantizer);  layer2_1_relu_post_act_fake_quantizer = None
    layer2_1_bn2 = getattr(self.layer2, "1").bn2(layer2_1_conv2);  layer2_1_conv2 = None
    add_10 = layer2_1_bn2 + layer2_0_relu_1_post_act_fake_quantizer;  layer2_1_bn2 = layer2_0_relu_1_post_act_fake_quantizer = None
    layer2_1_relu_1 = getattr(self.layer2, "1").relu_dup1(add_10);  add_10 = None
    layer2_1_relu_1_post_act_fake_quantizer = self.layer2_1_relu_1_post_act_fake_quantizer(layer2_1_relu_1);  layer2_1_relu_1 = None
    return layer2_1_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_1_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_2_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_1_relu_1_post_act_fake_quantizer, layer2_2_conv1, layer2_2_bn1, layer2_2_relu, layer2_2_relu_post_act_fake_quantizer, layer2_2_conv2, layer2_2_bn2, add_11, layer2_2_relu_1, layer2_2_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_1_relu_1_post_act_fake_quantizer):
    layer2_2_conv1 = getattr(self.layer2, "2").conv1(layer2_1_relu_1_post_act_fake_quantizer)
    layer2_2_bn1 = getattr(self.layer2, "2").bn1(layer2_2_conv1);  layer2_2_conv1 = None
    layer2_2_relu = getattr(self.layer2, "2").relu(layer2_2_bn1);  layer2_2_bn1 = None
    layer2_2_relu_post_act_fake_quantizer = self.layer2_2_relu_post_act_fake_quantizer(layer2_2_relu);  layer2_2_relu = None
    layer2_2_conv2 = getattr(self.layer2, "2").conv2(layer2_2_relu_post_act_fake_quantizer);  layer2_2_relu_post_act_fake_quantizer = None
    layer2_2_bn2 = getattr(self.layer2, "2").bn2(layer2_2_conv2);  layer2_2_conv2 = None
    add_11 = layer2_2_bn2 + layer2_1_relu_1_post_act_fake_quantizer;  layer2_2_bn2 = layer2_1_relu_1_post_act_fake_quantizer = None
    layer2_2_relu_1 = getattr(self.layer2, "2").relu_dup1(add_11);  add_11 = None
    layer2_2_relu_1_post_act_fake_quantizer = self.layer2_2_relu_1_post_act_fake_quantizer(layer2_2_relu_1);  layer2_2_relu_1 = None
    return layer2_2_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_2_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_3_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_2_relu_1_post_act_fake_quantizer, layer2_3_conv1, layer2_3_bn1, layer2_3_relu, layer2_3_relu_post_act_fake_quantizer, layer2_3_conv2, layer2_3_bn2, add_12, layer2_3_relu_1, layer2_3_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_2_relu_1_post_act_fake_quantizer):
    layer2_3_conv1 = getattr(self.layer2, "3").conv1(layer2_2_relu_1_post_act_fake_quantizer)
    layer2_3_bn1 = getattr(self.layer2, "3").bn1(layer2_3_conv1);  layer2_3_conv1 = None
    layer2_3_relu = getattr(self.layer2, "3").relu(layer2_3_bn1);  layer2_3_bn1 = None
    layer2_3_relu_post_act_fake_quantizer = self.layer2_3_relu_post_act_fake_quantizer(layer2_3_relu);  layer2_3_relu = None
    layer2_3_conv2 = getattr(self.layer2, "3").conv2(layer2_3_relu_post_act_fake_quantizer);  layer2_3_relu_post_act_fake_quantizer = None
    layer2_3_bn2 = getattr(self.layer2, "3").bn2(layer2_3_conv2);  layer2_3_conv2 = None
    add_12 = layer2_3_bn2 + layer2_2_relu_1_post_act_fake_quantizer;  layer2_3_bn2 = layer2_2_relu_1_post_act_fake_quantizer = None
    layer2_3_relu_1 = getattr(self.layer2, "3").relu_dup1(add_12);  add_12 = None
    layer2_3_relu_1_post_act_fake_quantizer = self.layer2_3_relu_1_post_act_fake_quantizer(layer2_3_relu_1);  layer2_3_relu_1 = None
    return layer2_3_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_3_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_4_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_3_relu_1_post_act_fake_quantizer, layer2_4_conv1, layer2_4_bn1, layer2_4_relu, layer2_4_relu_post_act_fake_quantizer, layer2_4_conv2, layer2_4_bn2, add_13, layer2_4_relu_1, layer2_4_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_3_relu_1_post_act_fake_quantizer):
    layer2_4_conv1 = getattr(self.layer2, "4").conv1(layer2_3_relu_1_post_act_fake_quantizer)
    layer2_4_bn1 = getattr(self.layer2, "4").bn1(layer2_4_conv1);  layer2_4_conv1 = None
    layer2_4_relu = getattr(self.layer2, "4").relu(layer2_4_bn1);  layer2_4_bn1 = None
    layer2_4_relu_post_act_fake_quantizer = self.layer2_4_relu_post_act_fake_quantizer(layer2_4_relu);  layer2_4_relu = None
    layer2_4_conv2 = getattr(self.layer2, "4").conv2(layer2_4_relu_post_act_fake_quantizer);  layer2_4_relu_post_act_fake_quantizer = None
    layer2_4_bn2 = getattr(self.layer2, "4").bn2(layer2_4_conv2);  layer2_4_conv2 = None
    add_13 = layer2_4_bn2 + layer2_3_relu_1_post_act_fake_quantizer;  layer2_4_bn2 = layer2_3_relu_1_post_act_fake_quantizer = None
    layer2_4_relu_1 = getattr(self.layer2, "4").relu_dup1(add_13);  add_13 = None
    layer2_4_relu_1_post_act_fake_quantizer = self.layer2_4_relu_1_post_act_fake_quantizer(layer2_4_relu_1);  layer2_4_relu_1 = None
    return layer2_4_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_4_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_5_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_4_relu_1_post_act_fake_quantizer, layer2_5_conv1, layer2_5_bn1, layer2_5_relu, layer2_5_relu_post_act_fake_quantizer, layer2_5_conv2, layer2_5_bn2, add_14, layer2_5_relu_1, layer2_5_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_4_relu_1_post_act_fake_quantizer):
    layer2_5_conv1 = getattr(self.layer2, "5").conv1(layer2_4_relu_1_post_act_fake_quantizer)
    layer2_5_bn1 = getattr(self.layer2, "5").bn1(layer2_5_conv1);  layer2_5_conv1 = None
    layer2_5_relu = getattr(self.layer2, "5").relu(layer2_5_bn1);  layer2_5_bn1 = None
    layer2_5_relu_post_act_fake_quantizer = self.layer2_5_relu_post_act_fake_quantizer(layer2_5_relu);  layer2_5_relu = None
    layer2_5_conv2 = getattr(self.layer2, "5").conv2(layer2_5_relu_post_act_fake_quantizer);  layer2_5_relu_post_act_fake_quantizer = None
    layer2_5_bn2 = getattr(self.layer2, "5").bn2(layer2_5_conv2);  layer2_5_conv2 = None
    add_14 = layer2_5_bn2 + layer2_4_relu_1_post_act_fake_quantizer;  layer2_5_bn2 = layer2_4_relu_1_post_act_fake_quantizer = None
    layer2_5_relu_1 = getattr(self.layer2, "5").relu_dup1(add_14);  add_14 = None
    layer2_5_relu_1_post_act_fake_quantizer = self.layer2_5_relu_1_post_act_fake_quantizer(layer2_5_relu_1);  layer2_5_relu_1 = None
    return layer2_5_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_5_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_6_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_5_relu_1_post_act_fake_quantizer, layer2_6_conv1, layer2_6_bn1, layer2_6_relu, layer2_6_relu_post_act_fake_quantizer, layer2_6_conv2, layer2_6_bn2, add_15, layer2_6_relu_1, layer2_6_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_5_relu_1_post_act_fake_quantizer):
    layer2_6_conv1 = getattr(self.layer2, "6").conv1(layer2_5_relu_1_post_act_fake_quantizer)
    layer2_6_bn1 = getattr(self.layer2, "6").bn1(layer2_6_conv1);  layer2_6_conv1 = None
    layer2_6_relu = getattr(self.layer2, "6").relu(layer2_6_bn1);  layer2_6_bn1 = None
    layer2_6_relu_post_act_fake_quantizer = self.layer2_6_relu_post_act_fake_quantizer(layer2_6_relu);  layer2_6_relu = None
    layer2_6_conv2 = getattr(self.layer2, "6").conv2(layer2_6_relu_post_act_fake_quantizer);  layer2_6_relu_post_act_fake_quantizer = None
    layer2_6_bn2 = getattr(self.layer2, "6").bn2(layer2_6_conv2);  layer2_6_conv2 = None
    add_15 = layer2_6_bn2 + layer2_5_relu_1_post_act_fake_quantizer;  layer2_6_bn2 = layer2_5_relu_1_post_act_fake_quantizer = None
    layer2_6_relu_1 = getattr(self.layer2, "6").relu_dup1(add_15);  add_15 = None
    layer2_6_relu_1_post_act_fake_quantizer = self.layer2_6_relu_1_post_act_fake_quantizer(layer2_6_relu_1);  layer2_6_relu_1 = None
    return layer2_6_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_6_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_7_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_6_relu_1_post_act_fake_quantizer, layer2_7_conv1, layer2_7_bn1, layer2_7_relu, layer2_7_relu_post_act_fake_quantizer, layer2_7_conv2, layer2_7_bn2, add_16, layer2_7_relu_1, layer2_7_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_6_relu_1_post_act_fake_quantizer):
    layer2_7_conv1 = getattr(self.layer2, "7").conv1(layer2_6_relu_1_post_act_fake_quantizer)
    layer2_7_bn1 = getattr(self.layer2, "7").bn1(layer2_7_conv1);  layer2_7_conv1 = None
    layer2_7_relu = getattr(self.layer2, "7").relu(layer2_7_bn1);  layer2_7_bn1 = None
    layer2_7_relu_post_act_fake_quantizer = self.layer2_7_relu_post_act_fake_quantizer(layer2_7_relu);  layer2_7_relu = None
    layer2_7_conv2 = getattr(self.layer2, "7").conv2(layer2_7_relu_post_act_fake_quantizer);  layer2_7_relu_post_act_fake_quantizer = None
    layer2_7_bn2 = getattr(self.layer2, "7").bn2(layer2_7_conv2);  layer2_7_conv2 = None
    add_16 = layer2_7_bn2 + layer2_6_relu_1_post_act_fake_quantizer;  layer2_7_bn2 = layer2_6_relu_1_post_act_fake_quantizer = None
    layer2_7_relu_1 = getattr(self.layer2, "7").relu_dup1(add_16);  add_16 = None
    layer2_7_relu_1_post_act_fake_quantizer = self.layer2_7_relu_1_post_act_fake_quantizer(layer2_7_relu_1);  layer2_7_relu_1 = None
    return layer2_7_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_7_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_8_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_7_relu_1_post_act_fake_quantizer, layer2_8_conv1, layer2_8_bn1, layer2_8_relu, layer2_8_relu_post_act_fake_quantizer, layer2_8_conv2, layer2_8_bn2, add_17, layer2_8_relu_1, layer2_8_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_7_relu_1_post_act_fake_quantizer):
    layer2_8_conv1 = getattr(self.layer2, "8").conv1(layer2_7_relu_1_post_act_fake_quantizer)
    layer2_8_bn1 = getattr(self.layer2, "8").bn1(layer2_8_conv1);  layer2_8_conv1 = None
    layer2_8_relu = getattr(self.layer2, "8").relu(layer2_8_bn1);  layer2_8_bn1 = None
    layer2_8_relu_post_act_fake_quantizer = self.layer2_8_relu_post_act_fake_quantizer(layer2_8_relu);  layer2_8_relu = None
    layer2_8_conv2 = getattr(self.layer2, "8").conv2(layer2_8_relu_post_act_fake_quantizer);  layer2_8_relu_post_act_fake_quantizer = None
    layer2_8_bn2 = getattr(self.layer2, "8").bn2(layer2_8_conv2);  layer2_8_conv2 = None
    add_17 = layer2_8_bn2 + layer2_7_relu_1_post_act_fake_quantizer;  layer2_8_bn2 = layer2_7_relu_1_post_act_fake_quantizer = None
    layer2_8_relu_1 = getattr(self.layer2, "8").relu_dup1(add_17);  add_17 = None
    layer2_8_relu_1_post_act_fake_quantizer = self.layer2_8_relu_1_post_act_fake_quantizer(layer2_8_relu_1);  layer2_8_relu_1 = None
    return layer2_8_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_8_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_0_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_8_relu_1_post_act_fake_quantizer, layer3_0_conv1, layer3_0_bn1, layer3_0_relu, layer3_0_relu_post_act_fake_quantizer, layer3_0_conv2, layer3_0_bn2, layer3_0_downsample_0, layer3_0_downsample_1, add_18, layer3_0_relu_1, layer3_0_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_8_relu_1_post_act_fake_quantizer):
    layer3_0_conv1 = getattr(self.layer3, "0").conv1(layer2_8_relu_1_post_act_fake_quantizer)
    layer3_0_bn1 = getattr(self.layer3, "0").bn1(layer3_0_conv1);  layer3_0_conv1 = None
    layer3_0_relu = getattr(self.layer3, "0").relu(layer3_0_bn1);  layer3_0_bn1 = None
    layer3_0_relu_post_act_fake_quantizer = self.layer3_0_relu_post_act_fake_quantizer(layer3_0_relu);  layer3_0_relu = None
    layer3_0_conv2 = getattr(self.layer3, "0").conv2(layer3_0_relu_post_act_fake_quantizer);  layer3_0_relu_post_act_fake_quantizer = None
    layer3_0_bn2 = getattr(self.layer3, "0").bn2(layer3_0_conv2);  layer3_0_conv2 = None
    layer3_0_downsample_0 = getattr(getattr(self.layer3, "0").downsample, "0")(layer2_8_relu_1_post_act_fake_quantizer);  layer2_8_relu_1_post_act_fake_quantizer = None
    layer3_0_downsample_1 = getattr(getattr(self.layer3, "0").downsample, "1")(layer3_0_downsample_0);  layer3_0_downsample_0 = None
    add_18 = layer3_0_bn2 + layer3_0_downsample_1;  layer3_0_bn2 = layer3_0_downsample_1 = None
    layer3_0_relu_1 = getattr(self.layer3, "0").relu_dup1(add_18);  add_18 = None
    layer3_0_relu_1_post_act_fake_quantizer = self.layer3_0_relu_1_post_act_fake_quantizer(layer3_0_relu_1);  layer3_0_relu_1 = None
    return layer3_0_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_0_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_1_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_0_relu_1_post_act_fake_quantizer, layer3_1_conv1, layer3_1_bn1, layer3_1_relu, layer3_1_relu_post_act_fake_quantizer, layer3_1_conv2, layer3_1_bn2, add_19, layer3_1_relu_1, layer3_1_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_0_relu_1_post_act_fake_quantizer):
    layer3_1_conv1 = getattr(self.layer3, "1").conv1(layer3_0_relu_1_post_act_fake_quantizer)
    layer3_1_bn1 = getattr(self.layer3, "1").bn1(layer3_1_conv1);  layer3_1_conv1 = None
    layer3_1_relu = getattr(self.layer3, "1").relu(layer3_1_bn1);  layer3_1_bn1 = None
    layer3_1_relu_post_act_fake_quantizer = self.layer3_1_relu_post_act_fake_quantizer(layer3_1_relu);  layer3_1_relu = None
    layer3_1_conv2 = getattr(self.layer3, "1").conv2(layer3_1_relu_post_act_fake_quantizer);  layer3_1_relu_post_act_fake_quantizer = None
    layer3_1_bn2 = getattr(self.layer3, "1").bn2(layer3_1_conv2);  layer3_1_conv2 = None
    add_19 = layer3_1_bn2 + layer3_0_relu_1_post_act_fake_quantizer;  layer3_1_bn2 = layer3_0_relu_1_post_act_fake_quantizer = None
    layer3_1_relu_1 = getattr(self.layer3, "1").relu_dup1(add_19);  add_19 = None
    layer3_1_relu_1_post_act_fake_quantizer = self.layer3_1_relu_1_post_act_fake_quantizer(layer3_1_relu_1);  layer3_1_relu_1 = None
    return layer3_1_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_1_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_2_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_1_relu_1_post_act_fake_quantizer, layer3_2_conv1, layer3_2_bn1, layer3_2_relu, layer3_2_relu_post_act_fake_quantizer, layer3_2_conv2, layer3_2_bn2, add_20, layer3_2_relu_1, layer3_2_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_1_relu_1_post_act_fake_quantizer):
    layer3_2_conv1 = getattr(self.layer3, "2").conv1(layer3_1_relu_1_post_act_fake_quantizer)
    layer3_2_bn1 = getattr(self.layer3, "2").bn1(layer3_2_conv1);  layer3_2_conv1 = None
    layer3_2_relu = getattr(self.layer3, "2").relu(layer3_2_bn1);  layer3_2_bn1 = None
    layer3_2_relu_post_act_fake_quantizer = self.layer3_2_relu_post_act_fake_quantizer(layer3_2_relu);  layer3_2_relu = None
    layer3_2_conv2 = getattr(self.layer3, "2").conv2(layer3_2_relu_post_act_fake_quantizer);  layer3_2_relu_post_act_fake_quantizer = None
    layer3_2_bn2 = getattr(self.layer3, "2").bn2(layer3_2_conv2);  layer3_2_conv2 = None
    add_20 = layer3_2_bn2 + layer3_1_relu_1_post_act_fake_quantizer;  layer3_2_bn2 = layer3_1_relu_1_post_act_fake_quantizer = None
    layer3_2_relu_1 = getattr(self.layer3, "2").relu_dup1(add_20);  add_20 = None
    layer3_2_relu_1_post_act_fake_quantizer = self.layer3_2_relu_1_post_act_fake_quantizer(layer3_2_relu_1);  layer3_2_relu_1 = None
    return layer3_2_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_2_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_3_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_2_relu_1_post_act_fake_quantizer, layer3_3_conv1, layer3_3_bn1, layer3_3_relu, layer3_3_relu_post_act_fake_quantizer, layer3_3_conv2, layer3_3_bn2, add_21, layer3_3_relu_1, layer3_3_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_2_relu_1_post_act_fake_quantizer):
    layer3_3_conv1 = getattr(self.layer3, "3").conv1(layer3_2_relu_1_post_act_fake_quantizer)
    layer3_3_bn1 = getattr(self.layer3, "3").bn1(layer3_3_conv1);  layer3_3_conv1 = None
    layer3_3_relu = getattr(self.layer3, "3").relu(layer3_3_bn1);  layer3_3_bn1 = None
    layer3_3_relu_post_act_fake_quantizer = self.layer3_3_relu_post_act_fake_quantizer(layer3_3_relu);  layer3_3_relu = None
    layer3_3_conv2 = getattr(self.layer3, "3").conv2(layer3_3_relu_post_act_fake_quantizer);  layer3_3_relu_post_act_fake_quantizer = None
    layer3_3_bn2 = getattr(self.layer3, "3").bn2(layer3_3_conv2);  layer3_3_conv2 = None
    add_21 = layer3_3_bn2 + layer3_2_relu_1_post_act_fake_quantizer;  layer3_3_bn2 = layer3_2_relu_1_post_act_fake_quantizer = None
    layer3_3_relu_1 = getattr(self.layer3, "3").relu_dup1(add_21);  add_21 = None
    layer3_3_relu_1_post_act_fake_quantizer = self.layer3_3_relu_1_post_act_fake_quantizer(layer3_3_relu_1);  layer3_3_relu_1 = None
    return layer3_3_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_3_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_4_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_3_relu_1_post_act_fake_quantizer, layer3_4_conv1, layer3_4_bn1, layer3_4_relu, layer3_4_relu_post_act_fake_quantizer, layer3_4_conv2, layer3_4_bn2, add_22, layer3_4_relu_1, layer3_4_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_3_relu_1_post_act_fake_quantizer):
    layer3_4_conv1 = getattr(self.layer3, "4").conv1(layer3_3_relu_1_post_act_fake_quantizer)
    layer3_4_bn1 = getattr(self.layer3, "4").bn1(layer3_4_conv1);  layer3_4_conv1 = None
    layer3_4_relu = getattr(self.layer3, "4").relu(layer3_4_bn1);  layer3_4_bn1 = None
    layer3_4_relu_post_act_fake_quantizer = self.layer3_4_relu_post_act_fake_quantizer(layer3_4_relu);  layer3_4_relu = None
    layer3_4_conv2 = getattr(self.layer3, "4").conv2(layer3_4_relu_post_act_fake_quantizer);  layer3_4_relu_post_act_fake_quantizer = None
    layer3_4_bn2 = getattr(self.layer3, "4").bn2(layer3_4_conv2);  layer3_4_conv2 = None
    add_22 = layer3_4_bn2 + layer3_3_relu_1_post_act_fake_quantizer;  layer3_4_bn2 = layer3_3_relu_1_post_act_fake_quantizer = None
    layer3_4_relu_1 = getattr(self.layer3, "4").relu_dup1(add_22);  add_22 = None
    layer3_4_relu_1_post_act_fake_quantizer = self.layer3_4_relu_1_post_act_fake_quantizer(layer3_4_relu_1);  layer3_4_relu_1 = None
    return layer3_4_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_4_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_5_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_4_relu_1_post_act_fake_quantizer, layer3_5_conv1, layer3_5_bn1, layer3_5_relu, layer3_5_relu_post_act_fake_quantizer, layer3_5_conv2, layer3_5_bn2, add_23, layer3_5_relu_1, layer3_5_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_4_relu_1_post_act_fake_quantizer):
    layer3_5_conv1 = getattr(self.layer3, "5").conv1(layer3_4_relu_1_post_act_fake_quantizer)
    layer3_5_bn1 = getattr(self.layer3, "5").bn1(layer3_5_conv1);  layer3_5_conv1 = None
    layer3_5_relu = getattr(self.layer3, "5").relu(layer3_5_bn1);  layer3_5_bn1 = None
    layer3_5_relu_post_act_fake_quantizer = self.layer3_5_relu_post_act_fake_quantizer(layer3_5_relu);  layer3_5_relu = None
    layer3_5_conv2 = getattr(self.layer3, "5").conv2(layer3_5_relu_post_act_fake_quantizer);  layer3_5_relu_post_act_fake_quantizer = None
    layer3_5_bn2 = getattr(self.layer3, "5").bn2(layer3_5_conv2);  layer3_5_conv2 = None
    add_23 = layer3_5_bn2 + layer3_4_relu_1_post_act_fake_quantizer;  layer3_5_bn2 = layer3_4_relu_1_post_act_fake_quantizer = None
    layer3_5_relu_1 = getattr(self.layer3, "5").relu_dup1(add_23);  add_23 = None
    layer3_5_relu_1_post_act_fake_quantizer = self.layer3_5_relu_1_post_act_fake_quantizer(layer3_5_relu_1);  layer3_5_relu_1 = None
    return layer3_5_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_5_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_6_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_5_relu_1_post_act_fake_quantizer, layer3_6_conv1, layer3_6_bn1, layer3_6_relu, layer3_6_relu_post_act_fake_quantizer, layer3_6_conv2, layer3_6_bn2, add_24, layer3_6_relu_1, layer3_6_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_5_relu_1_post_act_fake_quantizer):
    layer3_6_conv1 = getattr(self.layer3, "6").conv1(layer3_5_relu_1_post_act_fake_quantizer)
    layer3_6_bn1 = getattr(self.layer3, "6").bn1(layer3_6_conv1);  layer3_6_conv1 = None
    layer3_6_relu = getattr(self.layer3, "6").relu(layer3_6_bn1);  layer3_6_bn1 = None
    layer3_6_relu_post_act_fake_quantizer = self.layer3_6_relu_post_act_fake_quantizer(layer3_6_relu);  layer3_6_relu = None
    layer3_6_conv2 = getattr(self.layer3, "6").conv2(layer3_6_relu_post_act_fake_quantizer);  layer3_6_relu_post_act_fake_quantizer = None
    layer3_6_bn2 = getattr(self.layer3, "6").bn2(layer3_6_conv2);  layer3_6_conv2 = None
    add_24 = layer3_6_bn2 + layer3_5_relu_1_post_act_fake_quantizer;  layer3_6_bn2 = layer3_5_relu_1_post_act_fake_quantizer = None
    layer3_6_relu_1 = getattr(self.layer3, "6").relu_dup1(add_24);  add_24 = None
    layer3_6_relu_1_post_act_fake_quantizer = self.layer3_6_relu_1_post_act_fake_quantizer(layer3_6_relu_1);  layer3_6_relu_1 = None
    return layer3_6_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_6_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_7_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_6_relu_1_post_act_fake_quantizer, layer3_7_conv1, layer3_7_bn1, layer3_7_relu, layer3_7_relu_post_act_fake_quantizer, layer3_7_conv2, layer3_7_bn2, add_25, layer3_7_relu_1, layer3_7_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_6_relu_1_post_act_fake_quantizer):
    layer3_7_conv1 = getattr(self.layer3, "7").conv1(layer3_6_relu_1_post_act_fake_quantizer)
    layer3_7_bn1 = getattr(self.layer3, "7").bn1(layer3_7_conv1);  layer3_7_conv1 = None
    layer3_7_relu = getattr(self.layer3, "7").relu(layer3_7_bn1);  layer3_7_bn1 = None
    layer3_7_relu_post_act_fake_quantizer = self.layer3_7_relu_post_act_fake_quantizer(layer3_7_relu);  layer3_7_relu = None
    layer3_7_conv2 = getattr(self.layer3, "7").conv2(layer3_7_relu_post_act_fake_quantizer);  layer3_7_relu_post_act_fake_quantizer = None
    layer3_7_bn2 = getattr(self.layer3, "7").bn2(layer3_7_conv2);  layer3_7_conv2 = None
    add_25 = layer3_7_bn2 + layer3_6_relu_1_post_act_fake_quantizer;  layer3_7_bn2 = layer3_6_relu_1_post_act_fake_quantizer = None
    layer3_7_relu_1 = getattr(self.layer3, "7").relu_dup1(add_25);  add_25 = None
    layer3_7_relu_1_post_act_fake_quantizer = self.layer3_7_relu_1_post_act_fake_quantizer(layer3_7_relu_1);  layer3_7_relu_1 = None
    return layer3_7_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_7_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_8_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_7_relu_1_post_act_fake_quantizer, layer3_8_conv1, layer3_8_bn1, layer3_8_relu, layer3_8_relu_post_act_fake_quantizer, layer3_8_conv2, layer3_8_bn2, add_26, layer3_8_relu_1, avgpool, size, view, view_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_7_relu_1_post_act_fake_quantizer):
    layer3_8_conv1 = getattr(self.layer3, "8").conv1(layer3_7_relu_1_post_act_fake_quantizer)
    layer3_8_bn1 = getattr(self.layer3, "8").bn1(layer3_8_conv1);  layer3_8_conv1 = None
    layer3_8_relu = getattr(self.layer3, "8").relu(layer3_8_bn1);  layer3_8_bn1 = None
    layer3_8_relu_post_act_fake_quantizer = self.layer3_8_relu_post_act_fake_quantizer(layer3_8_relu);  layer3_8_relu = None
    layer3_8_conv2 = getattr(self.layer3, "8").conv2(layer3_8_relu_post_act_fake_quantizer);  layer3_8_relu_post_act_fake_quantizer = None
    layer3_8_bn2 = getattr(self.layer3, "8").bn2(layer3_8_conv2);  layer3_8_conv2 = None
    add_26 = layer3_8_bn2 + layer3_7_relu_1_post_act_fake_quantizer;  layer3_8_bn2 = layer3_7_relu_1_post_act_fake_quantizer = None
    layer3_8_relu_1 = getattr(self.layer3, "8").relu_dup1(add_26);  add_26 = None
    avgpool = self.avgpool(layer3_8_relu_1);  layer3_8_relu_1 = None
    size = avgpool.size(0)
    view = avgpool.view(size, -1);  avgpool = size = None
    view_post_act_fake_quantizer = self.view_post_act_fake_quantizer(view);  view = None
    return view_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for view_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for fc
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [view_post_act_fake_quantizer, fc]
[MQBENCH] INFO: 


def forward(self, view_post_act_fake_quantizer):
    fc = self.fc(view_post_act_fake_quantizer);  view_post_act_fake_quantizer = None
    return fc
    
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node x_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_0_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_0_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_1_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_1_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_2_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_2_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_3_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_3_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_4_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_4_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_5_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_5_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_6_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_6_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_7_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_7_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_8_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_8_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_0_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.downsample.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.downsample.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_0_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_1_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_1_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_2_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_2_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_3_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_3_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_4_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_4_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_5_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_5_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_6_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_6_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_7_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_7_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_8_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_8_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_0_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.downsample.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.downsample.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_0_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_1_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_1_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_2_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_2_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_3_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_3_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_4_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_4_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_5_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_5_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_6_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_6_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_7_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_7_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_8_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node avgpool in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node view_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node fc in quant
QDROP model already saved, now loading QDROP_2bits_cifar100_resnet56.pt
Prepare 4bits model using MQBench
[MQBENCH] INFO: Quantize model Scheme: BackendType.Academic Mode: Eval
[MQBENCH] INFO: Weight Qconfig:
    FakeQuantize: AdaRoundFakeQuantize Params: {}
    Oberver:      MSEObserver Params: Symmetric: True / Bitwidth: 4 / Per channel: False / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Activation Qconfig:
    FakeQuantize: QDropFakeQuantize Params: {}
    Oberver:      EMAMSEObserver Params: Symmetric: False / Bitwidth: 4 / Per channel: False / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Replace module to qat module.
[MQBENCH] INFO: Set layer conv1 to 8 bit.
[MQBENCH] INFO: Set layer fc to 8 bit.
dbg node_to_quantize_output
 odict_keys([x, relu, layer1_0_relu, layer1_0_relu_1, layer1_1_relu, layer1_1_relu_1, layer1_2_relu, layer1_2_relu_1, layer1_3_relu, layer1_3_relu_1, layer1_4_relu, layer1_4_relu_1, layer1_5_relu, layer1_5_relu_1, layer1_6_relu, layer1_6_relu_1, layer1_7_relu, layer1_7_relu_1, layer1_8_relu, layer1_8_relu_1, layer2_0_relu, layer2_0_relu_1, layer2_1_relu, layer2_1_relu_1, layer2_2_relu, layer2_2_relu_1, layer2_3_relu, layer2_3_relu_1, layer2_4_relu, layer2_4_relu_1, layer2_5_relu, layer2_5_relu_1, layer2_6_relu, layer2_6_relu_1, layer2_7_relu, layer2_7_relu_1, layer2_8_relu, layer2_8_relu_1, layer3_0_relu, layer3_0_relu_1, layer3_1_relu, layer3_1_relu_1, layer3_2_relu, layer3_2_relu_1, layer3_3_relu, layer3_3_relu_1, layer3_4_relu, layer3_4_relu_1, layer3_5_relu, layer3_5_relu_1, layer3_6_relu, layer3_6_relu_1, layer3_7_relu, layer3_7_relu_1, layer3_8_relu, view])
[MQBENCH] INFO: Set x post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant x_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_4_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_5_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_6_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_7_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_8_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_4_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_5_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_6_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_7_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_8_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_4_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_5_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_6_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_7_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: Set view post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant view_post_act_fake_quantizer
[MQBENCH] INFO: Enable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: prepare block reconstruction for conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [x_post_act_fake_quantizer, conv1, bn1, relu, relu_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, x_post_act_fake_quantizer):
    conv1 = self.conv1(x_post_act_fake_quantizer);  x_post_act_fake_quantizer = None
    bn1 = self.bn1(conv1);  conv1 = None
    relu = self.relu(bn1);  bn1 = None
    relu_post_act_fake_quantizer = self.relu_post_act_fake_quantizer(relu);  relu = None
    return relu_post_act_fake_quantizer
    
Init alpha to be FP32
[MQBENCH] INFO: learn the scale for relu_post_act_fake_quantizer
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_0_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [relu_post_act_fake_quantizer, layer1_0_conv1, layer1_0_bn1, layer1_0_relu, layer1_0_relu_post_act_fake_quantizer, layer1_0_conv2, layer1_0_bn2, add, layer1_0_relu_1, layer1_0_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, relu_post_act_fake_quantizer):
    layer1_0_conv1 = getattr(self.layer1, "0").conv1(relu_post_act_fake_quantizer)
    layer1_0_bn1 = getattr(self.layer1, "0").bn1(layer1_0_conv1);  layer1_0_conv1 = None
    layer1_0_relu = getattr(self.layer1, "0").relu(layer1_0_bn1);  layer1_0_bn1 = None
    layer1_0_relu_post_act_fake_quantizer = self.layer1_0_relu_post_act_fake_quantizer(layer1_0_relu);  layer1_0_relu = None
    layer1_0_conv2 = getattr(self.layer1, "0").conv2(layer1_0_relu_post_act_fake_quantizer);  layer1_0_relu_post_act_fake_quantizer = None
    layer1_0_bn2 = getattr(self.layer1, "0").bn2(layer1_0_conv2);  layer1_0_conv2 = None
    add = layer1_0_bn2 + relu_post_act_fake_quantizer;  layer1_0_bn2 = relu_post_act_fake_quantizer = None
    layer1_0_relu_1 = getattr(self.layer1, "0").relu_dup1(add);  add = None
    layer1_0_relu_1_post_act_fake_quantizer = self.layer1_0_relu_1_post_act_fake_quantizer(layer1_0_relu_1);  layer1_0_relu_1 = None
    return layer1_0_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_0_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_1_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_0_relu_1_post_act_fake_quantizer, layer1_1_conv1, layer1_1_bn1, layer1_1_relu, layer1_1_relu_post_act_fake_quantizer, layer1_1_conv2, layer1_1_bn2, add_1, layer1_1_relu_1, layer1_1_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_0_relu_1_post_act_fake_quantizer):
    layer1_1_conv1 = getattr(self.layer1, "1").conv1(layer1_0_relu_1_post_act_fake_quantizer)
    layer1_1_bn1 = getattr(self.layer1, "1").bn1(layer1_1_conv1);  layer1_1_conv1 = None
    layer1_1_relu = getattr(self.layer1, "1").relu(layer1_1_bn1);  layer1_1_bn1 = None
    layer1_1_relu_post_act_fake_quantizer = self.layer1_1_relu_post_act_fake_quantizer(layer1_1_relu);  layer1_1_relu = None
    layer1_1_conv2 = getattr(self.layer1, "1").conv2(layer1_1_relu_post_act_fake_quantizer);  layer1_1_relu_post_act_fake_quantizer = None
    layer1_1_bn2 = getattr(self.layer1, "1").bn2(layer1_1_conv2);  layer1_1_conv2 = None
    add_1 = layer1_1_bn2 + layer1_0_relu_1_post_act_fake_quantizer;  layer1_1_bn2 = layer1_0_relu_1_post_act_fake_quantizer = None
    layer1_1_relu_1 = getattr(self.layer1, "1").relu_dup1(add_1);  add_1 = None
    layer1_1_relu_1_post_act_fake_quantizer = self.layer1_1_relu_1_post_act_fake_quantizer(layer1_1_relu_1);  layer1_1_relu_1 = None
    return layer1_1_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_1_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_2_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_1_relu_1_post_act_fake_quantizer, layer1_2_conv1, layer1_2_bn1, layer1_2_relu, layer1_2_relu_post_act_fake_quantizer, layer1_2_conv2, layer1_2_bn2, add_2, layer1_2_relu_1, layer1_2_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_1_relu_1_post_act_fake_quantizer):
    layer1_2_conv1 = getattr(self.layer1, "2").conv1(layer1_1_relu_1_post_act_fake_quantizer)
    layer1_2_bn1 = getattr(self.layer1, "2").bn1(layer1_2_conv1);  layer1_2_conv1 = None
    layer1_2_relu = getattr(self.layer1, "2").relu(layer1_2_bn1);  layer1_2_bn1 = None
    layer1_2_relu_post_act_fake_quantizer = self.layer1_2_relu_post_act_fake_quantizer(layer1_2_relu);  layer1_2_relu = None
    layer1_2_conv2 = getattr(self.layer1, "2").conv2(layer1_2_relu_post_act_fake_quantizer);  layer1_2_relu_post_act_fake_quantizer = None
    layer1_2_bn2 = getattr(self.layer1, "2").bn2(layer1_2_conv2);  layer1_2_conv2 = None
    add_2 = layer1_2_bn2 + layer1_1_relu_1_post_act_fake_quantizer;  layer1_2_bn2 = layer1_1_relu_1_post_act_fake_quantizer = None
    layer1_2_relu_1 = getattr(self.layer1, "2").relu_dup1(add_2);  add_2 = None
    layer1_2_relu_1_post_act_fake_quantizer = self.layer1_2_relu_1_post_act_fake_quantizer(layer1_2_relu_1);  layer1_2_relu_1 = None
    return layer1_2_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_2_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_3_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_2_relu_1_post_act_fake_quantizer, layer1_3_conv1, layer1_3_bn1, layer1_3_relu, layer1_3_relu_post_act_fake_quantizer, layer1_3_conv2, layer1_3_bn2, add_3, layer1_3_relu_1, layer1_3_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_2_relu_1_post_act_fake_quantizer):
    layer1_3_conv1 = getattr(self.layer1, "3").conv1(layer1_2_relu_1_post_act_fake_quantizer)
    layer1_3_bn1 = getattr(self.layer1, "3").bn1(layer1_3_conv1);  layer1_3_conv1 = None
    layer1_3_relu = getattr(self.layer1, "3").relu(layer1_3_bn1);  layer1_3_bn1 = None
    layer1_3_relu_post_act_fake_quantizer = self.layer1_3_relu_post_act_fake_quantizer(layer1_3_relu);  layer1_3_relu = None
    layer1_3_conv2 = getattr(self.layer1, "3").conv2(layer1_3_relu_post_act_fake_quantizer);  layer1_3_relu_post_act_fake_quantizer = None
    layer1_3_bn2 = getattr(self.layer1, "3").bn2(layer1_3_conv2);  layer1_3_conv2 = None
    add_3 = layer1_3_bn2 + layer1_2_relu_1_post_act_fake_quantizer;  layer1_3_bn2 = layer1_2_relu_1_post_act_fake_quantizer = None
    layer1_3_relu_1 = getattr(self.layer1, "3").relu_dup1(add_3);  add_3 = None
    layer1_3_relu_1_post_act_fake_quantizer = self.layer1_3_relu_1_post_act_fake_quantizer(layer1_3_relu_1);  layer1_3_relu_1 = None
    return layer1_3_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_3_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_4_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_3_relu_1_post_act_fake_quantizer, layer1_4_conv1, layer1_4_bn1, layer1_4_relu, layer1_4_relu_post_act_fake_quantizer, layer1_4_conv2, layer1_4_bn2, add_4, layer1_4_relu_1, layer1_4_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_3_relu_1_post_act_fake_quantizer):
    layer1_4_conv1 = getattr(self.layer1, "4").conv1(layer1_3_relu_1_post_act_fake_quantizer)
    layer1_4_bn1 = getattr(self.layer1, "4").bn1(layer1_4_conv1);  layer1_4_conv1 = None
    layer1_4_relu = getattr(self.layer1, "4").relu(layer1_4_bn1);  layer1_4_bn1 = None
    layer1_4_relu_post_act_fake_quantizer = self.layer1_4_relu_post_act_fake_quantizer(layer1_4_relu);  layer1_4_relu = None
    layer1_4_conv2 = getattr(self.layer1, "4").conv2(layer1_4_relu_post_act_fake_quantizer);  layer1_4_relu_post_act_fake_quantizer = None
    layer1_4_bn2 = getattr(self.layer1, "4").bn2(layer1_4_conv2);  layer1_4_conv2 = None
    add_4 = layer1_4_bn2 + layer1_3_relu_1_post_act_fake_quantizer;  layer1_4_bn2 = layer1_3_relu_1_post_act_fake_quantizer = None
    layer1_4_relu_1 = getattr(self.layer1, "4").relu_dup1(add_4);  add_4 = None
    layer1_4_relu_1_post_act_fake_quantizer = self.layer1_4_relu_1_post_act_fake_quantizer(layer1_4_relu_1);  layer1_4_relu_1 = None
    return layer1_4_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_4_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_5_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_4_relu_1_post_act_fake_quantizer, layer1_5_conv1, layer1_5_bn1, layer1_5_relu, layer1_5_relu_post_act_fake_quantizer, layer1_5_conv2, layer1_5_bn2, add_5, layer1_5_relu_1, layer1_5_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_4_relu_1_post_act_fake_quantizer):
    layer1_5_conv1 = getattr(self.layer1, "5").conv1(layer1_4_relu_1_post_act_fake_quantizer)
    layer1_5_bn1 = getattr(self.layer1, "5").bn1(layer1_5_conv1);  layer1_5_conv1 = None
    layer1_5_relu = getattr(self.layer1, "5").relu(layer1_5_bn1);  layer1_5_bn1 = None
    layer1_5_relu_post_act_fake_quantizer = self.layer1_5_relu_post_act_fake_quantizer(layer1_5_relu);  layer1_5_relu = None
    layer1_5_conv2 = getattr(self.layer1, "5").conv2(layer1_5_relu_post_act_fake_quantizer);  layer1_5_relu_post_act_fake_quantizer = None
    layer1_5_bn2 = getattr(self.layer1, "5").bn2(layer1_5_conv2);  layer1_5_conv2 = None
    add_5 = layer1_5_bn2 + layer1_4_relu_1_post_act_fake_quantizer;  layer1_5_bn2 = layer1_4_relu_1_post_act_fake_quantizer = None
    layer1_5_relu_1 = getattr(self.layer1, "5").relu_dup1(add_5);  add_5 = None
    layer1_5_relu_1_post_act_fake_quantizer = self.layer1_5_relu_1_post_act_fake_quantizer(layer1_5_relu_1);  layer1_5_relu_1 = None
    return layer1_5_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_5_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_6_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_5_relu_1_post_act_fake_quantizer, layer1_6_conv1, layer1_6_bn1, layer1_6_relu, layer1_6_relu_post_act_fake_quantizer, layer1_6_conv2, layer1_6_bn2, add_6, layer1_6_relu_1, layer1_6_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_5_relu_1_post_act_fake_quantizer):
    layer1_6_conv1 = getattr(self.layer1, "6").conv1(layer1_5_relu_1_post_act_fake_quantizer)
    layer1_6_bn1 = getattr(self.layer1, "6").bn1(layer1_6_conv1);  layer1_6_conv1 = None
    layer1_6_relu = getattr(self.layer1, "6").relu(layer1_6_bn1);  layer1_6_bn1 = None
    layer1_6_relu_post_act_fake_quantizer = self.layer1_6_relu_post_act_fake_quantizer(layer1_6_relu);  layer1_6_relu = None
    layer1_6_conv2 = getattr(self.layer1, "6").conv2(layer1_6_relu_post_act_fake_quantizer);  layer1_6_relu_post_act_fake_quantizer = None
    layer1_6_bn2 = getattr(self.layer1, "6").bn2(layer1_6_conv2);  layer1_6_conv2 = None
    add_6 = layer1_6_bn2 + layer1_5_relu_1_post_act_fake_quantizer;  layer1_6_bn2 = layer1_5_relu_1_post_act_fake_quantizer = None
    layer1_6_relu_1 = getattr(self.layer1, "6").relu_dup1(add_6);  add_6 = None
    layer1_6_relu_1_post_act_fake_quantizer = self.layer1_6_relu_1_post_act_fake_quantizer(layer1_6_relu_1);  layer1_6_relu_1 = None
    return layer1_6_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_6_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_7_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_6_relu_1_post_act_fake_quantizer, layer1_7_conv1, layer1_7_bn1, layer1_7_relu, layer1_7_relu_post_act_fake_quantizer, layer1_7_conv2, layer1_7_bn2, add_7, layer1_7_relu_1, layer1_7_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_6_relu_1_post_act_fake_quantizer):
    layer1_7_conv1 = getattr(self.layer1, "7").conv1(layer1_6_relu_1_post_act_fake_quantizer)
    layer1_7_bn1 = getattr(self.layer1, "7").bn1(layer1_7_conv1);  layer1_7_conv1 = None
    layer1_7_relu = getattr(self.layer1, "7").relu(layer1_7_bn1);  layer1_7_bn1 = None
    layer1_7_relu_post_act_fake_quantizer = self.layer1_7_relu_post_act_fake_quantizer(layer1_7_relu);  layer1_7_relu = None
    layer1_7_conv2 = getattr(self.layer1, "7").conv2(layer1_7_relu_post_act_fake_quantizer);  layer1_7_relu_post_act_fake_quantizer = None
    layer1_7_bn2 = getattr(self.layer1, "7").bn2(layer1_7_conv2);  layer1_7_conv2 = None
    add_7 = layer1_7_bn2 + layer1_6_relu_1_post_act_fake_quantizer;  layer1_7_bn2 = layer1_6_relu_1_post_act_fake_quantizer = None
    layer1_7_relu_1 = getattr(self.layer1, "7").relu_dup1(add_7);  add_7 = None
    layer1_7_relu_1_post_act_fake_quantizer = self.layer1_7_relu_1_post_act_fake_quantizer(layer1_7_relu_1);  layer1_7_relu_1 = None
    return layer1_7_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_7_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_8_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_7_relu_1_post_act_fake_quantizer, layer1_8_conv1, layer1_8_bn1, layer1_8_relu, layer1_8_relu_post_act_fake_quantizer, layer1_8_conv2, layer1_8_bn2, add_8, layer1_8_relu_1, layer1_8_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_7_relu_1_post_act_fake_quantizer):
    layer1_8_conv1 = getattr(self.layer1, "8").conv1(layer1_7_relu_1_post_act_fake_quantizer)
    layer1_8_bn1 = getattr(self.layer1, "8").bn1(layer1_8_conv1);  layer1_8_conv1 = None
    layer1_8_relu = getattr(self.layer1, "8").relu(layer1_8_bn1);  layer1_8_bn1 = None
    layer1_8_relu_post_act_fake_quantizer = self.layer1_8_relu_post_act_fake_quantizer(layer1_8_relu);  layer1_8_relu = None
    layer1_8_conv2 = getattr(self.layer1, "8").conv2(layer1_8_relu_post_act_fake_quantizer);  layer1_8_relu_post_act_fake_quantizer = None
    layer1_8_bn2 = getattr(self.layer1, "8").bn2(layer1_8_conv2);  layer1_8_conv2 = None
    add_8 = layer1_8_bn2 + layer1_7_relu_1_post_act_fake_quantizer;  layer1_8_bn2 = layer1_7_relu_1_post_act_fake_quantizer = None
    layer1_8_relu_1 = getattr(self.layer1, "8").relu_dup1(add_8);  add_8 = None
    layer1_8_relu_1_post_act_fake_quantizer = self.layer1_8_relu_1_post_act_fake_quantizer(layer1_8_relu_1);  layer1_8_relu_1 = None
    return layer1_8_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_8_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_0_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_8_relu_1_post_act_fake_quantizer, layer2_0_conv1, layer2_0_bn1, layer2_0_relu, layer2_0_relu_post_act_fake_quantizer, layer2_0_conv2, layer2_0_bn2, layer2_0_downsample_0, layer2_0_downsample_1, add_9, layer2_0_relu_1, layer2_0_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_8_relu_1_post_act_fake_quantizer):
    layer2_0_conv1 = getattr(self.layer2, "0").conv1(layer1_8_relu_1_post_act_fake_quantizer)
    layer2_0_bn1 = getattr(self.layer2, "0").bn1(layer2_0_conv1);  layer2_0_conv1 = None
    layer2_0_relu = getattr(self.layer2, "0").relu(layer2_0_bn1);  layer2_0_bn1 = None
    layer2_0_relu_post_act_fake_quantizer = self.layer2_0_relu_post_act_fake_quantizer(layer2_0_relu);  layer2_0_relu = None
    layer2_0_conv2 = getattr(self.layer2, "0").conv2(layer2_0_relu_post_act_fake_quantizer);  layer2_0_relu_post_act_fake_quantizer = None
    layer2_0_bn2 = getattr(self.layer2, "0").bn2(layer2_0_conv2);  layer2_0_conv2 = None
    layer2_0_downsample_0 = getattr(getattr(self.layer2, "0").downsample, "0")(layer1_8_relu_1_post_act_fake_quantizer);  layer1_8_relu_1_post_act_fake_quantizer = None
    layer2_0_downsample_1 = getattr(getattr(self.layer2, "0").downsample, "1")(layer2_0_downsample_0);  layer2_0_downsample_0 = None
    add_9 = layer2_0_bn2 + layer2_0_downsample_1;  layer2_0_bn2 = layer2_0_downsample_1 = None
    layer2_0_relu_1 = getattr(self.layer2, "0").relu_dup1(add_9);  add_9 = None
    layer2_0_relu_1_post_act_fake_quantizer = self.layer2_0_relu_1_post_act_fake_quantizer(layer2_0_relu_1);  layer2_0_relu_1 = None
    return layer2_0_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_0_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_1_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_0_relu_1_post_act_fake_quantizer, layer2_1_conv1, layer2_1_bn1, layer2_1_relu, layer2_1_relu_post_act_fake_quantizer, layer2_1_conv2, layer2_1_bn2, add_10, layer2_1_relu_1, layer2_1_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_0_relu_1_post_act_fake_quantizer):
    layer2_1_conv1 = getattr(self.layer2, "1").conv1(layer2_0_relu_1_post_act_fake_quantizer)
    layer2_1_bn1 = getattr(self.layer2, "1").bn1(layer2_1_conv1);  layer2_1_conv1 = None
    layer2_1_relu = getattr(self.layer2, "1").relu(layer2_1_bn1);  layer2_1_bn1 = None
    layer2_1_relu_post_act_fake_quantizer = self.layer2_1_relu_post_act_fake_quantizer(layer2_1_relu);  layer2_1_relu = None
    layer2_1_conv2 = getattr(self.layer2, "1").conv2(layer2_1_relu_post_act_fake_quantizer);  layer2_1_relu_post_act_fake_quantizer = None
    layer2_1_bn2 = getattr(self.layer2, "1").bn2(layer2_1_conv2);  layer2_1_conv2 = None
    add_10 = layer2_1_bn2 + layer2_0_relu_1_post_act_fake_quantizer;  layer2_1_bn2 = layer2_0_relu_1_post_act_fake_quantizer = None
    layer2_1_relu_1 = getattr(self.layer2, "1").relu_dup1(add_10);  add_10 = None
    layer2_1_relu_1_post_act_fake_quantizer = self.layer2_1_relu_1_post_act_fake_quantizer(layer2_1_relu_1);  layer2_1_relu_1 = None
    return layer2_1_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_1_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_2_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_1_relu_1_post_act_fake_quantizer, layer2_2_conv1, layer2_2_bn1, layer2_2_relu, layer2_2_relu_post_act_fake_quantizer, layer2_2_conv2, layer2_2_bn2, add_11, layer2_2_relu_1, layer2_2_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_1_relu_1_post_act_fake_quantizer):
    layer2_2_conv1 = getattr(self.layer2, "2").conv1(layer2_1_relu_1_post_act_fake_quantizer)
    layer2_2_bn1 = getattr(self.layer2, "2").bn1(layer2_2_conv1);  layer2_2_conv1 = None
    layer2_2_relu = getattr(self.layer2, "2").relu(layer2_2_bn1);  layer2_2_bn1 = None
    layer2_2_relu_post_act_fake_quantizer = self.layer2_2_relu_post_act_fake_quantizer(layer2_2_relu);  layer2_2_relu = None
    layer2_2_conv2 = getattr(self.layer2, "2").conv2(layer2_2_relu_post_act_fake_quantizer);  layer2_2_relu_post_act_fake_quantizer = None
    layer2_2_bn2 = getattr(self.layer2, "2").bn2(layer2_2_conv2);  layer2_2_conv2 = None
    add_11 = layer2_2_bn2 + layer2_1_relu_1_post_act_fake_quantizer;  layer2_2_bn2 = layer2_1_relu_1_post_act_fake_quantizer = None
    layer2_2_relu_1 = getattr(self.layer2, "2").relu_dup1(add_11);  add_11 = None
    layer2_2_relu_1_post_act_fake_quantizer = self.layer2_2_relu_1_post_act_fake_quantizer(layer2_2_relu_1);  layer2_2_relu_1 = None
    return layer2_2_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_2_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_3_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_2_relu_1_post_act_fake_quantizer, layer2_3_conv1, layer2_3_bn1, layer2_3_relu, layer2_3_relu_post_act_fake_quantizer, layer2_3_conv2, layer2_3_bn2, add_12, layer2_3_relu_1, layer2_3_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_2_relu_1_post_act_fake_quantizer):
    layer2_3_conv1 = getattr(self.layer2, "3").conv1(layer2_2_relu_1_post_act_fake_quantizer)
    layer2_3_bn1 = getattr(self.layer2, "3").bn1(layer2_3_conv1);  layer2_3_conv1 = None
    layer2_3_relu = getattr(self.layer2, "3").relu(layer2_3_bn1);  layer2_3_bn1 = None
    layer2_3_relu_post_act_fake_quantizer = self.layer2_3_relu_post_act_fake_quantizer(layer2_3_relu);  layer2_3_relu = None
    layer2_3_conv2 = getattr(self.layer2, "3").conv2(layer2_3_relu_post_act_fake_quantizer);  layer2_3_relu_post_act_fake_quantizer = None
    layer2_3_bn2 = getattr(self.layer2, "3").bn2(layer2_3_conv2);  layer2_3_conv2 = None
    add_12 = layer2_3_bn2 + layer2_2_relu_1_post_act_fake_quantizer;  layer2_3_bn2 = layer2_2_relu_1_post_act_fake_quantizer = None
    layer2_3_relu_1 = getattr(self.layer2, "3").relu_dup1(add_12);  add_12 = None
    layer2_3_relu_1_post_act_fake_quantizer = self.layer2_3_relu_1_post_act_fake_quantizer(layer2_3_relu_1);  layer2_3_relu_1 = None
    return layer2_3_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_3_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_4_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_3_relu_1_post_act_fake_quantizer, layer2_4_conv1, layer2_4_bn1, layer2_4_relu, layer2_4_relu_post_act_fake_quantizer, layer2_4_conv2, layer2_4_bn2, add_13, layer2_4_relu_1, layer2_4_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_3_relu_1_post_act_fake_quantizer):
    layer2_4_conv1 = getattr(self.layer2, "4").conv1(layer2_3_relu_1_post_act_fake_quantizer)
    layer2_4_bn1 = getattr(self.layer2, "4").bn1(layer2_4_conv1);  layer2_4_conv1 = None
    layer2_4_relu = getattr(self.layer2, "4").relu(layer2_4_bn1);  layer2_4_bn1 = None
    layer2_4_relu_post_act_fake_quantizer = self.layer2_4_relu_post_act_fake_quantizer(layer2_4_relu);  layer2_4_relu = None
    layer2_4_conv2 = getattr(self.layer2, "4").conv2(layer2_4_relu_post_act_fake_quantizer);  layer2_4_relu_post_act_fake_quantizer = None
    layer2_4_bn2 = getattr(self.layer2, "4").bn2(layer2_4_conv2);  layer2_4_conv2 = None
    add_13 = layer2_4_bn2 + layer2_3_relu_1_post_act_fake_quantizer;  layer2_4_bn2 = layer2_3_relu_1_post_act_fake_quantizer = None
    layer2_4_relu_1 = getattr(self.layer2, "4").relu_dup1(add_13);  add_13 = None
    layer2_4_relu_1_post_act_fake_quantizer = self.layer2_4_relu_1_post_act_fake_quantizer(layer2_4_relu_1);  layer2_4_relu_1 = None
    return layer2_4_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_4_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_5_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_4_relu_1_post_act_fake_quantizer, layer2_5_conv1, layer2_5_bn1, layer2_5_relu, layer2_5_relu_post_act_fake_quantizer, layer2_5_conv2, layer2_5_bn2, add_14, layer2_5_relu_1, layer2_5_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_4_relu_1_post_act_fake_quantizer):
    layer2_5_conv1 = getattr(self.layer2, "5").conv1(layer2_4_relu_1_post_act_fake_quantizer)
    layer2_5_bn1 = getattr(self.layer2, "5").bn1(layer2_5_conv1);  layer2_5_conv1 = None
    layer2_5_relu = getattr(self.layer2, "5").relu(layer2_5_bn1);  layer2_5_bn1 = None
    layer2_5_relu_post_act_fake_quantizer = self.layer2_5_relu_post_act_fake_quantizer(layer2_5_relu);  layer2_5_relu = None
    layer2_5_conv2 = getattr(self.layer2, "5").conv2(layer2_5_relu_post_act_fake_quantizer);  layer2_5_relu_post_act_fake_quantizer = None
    layer2_5_bn2 = getattr(self.layer2, "5").bn2(layer2_5_conv2);  layer2_5_conv2 = None
    add_14 = layer2_5_bn2 + layer2_4_relu_1_post_act_fake_quantizer;  layer2_5_bn2 = layer2_4_relu_1_post_act_fake_quantizer = None
    layer2_5_relu_1 = getattr(self.layer2, "5").relu_dup1(add_14);  add_14 = None
    layer2_5_relu_1_post_act_fake_quantizer = self.layer2_5_relu_1_post_act_fake_quantizer(layer2_5_relu_1);  layer2_5_relu_1 = None
    return layer2_5_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_5_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_6_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_5_relu_1_post_act_fake_quantizer, layer2_6_conv1, layer2_6_bn1, layer2_6_relu, layer2_6_relu_post_act_fake_quantizer, layer2_6_conv2, layer2_6_bn2, add_15, layer2_6_relu_1, layer2_6_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_5_relu_1_post_act_fake_quantizer):
    layer2_6_conv1 = getattr(self.layer2, "6").conv1(layer2_5_relu_1_post_act_fake_quantizer)
    layer2_6_bn1 = getattr(self.layer2, "6").bn1(layer2_6_conv1);  layer2_6_conv1 = None
    layer2_6_relu = getattr(self.layer2, "6").relu(layer2_6_bn1);  layer2_6_bn1 = None
    layer2_6_relu_post_act_fake_quantizer = self.layer2_6_relu_post_act_fake_quantizer(layer2_6_relu);  layer2_6_relu = None
    layer2_6_conv2 = getattr(self.layer2, "6").conv2(layer2_6_relu_post_act_fake_quantizer);  layer2_6_relu_post_act_fake_quantizer = None
    layer2_6_bn2 = getattr(self.layer2, "6").bn2(layer2_6_conv2);  layer2_6_conv2 = None
    add_15 = layer2_6_bn2 + layer2_5_relu_1_post_act_fake_quantizer;  layer2_6_bn2 = layer2_5_relu_1_post_act_fake_quantizer = None
    layer2_6_relu_1 = getattr(self.layer2, "6").relu_dup1(add_15);  add_15 = None
    layer2_6_relu_1_post_act_fake_quantizer = self.layer2_6_relu_1_post_act_fake_quantizer(layer2_6_relu_1);  layer2_6_relu_1 = None
    return layer2_6_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_6_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_7_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_6_relu_1_post_act_fake_quantizer, layer2_7_conv1, layer2_7_bn1, layer2_7_relu, layer2_7_relu_post_act_fake_quantizer, layer2_7_conv2, layer2_7_bn2, add_16, layer2_7_relu_1, layer2_7_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_6_relu_1_post_act_fake_quantizer):
    layer2_7_conv1 = getattr(self.layer2, "7").conv1(layer2_6_relu_1_post_act_fake_quantizer)
    layer2_7_bn1 = getattr(self.layer2, "7").bn1(layer2_7_conv1);  layer2_7_conv1 = None
    layer2_7_relu = getattr(self.layer2, "7").relu(layer2_7_bn1);  layer2_7_bn1 = None
    layer2_7_relu_post_act_fake_quantizer = self.layer2_7_relu_post_act_fake_quantizer(layer2_7_relu);  layer2_7_relu = None
    layer2_7_conv2 = getattr(self.layer2, "7").conv2(layer2_7_relu_post_act_fake_quantizer);  layer2_7_relu_post_act_fake_quantizer = None
    layer2_7_bn2 = getattr(self.layer2, "7").bn2(layer2_7_conv2);  layer2_7_conv2 = None
    add_16 = layer2_7_bn2 + layer2_6_relu_1_post_act_fake_quantizer;  layer2_7_bn2 = layer2_6_relu_1_post_act_fake_quantizer = None
    layer2_7_relu_1 = getattr(self.layer2, "7").relu_dup1(add_16);  add_16 = None
    layer2_7_relu_1_post_act_fake_quantizer = self.layer2_7_relu_1_post_act_fake_quantizer(layer2_7_relu_1);  layer2_7_relu_1 = None
    return layer2_7_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_7_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_8_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_7_relu_1_post_act_fake_quantizer, layer2_8_conv1, layer2_8_bn1, layer2_8_relu, layer2_8_relu_post_act_fake_quantizer, layer2_8_conv2, layer2_8_bn2, add_17, layer2_8_relu_1, layer2_8_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_7_relu_1_post_act_fake_quantizer):
    layer2_8_conv1 = getattr(self.layer2, "8").conv1(layer2_7_relu_1_post_act_fake_quantizer)
    layer2_8_bn1 = getattr(self.layer2, "8").bn1(layer2_8_conv1);  layer2_8_conv1 = None
    layer2_8_relu = getattr(self.layer2, "8").relu(layer2_8_bn1);  layer2_8_bn1 = None
    layer2_8_relu_post_act_fake_quantizer = self.layer2_8_relu_post_act_fake_quantizer(layer2_8_relu);  layer2_8_relu = None
    layer2_8_conv2 = getattr(self.layer2, "8").conv2(layer2_8_relu_post_act_fake_quantizer);  layer2_8_relu_post_act_fake_quantizer = None
    layer2_8_bn2 = getattr(self.layer2, "8").bn2(layer2_8_conv2);  layer2_8_conv2 = None
    add_17 = layer2_8_bn2 + layer2_7_relu_1_post_act_fake_quantizer;  layer2_8_bn2 = layer2_7_relu_1_post_act_fake_quantizer = None
    layer2_8_relu_1 = getattr(self.layer2, "8").relu_dup1(add_17);  add_17 = None
    layer2_8_relu_1_post_act_fake_quantizer = self.layer2_8_relu_1_post_act_fake_quantizer(layer2_8_relu_1);  layer2_8_relu_1 = None
    return layer2_8_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_8_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_0_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_8_relu_1_post_act_fake_quantizer, layer3_0_conv1, layer3_0_bn1, layer3_0_relu, layer3_0_relu_post_act_fake_quantizer, layer3_0_conv2, layer3_0_bn2, layer3_0_downsample_0, layer3_0_downsample_1, add_18, layer3_0_relu_1, layer3_0_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_8_relu_1_post_act_fake_quantizer):
    layer3_0_conv1 = getattr(self.layer3, "0").conv1(layer2_8_relu_1_post_act_fake_quantizer)
    layer3_0_bn1 = getattr(self.layer3, "0").bn1(layer3_0_conv1);  layer3_0_conv1 = None
    layer3_0_relu = getattr(self.layer3, "0").relu(layer3_0_bn1);  layer3_0_bn1 = None
    layer3_0_relu_post_act_fake_quantizer = self.layer3_0_relu_post_act_fake_quantizer(layer3_0_relu);  layer3_0_relu = None
    layer3_0_conv2 = getattr(self.layer3, "0").conv2(layer3_0_relu_post_act_fake_quantizer);  layer3_0_relu_post_act_fake_quantizer = None
    layer3_0_bn2 = getattr(self.layer3, "0").bn2(layer3_0_conv2);  layer3_0_conv2 = None
    layer3_0_downsample_0 = getattr(getattr(self.layer3, "0").downsample, "0")(layer2_8_relu_1_post_act_fake_quantizer);  layer2_8_relu_1_post_act_fake_quantizer = None
    layer3_0_downsample_1 = getattr(getattr(self.layer3, "0").downsample, "1")(layer3_0_downsample_0);  layer3_0_downsample_0 = None
    add_18 = layer3_0_bn2 + layer3_0_downsample_1;  layer3_0_bn2 = layer3_0_downsample_1 = None
    layer3_0_relu_1 = getattr(self.layer3, "0").relu_dup1(add_18);  add_18 = None
    layer3_0_relu_1_post_act_fake_quantizer = self.layer3_0_relu_1_post_act_fake_quantizer(layer3_0_relu_1);  layer3_0_relu_1 = None
    return layer3_0_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_0_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_1_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_0_relu_1_post_act_fake_quantizer, layer3_1_conv1, layer3_1_bn1, layer3_1_relu, layer3_1_relu_post_act_fake_quantizer, layer3_1_conv2, layer3_1_bn2, add_19, layer3_1_relu_1, layer3_1_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_0_relu_1_post_act_fake_quantizer):
    layer3_1_conv1 = getattr(self.layer3, "1").conv1(layer3_0_relu_1_post_act_fake_quantizer)
    layer3_1_bn1 = getattr(self.layer3, "1").bn1(layer3_1_conv1);  layer3_1_conv1 = None
    layer3_1_relu = getattr(self.layer3, "1").relu(layer3_1_bn1);  layer3_1_bn1 = None
    layer3_1_relu_post_act_fake_quantizer = self.layer3_1_relu_post_act_fake_quantizer(layer3_1_relu);  layer3_1_relu = None
    layer3_1_conv2 = getattr(self.layer3, "1").conv2(layer3_1_relu_post_act_fake_quantizer);  layer3_1_relu_post_act_fake_quantizer = None
    layer3_1_bn2 = getattr(self.layer3, "1").bn2(layer3_1_conv2);  layer3_1_conv2 = None
    add_19 = layer3_1_bn2 + layer3_0_relu_1_post_act_fake_quantizer;  layer3_1_bn2 = layer3_0_relu_1_post_act_fake_quantizer = None
    layer3_1_relu_1 = getattr(self.layer3, "1").relu_dup1(add_19);  add_19 = None
    layer3_1_relu_1_post_act_fake_quantizer = self.layer3_1_relu_1_post_act_fake_quantizer(layer3_1_relu_1);  layer3_1_relu_1 = None
    return layer3_1_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_1_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_2_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_1_relu_1_post_act_fake_quantizer, layer3_2_conv1, layer3_2_bn1, layer3_2_relu, layer3_2_relu_post_act_fake_quantizer, layer3_2_conv2, layer3_2_bn2, add_20, layer3_2_relu_1, layer3_2_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_1_relu_1_post_act_fake_quantizer):
    layer3_2_conv1 = getattr(self.layer3, "2").conv1(layer3_1_relu_1_post_act_fake_quantizer)
    layer3_2_bn1 = getattr(self.layer3, "2").bn1(layer3_2_conv1);  layer3_2_conv1 = None
    layer3_2_relu = getattr(self.layer3, "2").relu(layer3_2_bn1);  layer3_2_bn1 = None
    layer3_2_relu_post_act_fake_quantizer = self.layer3_2_relu_post_act_fake_quantizer(layer3_2_relu);  layer3_2_relu = None
    layer3_2_conv2 = getattr(self.layer3, "2").conv2(layer3_2_relu_post_act_fake_quantizer);  layer3_2_relu_post_act_fake_quantizer = None
    layer3_2_bn2 = getattr(self.layer3, "2").bn2(layer3_2_conv2);  layer3_2_conv2 = None
    add_20 = layer3_2_bn2 + layer3_1_relu_1_post_act_fake_quantizer;  layer3_2_bn2 = layer3_1_relu_1_post_act_fake_quantizer = None
    layer3_2_relu_1 = getattr(self.layer3, "2").relu_dup1(add_20);  add_20 = None
    layer3_2_relu_1_post_act_fake_quantizer = self.layer3_2_relu_1_post_act_fake_quantizer(layer3_2_relu_1);  layer3_2_relu_1 = None
    return layer3_2_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_2_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_3_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_2_relu_1_post_act_fake_quantizer, layer3_3_conv1, layer3_3_bn1, layer3_3_relu, layer3_3_relu_post_act_fake_quantizer, layer3_3_conv2, layer3_3_bn2, add_21, layer3_3_relu_1, layer3_3_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_2_relu_1_post_act_fake_quantizer):
    layer3_3_conv1 = getattr(self.layer3, "3").conv1(layer3_2_relu_1_post_act_fake_quantizer)
    layer3_3_bn1 = getattr(self.layer3, "3").bn1(layer3_3_conv1);  layer3_3_conv1 = None
    layer3_3_relu = getattr(self.layer3, "3").relu(layer3_3_bn1);  layer3_3_bn1 = None
    layer3_3_relu_post_act_fake_quantizer = self.layer3_3_relu_post_act_fake_quantizer(layer3_3_relu);  layer3_3_relu = None
    layer3_3_conv2 = getattr(self.layer3, "3").conv2(layer3_3_relu_post_act_fake_quantizer);  layer3_3_relu_post_act_fake_quantizer = None
    layer3_3_bn2 = getattr(self.layer3, "3").bn2(layer3_3_conv2);  layer3_3_conv2 = None
    add_21 = layer3_3_bn2 + layer3_2_relu_1_post_act_fake_quantizer;  layer3_3_bn2 = layer3_2_relu_1_post_act_fake_quantizer = None
    layer3_3_relu_1 = getattr(self.layer3, "3").relu_dup1(add_21);  add_21 = None
    layer3_3_relu_1_post_act_fake_quantizer = self.layer3_3_relu_1_post_act_fake_quantizer(layer3_3_relu_1);  layer3_3_relu_1 = None
    return layer3_3_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_3_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_4_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_3_relu_1_post_act_fake_quantizer, layer3_4_conv1, layer3_4_bn1, layer3_4_relu, layer3_4_relu_post_act_fake_quantizer, layer3_4_conv2, layer3_4_bn2, add_22, layer3_4_relu_1, layer3_4_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_3_relu_1_post_act_fake_quantizer):
    layer3_4_conv1 = getattr(self.layer3, "4").conv1(layer3_3_relu_1_post_act_fake_quantizer)
    layer3_4_bn1 = getattr(self.layer3, "4").bn1(layer3_4_conv1);  layer3_4_conv1 = None
    layer3_4_relu = getattr(self.layer3, "4").relu(layer3_4_bn1);  layer3_4_bn1 = None
    layer3_4_relu_post_act_fake_quantizer = self.layer3_4_relu_post_act_fake_quantizer(layer3_4_relu);  layer3_4_relu = None
    layer3_4_conv2 = getattr(self.layer3, "4").conv2(layer3_4_relu_post_act_fake_quantizer);  layer3_4_relu_post_act_fake_quantizer = None
    layer3_4_bn2 = getattr(self.layer3, "4").bn2(layer3_4_conv2);  layer3_4_conv2 = None
    add_22 = layer3_4_bn2 + layer3_3_relu_1_post_act_fake_quantizer;  layer3_4_bn2 = layer3_3_relu_1_post_act_fake_quantizer = None
    layer3_4_relu_1 = getattr(self.layer3, "4").relu_dup1(add_22);  add_22 = None
    layer3_4_relu_1_post_act_fake_quantizer = self.layer3_4_relu_1_post_act_fake_quantizer(layer3_4_relu_1);  layer3_4_relu_1 = None
    return layer3_4_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_4_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_5_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_4_relu_1_post_act_fake_quantizer, layer3_5_conv1, layer3_5_bn1, layer3_5_relu, layer3_5_relu_post_act_fake_quantizer, layer3_5_conv2, layer3_5_bn2, add_23, layer3_5_relu_1, layer3_5_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_4_relu_1_post_act_fake_quantizer):
    layer3_5_conv1 = getattr(self.layer3, "5").conv1(layer3_4_relu_1_post_act_fake_quantizer)
    layer3_5_bn1 = getattr(self.layer3, "5").bn1(layer3_5_conv1);  layer3_5_conv1 = None
    layer3_5_relu = getattr(self.layer3, "5").relu(layer3_5_bn1);  layer3_5_bn1 = None
    layer3_5_relu_post_act_fake_quantizer = self.layer3_5_relu_post_act_fake_quantizer(layer3_5_relu);  layer3_5_relu = None
    layer3_5_conv2 = getattr(self.layer3, "5").conv2(layer3_5_relu_post_act_fake_quantizer);  layer3_5_relu_post_act_fake_quantizer = None
    layer3_5_bn2 = getattr(self.layer3, "5").bn2(layer3_5_conv2);  layer3_5_conv2 = None
    add_23 = layer3_5_bn2 + layer3_4_relu_1_post_act_fake_quantizer;  layer3_5_bn2 = layer3_4_relu_1_post_act_fake_quantizer = None
    layer3_5_relu_1 = getattr(self.layer3, "5").relu_dup1(add_23);  add_23 = None
    layer3_5_relu_1_post_act_fake_quantizer = self.layer3_5_relu_1_post_act_fake_quantizer(layer3_5_relu_1);  layer3_5_relu_1 = None
    return layer3_5_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_5_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_6_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_5_relu_1_post_act_fake_quantizer, layer3_6_conv1, layer3_6_bn1, layer3_6_relu, layer3_6_relu_post_act_fake_quantizer, layer3_6_conv2, layer3_6_bn2, add_24, layer3_6_relu_1, layer3_6_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_5_relu_1_post_act_fake_quantizer):
    layer3_6_conv1 = getattr(self.layer3, "6").conv1(layer3_5_relu_1_post_act_fake_quantizer)
    layer3_6_bn1 = getattr(self.layer3, "6").bn1(layer3_6_conv1);  layer3_6_conv1 = None
    layer3_6_relu = getattr(self.layer3, "6").relu(layer3_6_bn1);  layer3_6_bn1 = None
    layer3_6_relu_post_act_fake_quantizer = self.layer3_6_relu_post_act_fake_quantizer(layer3_6_relu);  layer3_6_relu = None
    layer3_6_conv2 = getattr(self.layer3, "6").conv2(layer3_6_relu_post_act_fake_quantizer);  layer3_6_relu_post_act_fake_quantizer = None
    layer3_6_bn2 = getattr(self.layer3, "6").bn2(layer3_6_conv2);  layer3_6_conv2 = None
    add_24 = layer3_6_bn2 + layer3_5_relu_1_post_act_fake_quantizer;  layer3_6_bn2 = layer3_5_relu_1_post_act_fake_quantizer = None
    layer3_6_relu_1 = getattr(self.layer3, "6").relu_dup1(add_24);  add_24 = None
    layer3_6_relu_1_post_act_fake_quantizer = self.layer3_6_relu_1_post_act_fake_quantizer(layer3_6_relu_1);  layer3_6_relu_1 = None
    return layer3_6_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_6_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_7_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_6_relu_1_post_act_fake_quantizer, layer3_7_conv1, layer3_7_bn1, layer3_7_relu, layer3_7_relu_post_act_fake_quantizer, layer3_7_conv2, layer3_7_bn2, add_25, layer3_7_relu_1, layer3_7_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_6_relu_1_post_act_fake_quantizer):
    layer3_7_conv1 = getattr(self.layer3, "7").conv1(layer3_6_relu_1_post_act_fake_quantizer)
    layer3_7_bn1 = getattr(self.layer3, "7").bn1(layer3_7_conv1);  layer3_7_conv1 = None
    layer3_7_relu = getattr(self.layer3, "7").relu(layer3_7_bn1);  layer3_7_bn1 = None
    layer3_7_relu_post_act_fake_quantizer = self.layer3_7_relu_post_act_fake_quantizer(layer3_7_relu);  layer3_7_relu = None
    layer3_7_conv2 = getattr(self.layer3, "7").conv2(layer3_7_relu_post_act_fake_quantizer);  layer3_7_relu_post_act_fake_quantizer = None
    layer3_7_bn2 = getattr(self.layer3, "7").bn2(layer3_7_conv2);  layer3_7_conv2 = None
    add_25 = layer3_7_bn2 + layer3_6_relu_1_post_act_fake_quantizer;  layer3_7_bn2 = layer3_6_relu_1_post_act_fake_quantizer = None
    layer3_7_relu_1 = getattr(self.layer3, "7").relu_dup1(add_25);  add_25 = None
    layer3_7_relu_1_post_act_fake_quantizer = self.layer3_7_relu_1_post_act_fake_quantizer(layer3_7_relu_1);  layer3_7_relu_1 = None
    return layer3_7_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_7_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_8_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_7_relu_1_post_act_fake_quantizer, layer3_8_conv1, layer3_8_bn1, layer3_8_relu, layer3_8_relu_post_act_fake_quantizer, layer3_8_conv2, layer3_8_bn2, add_26, layer3_8_relu_1, avgpool, size, view, view_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_7_relu_1_post_act_fake_quantizer):
    layer3_8_conv1 = getattr(self.layer3, "8").conv1(layer3_7_relu_1_post_act_fake_quantizer)
    layer3_8_bn1 = getattr(self.layer3, "8").bn1(layer3_8_conv1);  layer3_8_conv1 = None
    layer3_8_relu = getattr(self.layer3, "8").relu(layer3_8_bn1);  layer3_8_bn1 = None
    layer3_8_relu_post_act_fake_quantizer = self.layer3_8_relu_post_act_fake_quantizer(layer3_8_relu);  layer3_8_relu = None
    layer3_8_conv2 = getattr(self.layer3, "8").conv2(layer3_8_relu_post_act_fake_quantizer);  layer3_8_relu_post_act_fake_quantizer = None
    layer3_8_bn2 = getattr(self.layer3, "8").bn2(layer3_8_conv2);  layer3_8_conv2 = None
    add_26 = layer3_8_bn2 + layer3_7_relu_1_post_act_fake_quantizer;  layer3_8_bn2 = layer3_7_relu_1_post_act_fake_quantizer = None
    layer3_8_relu_1 = getattr(self.layer3, "8").relu_dup1(add_26);  add_26 = None
    avgpool = self.avgpool(layer3_8_relu_1);  layer3_8_relu_1 = None
    size = avgpool.size(0)
    view = avgpool.view(size, -1);  avgpool = size = None
    view_post_act_fake_quantizer = self.view_post_act_fake_quantizer(view);  view = None
    return view_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for view_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for fc
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [view_post_act_fake_quantizer, fc]
[MQBENCH] INFO: 


def forward(self, view_post_act_fake_quantizer):
    fc = self.fc(view_post_act_fake_quantizer);  view_post_act_fake_quantizer = None
    return fc
    
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node x_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_0_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_0_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_1_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_1_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_2_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_2_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_3_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_3_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_4_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_4_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_5_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_5_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_6_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_6_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_7_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_7_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_8_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_8_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_0_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.downsample.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.downsample.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_0_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_1_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_1_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_2_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_2_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_3_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_3_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_4_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_4_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_5_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_5_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_6_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_6_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_7_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_7_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_8_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_8_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_0_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.downsample.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.downsample.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_0_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_1_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_1_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_2_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_2_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_3_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_3_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_4_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_4_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_5_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_5_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_6_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_6_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_7_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_7_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_8_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node avgpool in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node view_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node fc in quant
QDROP model already saved, now loading QDROP_4bits_cifar100_resnet56.pt
Prepare 8bits model using MQBench
[MQBENCH] INFO: Quantize model Scheme: BackendType.Academic Mode: Eval
[MQBENCH] INFO: Weight Qconfig:
    FakeQuantize: AdaRoundFakeQuantize Params: {}
    Oberver:      MSEObserver Params: Symmetric: True / Bitwidth: 8 / Per channel: False / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Activation Qconfig:
    FakeQuantize: QDropFakeQuantize Params: {}
    Oberver:      EMAMSEObserver Params: Symmetric: False / Bitwidth: 8 / Per channel: False / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Replace module to qat module.
[MQBENCH] INFO: Set layer conv1 to 8 bit.
[MQBENCH] INFO: Set layer fc to 8 bit.
dbg node_to_quantize_output
 odict_keys([x, relu, layer1_0_relu, layer1_0_relu_1, layer1_1_relu, layer1_1_relu_1, layer1_2_relu, layer1_2_relu_1, layer1_3_relu, layer1_3_relu_1, layer1_4_relu, layer1_4_relu_1, layer1_5_relu, layer1_5_relu_1, layer1_6_relu, layer1_6_relu_1, layer1_7_relu, layer1_7_relu_1, layer1_8_relu, layer1_8_relu_1, layer2_0_relu, layer2_0_relu_1, layer2_1_relu, layer2_1_relu_1, layer2_2_relu, layer2_2_relu_1, layer2_3_relu, layer2_3_relu_1, layer2_4_relu, layer2_4_relu_1, layer2_5_relu, layer2_5_relu_1, layer2_6_relu, layer2_6_relu_1, layer2_7_relu, layer2_7_relu_1, layer2_8_relu, layer2_8_relu_1, layer3_0_relu, layer3_0_relu_1, layer3_1_relu, layer3_1_relu_1, layer3_2_relu, layer3_2_relu_1, layer3_3_relu, layer3_3_relu_1, layer3_4_relu, layer3_4_relu_1, layer3_5_relu, layer3_5_relu_1, layer3_6_relu, layer3_6_relu_1, layer3_7_relu, layer3_7_relu_1, layer3_8_relu, view])
[MQBENCH] INFO: Set x post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant x_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_4_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_5_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_6_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_7_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_8_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_4_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_5_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_6_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_7_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_8_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_4_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_5_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_6_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_7_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: Set view post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant view_post_act_fake_quantizer
[MQBENCH] INFO: Enable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: prepare block reconstruction for conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [x_post_act_fake_quantizer, conv1, bn1, relu, relu_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, x_post_act_fake_quantizer):
    conv1 = self.conv1(x_post_act_fake_quantizer);  x_post_act_fake_quantizer = None
    bn1 = self.bn1(conv1);  conv1 = None
    relu = self.relu(bn1);  bn1 = None
    relu_post_act_fake_quantizer = self.relu_post_act_fake_quantizer(relu);  relu = None
    return relu_post_act_fake_quantizer
    
Init alpha to be FP32
[MQBENCH] INFO: learn the scale for relu_post_act_fake_quantizer
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_0_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [relu_post_act_fake_quantizer, layer1_0_conv1, layer1_0_bn1, layer1_0_relu, layer1_0_relu_post_act_fake_quantizer, layer1_0_conv2, layer1_0_bn2, add, layer1_0_relu_1, layer1_0_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, relu_post_act_fake_quantizer):
    layer1_0_conv1 = getattr(self.layer1, "0").conv1(relu_post_act_fake_quantizer)
    layer1_0_bn1 = getattr(self.layer1, "0").bn1(layer1_0_conv1);  layer1_0_conv1 = None
    layer1_0_relu = getattr(self.layer1, "0").relu(layer1_0_bn1);  layer1_0_bn1 = None
    layer1_0_relu_post_act_fake_quantizer = self.layer1_0_relu_post_act_fake_quantizer(layer1_0_relu);  layer1_0_relu = None
    layer1_0_conv2 = getattr(self.layer1, "0").conv2(layer1_0_relu_post_act_fake_quantizer);  layer1_0_relu_post_act_fake_quantizer = None
    layer1_0_bn2 = getattr(self.layer1, "0").bn2(layer1_0_conv2);  layer1_0_conv2 = None
    add = layer1_0_bn2 + relu_post_act_fake_quantizer;  layer1_0_bn2 = relu_post_act_fake_quantizer = None
    layer1_0_relu_1 = getattr(self.layer1, "0").relu_dup1(add);  add = None
    layer1_0_relu_1_post_act_fake_quantizer = self.layer1_0_relu_1_post_act_fake_quantizer(layer1_0_relu_1);  layer1_0_relu_1 = None
    return layer1_0_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_0_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_1_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_0_relu_1_post_act_fake_quantizer, layer1_1_conv1, layer1_1_bn1, layer1_1_relu, layer1_1_relu_post_act_fake_quantizer, layer1_1_conv2, layer1_1_bn2, add_1, layer1_1_relu_1, layer1_1_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_0_relu_1_post_act_fake_quantizer):
    layer1_1_conv1 = getattr(self.layer1, "1").conv1(layer1_0_relu_1_post_act_fake_quantizer)
    layer1_1_bn1 = getattr(self.layer1, "1").bn1(layer1_1_conv1);  layer1_1_conv1 = None
    layer1_1_relu = getattr(self.layer1, "1").relu(layer1_1_bn1);  layer1_1_bn1 = None
    layer1_1_relu_post_act_fake_quantizer = self.layer1_1_relu_post_act_fake_quantizer(layer1_1_relu);  layer1_1_relu = None
    layer1_1_conv2 = getattr(self.layer1, "1").conv2(layer1_1_relu_post_act_fake_quantizer);  layer1_1_relu_post_act_fake_quantizer = None
    layer1_1_bn2 = getattr(self.layer1, "1").bn2(layer1_1_conv2);  layer1_1_conv2 = None
    add_1 = layer1_1_bn2 + layer1_0_relu_1_post_act_fake_quantizer;  layer1_1_bn2 = layer1_0_relu_1_post_act_fake_quantizer = None
    layer1_1_relu_1 = getattr(self.layer1, "1").relu_dup1(add_1);  add_1 = None
    layer1_1_relu_1_post_act_fake_quantizer = self.layer1_1_relu_1_post_act_fake_quantizer(layer1_1_relu_1);  layer1_1_relu_1 = None
    return layer1_1_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_1_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_2_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_1_relu_1_post_act_fake_quantizer, layer1_2_conv1, layer1_2_bn1, layer1_2_relu, layer1_2_relu_post_act_fake_quantizer, layer1_2_conv2, layer1_2_bn2, add_2, layer1_2_relu_1, layer1_2_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_1_relu_1_post_act_fake_quantizer):
    layer1_2_conv1 = getattr(self.layer1, "2").conv1(layer1_1_relu_1_post_act_fake_quantizer)
    layer1_2_bn1 = getattr(self.layer1, "2").bn1(layer1_2_conv1);  layer1_2_conv1 = None
    layer1_2_relu = getattr(self.layer1, "2").relu(layer1_2_bn1);  layer1_2_bn1 = None
    layer1_2_relu_post_act_fake_quantizer = self.layer1_2_relu_post_act_fake_quantizer(layer1_2_relu);  layer1_2_relu = None
    layer1_2_conv2 = getattr(self.layer1, "2").conv2(layer1_2_relu_post_act_fake_quantizer);  layer1_2_relu_post_act_fake_quantizer = None
    layer1_2_bn2 = getattr(self.layer1, "2").bn2(layer1_2_conv2);  layer1_2_conv2 = None
    add_2 = layer1_2_bn2 + layer1_1_relu_1_post_act_fake_quantizer;  layer1_2_bn2 = layer1_1_relu_1_post_act_fake_quantizer = None
    layer1_2_relu_1 = getattr(self.layer1, "2").relu_dup1(add_2);  add_2 = None
    layer1_2_relu_1_post_act_fake_quantizer = self.layer1_2_relu_1_post_act_fake_quantizer(layer1_2_relu_1);  layer1_2_relu_1 = None
    return layer1_2_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_2_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_3_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_2_relu_1_post_act_fake_quantizer, layer1_3_conv1, layer1_3_bn1, layer1_3_relu, layer1_3_relu_post_act_fake_quantizer, layer1_3_conv2, layer1_3_bn2, add_3, layer1_3_relu_1, layer1_3_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_2_relu_1_post_act_fake_quantizer):
    layer1_3_conv1 = getattr(self.layer1, "3").conv1(layer1_2_relu_1_post_act_fake_quantizer)
    layer1_3_bn1 = getattr(self.layer1, "3").bn1(layer1_3_conv1);  layer1_3_conv1 = None
    layer1_3_relu = getattr(self.layer1, "3").relu(layer1_3_bn1);  layer1_3_bn1 = None
    layer1_3_relu_post_act_fake_quantizer = self.layer1_3_relu_post_act_fake_quantizer(layer1_3_relu);  layer1_3_relu = None
    layer1_3_conv2 = getattr(self.layer1, "3").conv2(layer1_3_relu_post_act_fake_quantizer);  layer1_3_relu_post_act_fake_quantizer = None
    layer1_3_bn2 = getattr(self.layer1, "3").bn2(layer1_3_conv2);  layer1_3_conv2 = None
    add_3 = layer1_3_bn2 + layer1_2_relu_1_post_act_fake_quantizer;  layer1_3_bn2 = layer1_2_relu_1_post_act_fake_quantizer = None
    layer1_3_relu_1 = getattr(self.layer1, "3").relu_dup1(add_3);  add_3 = None
    layer1_3_relu_1_post_act_fake_quantizer = self.layer1_3_relu_1_post_act_fake_quantizer(layer1_3_relu_1);  layer1_3_relu_1 = None
    return layer1_3_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_3_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_4_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_3_relu_1_post_act_fake_quantizer, layer1_4_conv1, layer1_4_bn1, layer1_4_relu, layer1_4_relu_post_act_fake_quantizer, layer1_4_conv2, layer1_4_bn2, add_4, layer1_4_relu_1, layer1_4_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_3_relu_1_post_act_fake_quantizer):
    layer1_4_conv1 = getattr(self.layer1, "4").conv1(layer1_3_relu_1_post_act_fake_quantizer)
    layer1_4_bn1 = getattr(self.layer1, "4").bn1(layer1_4_conv1);  layer1_4_conv1 = None
    layer1_4_relu = getattr(self.layer1, "4").relu(layer1_4_bn1);  layer1_4_bn1 = None
    layer1_4_relu_post_act_fake_quantizer = self.layer1_4_relu_post_act_fake_quantizer(layer1_4_relu);  layer1_4_relu = None
    layer1_4_conv2 = getattr(self.layer1, "4").conv2(layer1_4_relu_post_act_fake_quantizer);  layer1_4_relu_post_act_fake_quantizer = None
    layer1_4_bn2 = getattr(self.layer1, "4").bn2(layer1_4_conv2);  layer1_4_conv2 = None
    add_4 = layer1_4_bn2 + layer1_3_relu_1_post_act_fake_quantizer;  layer1_4_bn2 = layer1_3_relu_1_post_act_fake_quantizer = None
    layer1_4_relu_1 = getattr(self.layer1, "4").relu_dup1(add_4);  add_4 = None
    layer1_4_relu_1_post_act_fake_quantizer = self.layer1_4_relu_1_post_act_fake_quantizer(layer1_4_relu_1);  layer1_4_relu_1 = None
    return layer1_4_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_4_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_5_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_4_relu_1_post_act_fake_quantizer, layer1_5_conv1, layer1_5_bn1, layer1_5_relu, layer1_5_relu_post_act_fake_quantizer, layer1_5_conv2, layer1_5_bn2, add_5, layer1_5_relu_1, layer1_5_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_4_relu_1_post_act_fake_quantizer):
    layer1_5_conv1 = getattr(self.layer1, "5").conv1(layer1_4_relu_1_post_act_fake_quantizer)
    layer1_5_bn1 = getattr(self.layer1, "5").bn1(layer1_5_conv1);  layer1_5_conv1 = None
    layer1_5_relu = getattr(self.layer1, "5").relu(layer1_5_bn1);  layer1_5_bn1 = None
    layer1_5_relu_post_act_fake_quantizer = self.layer1_5_relu_post_act_fake_quantizer(layer1_5_relu);  layer1_5_relu = None
    layer1_5_conv2 = getattr(self.layer1, "5").conv2(layer1_5_relu_post_act_fake_quantizer);  layer1_5_relu_post_act_fake_quantizer = None
    layer1_5_bn2 = getattr(self.layer1, "5").bn2(layer1_5_conv2);  layer1_5_conv2 = None
    add_5 = layer1_5_bn2 + layer1_4_relu_1_post_act_fake_quantizer;  layer1_5_bn2 = layer1_4_relu_1_post_act_fake_quantizer = None
    layer1_5_relu_1 = getattr(self.layer1, "5").relu_dup1(add_5);  add_5 = None
    layer1_5_relu_1_post_act_fake_quantizer = self.layer1_5_relu_1_post_act_fake_quantizer(layer1_5_relu_1);  layer1_5_relu_1 = None
    return layer1_5_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_5_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_6_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_5_relu_1_post_act_fake_quantizer, layer1_6_conv1, layer1_6_bn1, layer1_6_relu, layer1_6_relu_post_act_fake_quantizer, layer1_6_conv2, layer1_6_bn2, add_6, layer1_6_relu_1, layer1_6_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_5_relu_1_post_act_fake_quantizer):
    layer1_6_conv1 = getattr(self.layer1, "6").conv1(layer1_5_relu_1_post_act_fake_quantizer)
    layer1_6_bn1 = getattr(self.layer1, "6").bn1(layer1_6_conv1);  layer1_6_conv1 = None
    layer1_6_relu = getattr(self.layer1, "6").relu(layer1_6_bn1);  layer1_6_bn1 = None
    layer1_6_relu_post_act_fake_quantizer = self.layer1_6_relu_post_act_fake_quantizer(layer1_6_relu);  layer1_6_relu = None
    layer1_6_conv2 = getattr(self.layer1, "6").conv2(layer1_6_relu_post_act_fake_quantizer);  layer1_6_relu_post_act_fake_quantizer = None
    layer1_6_bn2 = getattr(self.layer1, "6").bn2(layer1_6_conv2);  layer1_6_conv2 = None
    add_6 = layer1_6_bn2 + layer1_5_relu_1_post_act_fake_quantizer;  layer1_6_bn2 = layer1_5_relu_1_post_act_fake_quantizer = None
    layer1_6_relu_1 = getattr(self.layer1, "6").relu_dup1(add_6);  add_6 = None
    layer1_6_relu_1_post_act_fake_quantizer = self.layer1_6_relu_1_post_act_fake_quantizer(layer1_6_relu_1);  layer1_6_relu_1 = None
    return layer1_6_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_6_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_7_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_6_relu_1_post_act_fake_quantizer, layer1_7_conv1, layer1_7_bn1, layer1_7_relu, layer1_7_relu_post_act_fake_quantizer, layer1_7_conv2, layer1_7_bn2, add_7, layer1_7_relu_1, layer1_7_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_6_relu_1_post_act_fake_quantizer):
    layer1_7_conv1 = getattr(self.layer1, "7").conv1(layer1_6_relu_1_post_act_fake_quantizer)
    layer1_7_bn1 = getattr(self.layer1, "7").bn1(layer1_7_conv1);  layer1_7_conv1 = None
    layer1_7_relu = getattr(self.layer1, "7").relu(layer1_7_bn1);  layer1_7_bn1 = None
    layer1_7_relu_post_act_fake_quantizer = self.layer1_7_relu_post_act_fake_quantizer(layer1_7_relu);  layer1_7_relu = None
    layer1_7_conv2 = getattr(self.layer1, "7").conv2(layer1_7_relu_post_act_fake_quantizer);  layer1_7_relu_post_act_fake_quantizer = None
    layer1_7_bn2 = getattr(self.layer1, "7").bn2(layer1_7_conv2);  layer1_7_conv2 = None
    add_7 = layer1_7_bn2 + layer1_6_relu_1_post_act_fake_quantizer;  layer1_7_bn2 = layer1_6_relu_1_post_act_fake_quantizer = None
    layer1_7_relu_1 = getattr(self.layer1, "7").relu_dup1(add_7);  add_7 = None
    layer1_7_relu_1_post_act_fake_quantizer = self.layer1_7_relu_1_post_act_fake_quantizer(layer1_7_relu_1);  layer1_7_relu_1 = None
    return layer1_7_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_7_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer1_8_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_7_relu_1_post_act_fake_quantizer, layer1_8_conv1, layer1_8_bn1, layer1_8_relu, layer1_8_relu_post_act_fake_quantizer, layer1_8_conv2, layer1_8_bn2, add_8, layer1_8_relu_1, layer1_8_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_7_relu_1_post_act_fake_quantizer):
    layer1_8_conv1 = getattr(self.layer1, "8").conv1(layer1_7_relu_1_post_act_fake_quantizer)
    layer1_8_bn1 = getattr(self.layer1, "8").bn1(layer1_8_conv1);  layer1_8_conv1 = None
    layer1_8_relu = getattr(self.layer1, "8").relu(layer1_8_bn1);  layer1_8_bn1 = None
    layer1_8_relu_post_act_fake_quantizer = self.layer1_8_relu_post_act_fake_quantizer(layer1_8_relu);  layer1_8_relu = None
    layer1_8_conv2 = getattr(self.layer1, "8").conv2(layer1_8_relu_post_act_fake_quantizer);  layer1_8_relu_post_act_fake_quantizer = None
    layer1_8_bn2 = getattr(self.layer1, "8").bn2(layer1_8_conv2);  layer1_8_conv2 = None
    add_8 = layer1_8_bn2 + layer1_7_relu_1_post_act_fake_quantizer;  layer1_8_bn2 = layer1_7_relu_1_post_act_fake_quantizer = None
    layer1_8_relu_1 = getattr(self.layer1, "8").relu_dup1(add_8);  add_8 = None
    layer1_8_relu_1_post_act_fake_quantizer = self.layer1_8_relu_1_post_act_fake_quantizer(layer1_8_relu_1);  layer1_8_relu_1 = None
    return layer1_8_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_8_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_0_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_8_relu_1_post_act_fake_quantizer, layer2_0_conv1, layer2_0_bn1, layer2_0_relu, layer2_0_relu_post_act_fake_quantizer, layer2_0_conv2, layer2_0_bn2, layer2_0_downsample_0, layer2_0_downsample_1, add_9, layer2_0_relu_1, layer2_0_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer1_8_relu_1_post_act_fake_quantizer):
    layer2_0_conv1 = getattr(self.layer2, "0").conv1(layer1_8_relu_1_post_act_fake_quantizer)
    layer2_0_bn1 = getattr(self.layer2, "0").bn1(layer2_0_conv1);  layer2_0_conv1 = None
    layer2_0_relu = getattr(self.layer2, "0").relu(layer2_0_bn1);  layer2_0_bn1 = None
    layer2_0_relu_post_act_fake_quantizer = self.layer2_0_relu_post_act_fake_quantizer(layer2_0_relu);  layer2_0_relu = None
    layer2_0_conv2 = getattr(self.layer2, "0").conv2(layer2_0_relu_post_act_fake_quantizer);  layer2_0_relu_post_act_fake_quantizer = None
    layer2_0_bn2 = getattr(self.layer2, "0").bn2(layer2_0_conv2);  layer2_0_conv2 = None
    layer2_0_downsample_0 = getattr(getattr(self.layer2, "0").downsample, "0")(layer1_8_relu_1_post_act_fake_quantizer);  layer1_8_relu_1_post_act_fake_quantizer = None
    layer2_0_downsample_1 = getattr(getattr(self.layer2, "0").downsample, "1")(layer2_0_downsample_0);  layer2_0_downsample_0 = None
    add_9 = layer2_0_bn2 + layer2_0_downsample_1;  layer2_0_bn2 = layer2_0_downsample_1 = None
    layer2_0_relu_1 = getattr(self.layer2, "0").relu_dup1(add_9);  add_9 = None
    layer2_0_relu_1_post_act_fake_quantizer = self.layer2_0_relu_1_post_act_fake_quantizer(layer2_0_relu_1);  layer2_0_relu_1 = None
    return layer2_0_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_0_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_1_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_0_relu_1_post_act_fake_quantizer, layer2_1_conv1, layer2_1_bn1, layer2_1_relu, layer2_1_relu_post_act_fake_quantizer, layer2_1_conv2, layer2_1_bn2, add_10, layer2_1_relu_1, layer2_1_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_0_relu_1_post_act_fake_quantizer):
    layer2_1_conv1 = getattr(self.layer2, "1").conv1(layer2_0_relu_1_post_act_fake_quantizer)
    layer2_1_bn1 = getattr(self.layer2, "1").bn1(layer2_1_conv1);  layer2_1_conv1 = None
    layer2_1_relu = getattr(self.layer2, "1").relu(layer2_1_bn1);  layer2_1_bn1 = None
    layer2_1_relu_post_act_fake_quantizer = self.layer2_1_relu_post_act_fake_quantizer(layer2_1_relu);  layer2_1_relu = None
    layer2_1_conv2 = getattr(self.layer2, "1").conv2(layer2_1_relu_post_act_fake_quantizer);  layer2_1_relu_post_act_fake_quantizer = None
    layer2_1_bn2 = getattr(self.layer2, "1").bn2(layer2_1_conv2);  layer2_1_conv2 = None
    add_10 = layer2_1_bn2 + layer2_0_relu_1_post_act_fake_quantizer;  layer2_1_bn2 = layer2_0_relu_1_post_act_fake_quantizer = None
    layer2_1_relu_1 = getattr(self.layer2, "1").relu_dup1(add_10);  add_10 = None
    layer2_1_relu_1_post_act_fake_quantizer = self.layer2_1_relu_1_post_act_fake_quantizer(layer2_1_relu_1);  layer2_1_relu_1 = None
    return layer2_1_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_1_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_2_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_1_relu_1_post_act_fake_quantizer, layer2_2_conv1, layer2_2_bn1, layer2_2_relu, layer2_2_relu_post_act_fake_quantizer, layer2_2_conv2, layer2_2_bn2, add_11, layer2_2_relu_1, layer2_2_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_1_relu_1_post_act_fake_quantizer):
    layer2_2_conv1 = getattr(self.layer2, "2").conv1(layer2_1_relu_1_post_act_fake_quantizer)
    layer2_2_bn1 = getattr(self.layer2, "2").bn1(layer2_2_conv1);  layer2_2_conv1 = None
    layer2_2_relu = getattr(self.layer2, "2").relu(layer2_2_bn1);  layer2_2_bn1 = None
    layer2_2_relu_post_act_fake_quantizer = self.layer2_2_relu_post_act_fake_quantizer(layer2_2_relu);  layer2_2_relu = None
    layer2_2_conv2 = getattr(self.layer2, "2").conv2(layer2_2_relu_post_act_fake_quantizer);  layer2_2_relu_post_act_fake_quantizer = None
    layer2_2_bn2 = getattr(self.layer2, "2").bn2(layer2_2_conv2);  layer2_2_conv2 = None
    add_11 = layer2_2_bn2 + layer2_1_relu_1_post_act_fake_quantizer;  layer2_2_bn2 = layer2_1_relu_1_post_act_fake_quantizer = None
    layer2_2_relu_1 = getattr(self.layer2, "2").relu_dup1(add_11);  add_11 = None
    layer2_2_relu_1_post_act_fake_quantizer = self.layer2_2_relu_1_post_act_fake_quantizer(layer2_2_relu_1);  layer2_2_relu_1 = None
    return layer2_2_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_2_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_3_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_2_relu_1_post_act_fake_quantizer, layer2_3_conv1, layer2_3_bn1, layer2_3_relu, layer2_3_relu_post_act_fake_quantizer, layer2_3_conv2, layer2_3_bn2, add_12, layer2_3_relu_1, layer2_3_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_2_relu_1_post_act_fake_quantizer):
    layer2_3_conv1 = getattr(self.layer2, "3").conv1(layer2_2_relu_1_post_act_fake_quantizer)
    layer2_3_bn1 = getattr(self.layer2, "3").bn1(layer2_3_conv1);  layer2_3_conv1 = None
    layer2_3_relu = getattr(self.layer2, "3").relu(layer2_3_bn1);  layer2_3_bn1 = None
    layer2_3_relu_post_act_fake_quantizer = self.layer2_3_relu_post_act_fake_quantizer(layer2_3_relu);  layer2_3_relu = None
    layer2_3_conv2 = getattr(self.layer2, "3").conv2(layer2_3_relu_post_act_fake_quantizer);  layer2_3_relu_post_act_fake_quantizer = None
    layer2_3_bn2 = getattr(self.layer2, "3").bn2(layer2_3_conv2);  layer2_3_conv2 = None
    add_12 = layer2_3_bn2 + layer2_2_relu_1_post_act_fake_quantizer;  layer2_3_bn2 = layer2_2_relu_1_post_act_fake_quantizer = None
    layer2_3_relu_1 = getattr(self.layer2, "3").relu_dup1(add_12);  add_12 = None
    layer2_3_relu_1_post_act_fake_quantizer = self.layer2_3_relu_1_post_act_fake_quantizer(layer2_3_relu_1);  layer2_3_relu_1 = None
    return layer2_3_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_3_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_4_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_3_relu_1_post_act_fake_quantizer, layer2_4_conv1, layer2_4_bn1, layer2_4_relu, layer2_4_relu_post_act_fake_quantizer, layer2_4_conv2, layer2_4_bn2, add_13, layer2_4_relu_1, layer2_4_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_3_relu_1_post_act_fake_quantizer):
    layer2_4_conv1 = getattr(self.layer2, "4").conv1(layer2_3_relu_1_post_act_fake_quantizer)
    layer2_4_bn1 = getattr(self.layer2, "4").bn1(layer2_4_conv1);  layer2_4_conv1 = None
    layer2_4_relu = getattr(self.layer2, "4").relu(layer2_4_bn1);  layer2_4_bn1 = None
    layer2_4_relu_post_act_fake_quantizer = self.layer2_4_relu_post_act_fake_quantizer(layer2_4_relu);  layer2_4_relu = None
    layer2_4_conv2 = getattr(self.layer2, "4").conv2(layer2_4_relu_post_act_fake_quantizer);  layer2_4_relu_post_act_fake_quantizer = None
    layer2_4_bn2 = getattr(self.layer2, "4").bn2(layer2_4_conv2);  layer2_4_conv2 = None
    add_13 = layer2_4_bn2 + layer2_3_relu_1_post_act_fake_quantizer;  layer2_4_bn2 = layer2_3_relu_1_post_act_fake_quantizer = None
    layer2_4_relu_1 = getattr(self.layer2, "4").relu_dup1(add_13);  add_13 = None
    layer2_4_relu_1_post_act_fake_quantizer = self.layer2_4_relu_1_post_act_fake_quantizer(layer2_4_relu_1);  layer2_4_relu_1 = None
    return layer2_4_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_4_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_5_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_4_relu_1_post_act_fake_quantizer, layer2_5_conv1, layer2_5_bn1, layer2_5_relu, layer2_5_relu_post_act_fake_quantizer, layer2_5_conv2, layer2_5_bn2, add_14, layer2_5_relu_1, layer2_5_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_4_relu_1_post_act_fake_quantizer):
    layer2_5_conv1 = getattr(self.layer2, "5").conv1(layer2_4_relu_1_post_act_fake_quantizer)
    layer2_5_bn1 = getattr(self.layer2, "5").bn1(layer2_5_conv1);  layer2_5_conv1 = None
    layer2_5_relu = getattr(self.layer2, "5").relu(layer2_5_bn1);  layer2_5_bn1 = None
    layer2_5_relu_post_act_fake_quantizer = self.layer2_5_relu_post_act_fake_quantizer(layer2_5_relu);  layer2_5_relu = None
    layer2_5_conv2 = getattr(self.layer2, "5").conv2(layer2_5_relu_post_act_fake_quantizer);  layer2_5_relu_post_act_fake_quantizer = None
    layer2_5_bn2 = getattr(self.layer2, "5").bn2(layer2_5_conv2);  layer2_5_conv2 = None
    add_14 = layer2_5_bn2 + layer2_4_relu_1_post_act_fake_quantizer;  layer2_5_bn2 = layer2_4_relu_1_post_act_fake_quantizer = None
    layer2_5_relu_1 = getattr(self.layer2, "5").relu_dup1(add_14);  add_14 = None
    layer2_5_relu_1_post_act_fake_quantizer = self.layer2_5_relu_1_post_act_fake_quantizer(layer2_5_relu_1);  layer2_5_relu_1 = None
    return layer2_5_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_5_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_6_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_5_relu_1_post_act_fake_quantizer, layer2_6_conv1, layer2_6_bn1, layer2_6_relu, layer2_6_relu_post_act_fake_quantizer, layer2_6_conv2, layer2_6_bn2, add_15, layer2_6_relu_1, layer2_6_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_5_relu_1_post_act_fake_quantizer):
    layer2_6_conv1 = getattr(self.layer2, "6").conv1(layer2_5_relu_1_post_act_fake_quantizer)
    layer2_6_bn1 = getattr(self.layer2, "6").bn1(layer2_6_conv1);  layer2_6_conv1 = None
    layer2_6_relu = getattr(self.layer2, "6").relu(layer2_6_bn1);  layer2_6_bn1 = None
    layer2_6_relu_post_act_fake_quantizer = self.layer2_6_relu_post_act_fake_quantizer(layer2_6_relu);  layer2_6_relu = None
    layer2_6_conv2 = getattr(self.layer2, "6").conv2(layer2_6_relu_post_act_fake_quantizer);  layer2_6_relu_post_act_fake_quantizer = None
    layer2_6_bn2 = getattr(self.layer2, "6").bn2(layer2_6_conv2);  layer2_6_conv2 = None
    add_15 = layer2_6_bn2 + layer2_5_relu_1_post_act_fake_quantizer;  layer2_6_bn2 = layer2_5_relu_1_post_act_fake_quantizer = None
    layer2_6_relu_1 = getattr(self.layer2, "6").relu_dup1(add_15);  add_15 = None
    layer2_6_relu_1_post_act_fake_quantizer = self.layer2_6_relu_1_post_act_fake_quantizer(layer2_6_relu_1);  layer2_6_relu_1 = None
    return layer2_6_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_6_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_7_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_6_relu_1_post_act_fake_quantizer, layer2_7_conv1, layer2_7_bn1, layer2_7_relu, layer2_7_relu_post_act_fake_quantizer, layer2_7_conv2, layer2_7_bn2, add_16, layer2_7_relu_1, layer2_7_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_6_relu_1_post_act_fake_quantizer):
    layer2_7_conv1 = getattr(self.layer2, "7").conv1(layer2_6_relu_1_post_act_fake_quantizer)
    layer2_7_bn1 = getattr(self.layer2, "7").bn1(layer2_7_conv1);  layer2_7_conv1 = None
    layer2_7_relu = getattr(self.layer2, "7").relu(layer2_7_bn1);  layer2_7_bn1 = None
    layer2_7_relu_post_act_fake_quantizer = self.layer2_7_relu_post_act_fake_quantizer(layer2_7_relu);  layer2_7_relu = None
    layer2_7_conv2 = getattr(self.layer2, "7").conv2(layer2_7_relu_post_act_fake_quantizer);  layer2_7_relu_post_act_fake_quantizer = None
    layer2_7_bn2 = getattr(self.layer2, "7").bn2(layer2_7_conv2);  layer2_7_conv2 = None
    add_16 = layer2_7_bn2 + layer2_6_relu_1_post_act_fake_quantizer;  layer2_7_bn2 = layer2_6_relu_1_post_act_fake_quantizer = None
    layer2_7_relu_1 = getattr(self.layer2, "7").relu_dup1(add_16);  add_16 = None
    layer2_7_relu_1_post_act_fake_quantizer = self.layer2_7_relu_1_post_act_fake_quantizer(layer2_7_relu_1);  layer2_7_relu_1 = None
    return layer2_7_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_7_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer2_8_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_7_relu_1_post_act_fake_quantizer, layer2_8_conv1, layer2_8_bn1, layer2_8_relu, layer2_8_relu_post_act_fake_quantizer, layer2_8_conv2, layer2_8_bn2, add_17, layer2_8_relu_1, layer2_8_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_7_relu_1_post_act_fake_quantizer):
    layer2_8_conv1 = getattr(self.layer2, "8").conv1(layer2_7_relu_1_post_act_fake_quantizer)
    layer2_8_bn1 = getattr(self.layer2, "8").bn1(layer2_8_conv1);  layer2_8_conv1 = None
    layer2_8_relu = getattr(self.layer2, "8").relu(layer2_8_bn1);  layer2_8_bn1 = None
    layer2_8_relu_post_act_fake_quantizer = self.layer2_8_relu_post_act_fake_quantizer(layer2_8_relu);  layer2_8_relu = None
    layer2_8_conv2 = getattr(self.layer2, "8").conv2(layer2_8_relu_post_act_fake_quantizer);  layer2_8_relu_post_act_fake_quantizer = None
    layer2_8_bn2 = getattr(self.layer2, "8").bn2(layer2_8_conv2);  layer2_8_conv2 = None
    add_17 = layer2_8_bn2 + layer2_7_relu_1_post_act_fake_quantizer;  layer2_8_bn2 = layer2_7_relu_1_post_act_fake_quantizer = None
    layer2_8_relu_1 = getattr(self.layer2, "8").relu_dup1(add_17);  add_17 = None
    layer2_8_relu_1_post_act_fake_quantizer = self.layer2_8_relu_1_post_act_fake_quantizer(layer2_8_relu_1);  layer2_8_relu_1 = None
    return layer2_8_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_8_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_0_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_8_relu_1_post_act_fake_quantizer, layer3_0_conv1, layer3_0_bn1, layer3_0_relu, layer3_0_relu_post_act_fake_quantizer, layer3_0_conv2, layer3_0_bn2, layer3_0_downsample_0, layer3_0_downsample_1, add_18, layer3_0_relu_1, layer3_0_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer2_8_relu_1_post_act_fake_quantizer):
    layer3_0_conv1 = getattr(self.layer3, "0").conv1(layer2_8_relu_1_post_act_fake_quantizer)
    layer3_0_bn1 = getattr(self.layer3, "0").bn1(layer3_0_conv1);  layer3_0_conv1 = None
    layer3_0_relu = getattr(self.layer3, "0").relu(layer3_0_bn1);  layer3_0_bn1 = None
    layer3_0_relu_post_act_fake_quantizer = self.layer3_0_relu_post_act_fake_quantizer(layer3_0_relu);  layer3_0_relu = None
    layer3_0_conv2 = getattr(self.layer3, "0").conv2(layer3_0_relu_post_act_fake_quantizer);  layer3_0_relu_post_act_fake_quantizer = None
    layer3_0_bn2 = getattr(self.layer3, "0").bn2(layer3_0_conv2);  layer3_0_conv2 = None
    layer3_0_downsample_0 = getattr(getattr(self.layer3, "0").downsample, "0")(layer2_8_relu_1_post_act_fake_quantizer);  layer2_8_relu_1_post_act_fake_quantizer = None
    layer3_0_downsample_1 = getattr(getattr(self.layer3, "0").downsample, "1")(layer3_0_downsample_0);  layer3_0_downsample_0 = None
    add_18 = layer3_0_bn2 + layer3_0_downsample_1;  layer3_0_bn2 = layer3_0_downsample_1 = None
    layer3_0_relu_1 = getattr(self.layer3, "0").relu_dup1(add_18);  add_18 = None
    layer3_0_relu_1_post_act_fake_quantizer = self.layer3_0_relu_1_post_act_fake_quantizer(layer3_0_relu_1);  layer3_0_relu_1 = None
    return layer3_0_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_0_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_1_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_0_relu_1_post_act_fake_quantizer, layer3_1_conv1, layer3_1_bn1, layer3_1_relu, layer3_1_relu_post_act_fake_quantizer, layer3_1_conv2, layer3_1_bn2, add_19, layer3_1_relu_1, layer3_1_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_0_relu_1_post_act_fake_quantizer):
    layer3_1_conv1 = getattr(self.layer3, "1").conv1(layer3_0_relu_1_post_act_fake_quantizer)
    layer3_1_bn1 = getattr(self.layer3, "1").bn1(layer3_1_conv1);  layer3_1_conv1 = None
    layer3_1_relu = getattr(self.layer3, "1").relu(layer3_1_bn1);  layer3_1_bn1 = None
    layer3_1_relu_post_act_fake_quantizer = self.layer3_1_relu_post_act_fake_quantizer(layer3_1_relu);  layer3_1_relu = None
    layer3_1_conv2 = getattr(self.layer3, "1").conv2(layer3_1_relu_post_act_fake_quantizer);  layer3_1_relu_post_act_fake_quantizer = None
    layer3_1_bn2 = getattr(self.layer3, "1").bn2(layer3_1_conv2);  layer3_1_conv2 = None
    add_19 = layer3_1_bn2 + layer3_0_relu_1_post_act_fake_quantizer;  layer3_1_bn2 = layer3_0_relu_1_post_act_fake_quantizer = None
    layer3_1_relu_1 = getattr(self.layer3, "1").relu_dup1(add_19);  add_19 = None
    layer3_1_relu_1_post_act_fake_quantizer = self.layer3_1_relu_1_post_act_fake_quantizer(layer3_1_relu_1);  layer3_1_relu_1 = None
    return layer3_1_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_1_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_2_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_1_relu_1_post_act_fake_quantizer, layer3_2_conv1, layer3_2_bn1, layer3_2_relu, layer3_2_relu_post_act_fake_quantizer, layer3_2_conv2, layer3_2_bn2, add_20, layer3_2_relu_1, layer3_2_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_1_relu_1_post_act_fake_quantizer):
    layer3_2_conv1 = getattr(self.layer3, "2").conv1(layer3_1_relu_1_post_act_fake_quantizer)
    layer3_2_bn1 = getattr(self.layer3, "2").bn1(layer3_2_conv1);  layer3_2_conv1 = None
    layer3_2_relu = getattr(self.layer3, "2").relu(layer3_2_bn1);  layer3_2_bn1 = None
    layer3_2_relu_post_act_fake_quantizer = self.layer3_2_relu_post_act_fake_quantizer(layer3_2_relu);  layer3_2_relu = None
    layer3_2_conv2 = getattr(self.layer3, "2").conv2(layer3_2_relu_post_act_fake_quantizer);  layer3_2_relu_post_act_fake_quantizer = None
    layer3_2_bn2 = getattr(self.layer3, "2").bn2(layer3_2_conv2);  layer3_2_conv2 = None
    add_20 = layer3_2_bn2 + layer3_1_relu_1_post_act_fake_quantizer;  layer3_2_bn2 = layer3_1_relu_1_post_act_fake_quantizer = None
    layer3_2_relu_1 = getattr(self.layer3, "2").relu_dup1(add_20);  add_20 = None
    layer3_2_relu_1_post_act_fake_quantizer = self.layer3_2_relu_1_post_act_fake_quantizer(layer3_2_relu_1);  layer3_2_relu_1 = None
    return layer3_2_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_2_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_3_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_2_relu_1_post_act_fake_quantizer, layer3_3_conv1, layer3_3_bn1, layer3_3_relu, layer3_3_relu_post_act_fake_quantizer, layer3_3_conv2, layer3_3_bn2, add_21, layer3_3_relu_1, layer3_3_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_2_relu_1_post_act_fake_quantizer):
    layer3_3_conv1 = getattr(self.layer3, "3").conv1(layer3_2_relu_1_post_act_fake_quantizer)
    layer3_3_bn1 = getattr(self.layer3, "3").bn1(layer3_3_conv1);  layer3_3_conv1 = None
    layer3_3_relu = getattr(self.layer3, "3").relu(layer3_3_bn1);  layer3_3_bn1 = None
    layer3_3_relu_post_act_fake_quantizer = self.layer3_3_relu_post_act_fake_quantizer(layer3_3_relu);  layer3_3_relu = None
    layer3_3_conv2 = getattr(self.layer3, "3").conv2(layer3_3_relu_post_act_fake_quantizer);  layer3_3_relu_post_act_fake_quantizer = None
    layer3_3_bn2 = getattr(self.layer3, "3").bn2(layer3_3_conv2);  layer3_3_conv2 = None
    add_21 = layer3_3_bn2 + layer3_2_relu_1_post_act_fake_quantizer;  layer3_3_bn2 = layer3_2_relu_1_post_act_fake_quantizer = None
    layer3_3_relu_1 = getattr(self.layer3, "3").relu_dup1(add_21);  add_21 = None
    layer3_3_relu_1_post_act_fake_quantizer = self.layer3_3_relu_1_post_act_fake_quantizer(layer3_3_relu_1);  layer3_3_relu_1 = None
    return layer3_3_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_3_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_4_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_3_relu_1_post_act_fake_quantizer, layer3_4_conv1, layer3_4_bn1, layer3_4_relu, layer3_4_relu_post_act_fake_quantizer, layer3_4_conv2, layer3_4_bn2, add_22, layer3_4_relu_1, layer3_4_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_3_relu_1_post_act_fake_quantizer):
    layer3_4_conv1 = getattr(self.layer3, "4").conv1(layer3_3_relu_1_post_act_fake_quantizer)
    layer3_4_bn1 = getattr(self.layer3, "4").bn1(layer3_4_conv1);  layer3_4_conv1 = None
    layer3_4_relu = getattr(self.layer3, "4").relu(layer3_4_bn1);  layer3_4_bn1 = None
    layer3_4_relu_post_act_fake_quantizer = self.layer3_4_relu_post_act_fake_quantizer(layer3_4_relu);  layer3_4_relu = None
    layer3_4_conv2 = getattr(self.layer3, "4").conv2(layer3_4_relu_post_act_fake_quantizer);  layer3_4_relu_post_act_fake_quantizer = None
    layer3_4_bn2 = getattr(self.layer3, "4").bn2(layer3_4_conv2);  layer3_4_conv2 = None
    add_22 = layer3_4_bn2 + layer3_3_relu_1_post_act_fake_quantizer;  layer3_4_bn2 = layer3_3_relu_1_post_act_fake_quantizer = None
    layer3_4_relu_1 = getattr(self.layer3, "4").relu_dup1(add_22);  add_22 = None
    layer3_4_relu_1_post_act_fake_quantizer = self.layer3_4_relu_1_post_act_fake_quantizer(layer3_4_relu_1);  layer3_4_relu_1 = None
    return layer3_4_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_4_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_5_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_4_relu_1_post_act_fake_quantizer, layer3_5_conv1, layer3_5_bn1, layer3_5_relu, layer3_5_relu_post_act_fake_quantizer, layer3_5_conv2, layer3_5_bn2, add_23, layer3_5_relu_1, layer3_5_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_4_relu_1_post_act_fake_quantizer):
    layer3_5_conv1 = getattr(self.layer3, "5").conv1(layer3_4_relu_1_post_act_fake_quantizer)
    layer3_5_bn1 = getattr(self.layer3, "5").bn1(layer3_5_conv1);  layer3_5_conv1 = None
    layer3_5_relu = getattr(self.layer3, "5").relu(layer3_5_bn1);  layer3_5_bn1 = None
    layer3_5_relu_post_act_fake_quantizer = self.layer3_5_relu_post_act_fake_quantizer(layer3_5_relu);  layer3_5_relu = None
    layer3_5_conv2 = getattr(self.layer3, "5").conv2(layer3_5_relu_post_act_fake_quantizer);  layer3_5_relu_post_act_fake_quantizer = None
    layer3_5_bn2 = getattr(self.layer3, "5").bn2(layer3_5_conv2);  layer3_5_conv2 = None
    add_23 = layer3_5_bn2 + layer3_4_relu_1_post_act_fake_quantizer;  layer3_5_bn2 = layer3_4_relu_1_post_act_fake_quantizer = None
    layer3_5_relu_1 = getattr(self.layer3, "5").relu_dup1(add_23);  add_23 = None
    layer3_5_relu_1_post_act_fake_quantizer = self.layer3_5_relu_1_post_act_fake_quantizer(layer3_5_relu_1);  layer3_5_relu_1 = None
    return layer3_5_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_5_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_6_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_5_relu_1_post_act_fake_quantizer, layer3_6_conv1, layer3_6_bn1, layer3_6_relu, layer3_6_relu_post_act_fake_quantizer, layer3_6_conv2, layer3_6_bn2, add_24, layer3_6_relu_1, layer3_6_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_5_relu_1_post_act_fake_quantizer):
    layer3_6_conv1 = getattr(self.layer3, "6").conv1(layer3_5_relu_1_post_act_fake_quantizer)
    layer3_6_bn1 = getattr(self.layer3, "6").bn1(layer3_6_conv1);  layer3_6_conv1 = None
    layer3_6_relu = getattr(self.layer3, "6").relu(layer3_6_bn1);  layer3_6_bn1 = None
    layer3_6_relu_post_act_fake_quantizer = self.layer3_6_relu_post_act_fake_quantizer(layer3_6_relu);  layer3_6_relu = None
    layer3_6_conv2 = getattr(self.layer3, "6").conv2(layer3_6_relu_post_act_fake_quantizer);  layer3_6_relu_post_act_fake_quantizer = None
    layer3_6_bn2 = getattr(self.layer3, "6").bn2(layer3_6_conv2);  layer3_6_conv2 = None
    add_24 = layer3_6_bn2 + layer3_5_relu_1_post_act_fake_quantizer;  layer3_6_bn2 = layer3_5_relu_1_post_act_fake_quantizer = None
    layer3_6_relu_1 = getattr(self.layer3, "6").relu_dup1(add_24);  add_24 = None
    layer3_6_relu_1_post_act_fake_quantizer = self.layer3_6_relu_1_post_act_fake_quantizer(layer3_6_relu_1);  layer3_6_relu_1 = None
    return layer3_6_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_6_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_7_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_6_relu_1_post_act_fake_quantizer, layer3_7_conv1, layer3_7_bn1, layer3_7_relu, layer3_7_relu_post_act_fake_quantizer, layer3_7_conv2, layer3_7_bn2, add_25, layer3_7_relu_1, layer3_7_relu_1_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_6_relu_1_post_act_fake_quantizer):
    layer3_7_conv1 = getattr(self.layer3, "7").conv1(layer3_6_relu_1_post_act_fake_quantizer)
    layer3_7_bn1 = getattr(self.layer3, "7").bn1(layer3_7_conv1);  layer3_7_conv1 = None
    layer3_7_relu = getattr(self.layer3, "7").relu(layer3_7_bn1);  layer3_7_bn1 = None
    layer3_7_relu_post_act_fake_quantizer = self.layer3_7_relu_post_act_fake_quantizer(layer3_7_relu);  layer3_7_relu = None
    layer3_7_conv2 = getattr(self.layer3, "7").conv2(layer3_7_relu_post_act_fake_quantizer);  layer3_7_relu_post_act_fake_quantizer = None
    layer3_7_bn2 = getattr(self.layer3, "7").bn2(layer3_7_conv2);  layer3_7_conv2 = None
    add_25 = layer3_7_bn2 + layer3_6_relu_1_post_act_fake_quantizer;  layer3_7_bn2 = layer3_6_relu_1_post_act_fake_quantizer = None
    layer3_7_relu_1 = getattr(self.layer3, "7").relu_dup1(add_25);  add_25 = None
    layer3_7_relu_1_post_act_fake_quantizer = self.layer3_7_relu_1_post_act_fake_quantizer(layer3_7_relu_1);  layer3_7_relu_1 = None
    return layer3_7_relu_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_7_relu_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for layer3_8_conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_7_relu_1_post_act_fake_quantizer, layer3_8_conv1, layer3_8_bn1, layer3_8_relu, layer3_8_relu_post_act_fake_quantizer, layer3_8_conv2, layer3_8_bn2, add_26, layer3_8_relu_1, avgpool, size, view, view_post_act_fake_quantizer]
[MQBENCH] INFO: 


def forward(self, layer3_7_relu_1_post_act_fake_quantizer):
    layer3_8_conv1 = getattr(self.layer3, "8").conv1(layer3_7_relu_1_post_act_fake_quantizer)
    layer3_8_bn1 = getattr(self.layer3, "8").bn1(layer3_8_conv1);  layer3_8_conv1 = None
    layer3_8_relu = getattr(self.layer3, "8").relu(layer3_8_bn1);  layer3_8_bn1 = None
    layer3_8_relu_post_act_fake_quantizer = self.layer3_8_relu_post_act_fake_quantizer(layer3_8_relu);  layer3_8_relu = None
    layer3_8_conv2 = getattr(self.layer3, "8").conv2(layer3_8_relu_post_act_fake_quantizer);  layer3_8_relu_post_act_fake_quantizer = None
    layer3_8_bn2 = getattr(self.layer3, "8").bn2(layer3_8_conv2);  layer3_8_conv2 = None
    add_26 = layer3_8_bn2 + layer3_7_relu_1_post_act_fake_quantizer;  layer3_8_bn2 = layer3_7_relu_1_post_act_fake_quantizer = None
    layer3_8_relu_1 = getattr(self.layer3, "8").relu_dup1(add_26);  add_26 = None
    avgpool = self.avgpool(layer3_8_relu_1);  layer3_8_relu_1 = None
    size = avgpool.size(0)
    view = avgpool.view(size, -1);  avgpool = size = None
    view_post_act_fake_quantizer = self.view_post_act_fake_quantizer(view);  view = None
    return view_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for view_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: prepare block reconstruction for fc
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [view_post_act_fake_quantizer, fc]
[MQBENCH] INFO: 


def forward(self, view_post_act_fake_quantizer):
    fc = self.fc(view_post_act_fake_quantizer);  view_post_act_fake_quantizer = None
    return fc
    
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node x_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_0_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.0.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_0_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_1_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.1.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_1_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_2_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.2.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_2_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_3_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.3.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_3_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_4_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.4.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_4_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_5_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.5.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_5_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_6_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.6.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_6_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_7_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.7.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_7_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_8_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1.8.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer1_8_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_0_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.downsample.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.downsample.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.0.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_0_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_1_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.1.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_1_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_2_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.2.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_2_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_3_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.3.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_3_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_4_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.4.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_4_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_5_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.5.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_5_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_6_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.6.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_6_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_7_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.7.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_7_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_8_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2.8.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer2_8_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_0_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.downsample.0 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.downsample.1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.0.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_0_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_1_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.1.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_1_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_2_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.2.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_2_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_3_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.3.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_3_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_4_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.4.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_4_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_5_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.5.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_5_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_6_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.6.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_6_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_7_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.7.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_7_relu_1_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.conv1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.bn1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.relu in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3_8_relu_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.conv2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.bn2 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node layer3.8.relu_dup1 in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node avgpool in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node view_post_act_fake_quantizer in quant
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: set the node fc in quant
QDROP model already saved, now loading QDROP_8bits_cifar100_resnet56.pt
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
evaluate mqb quantized model
{'mean_acc': 0.3691, 'qtl_acc': 0.3691, 'mean_loss': 2.938320410402515, 'qtl_loss': 2.938320410402515, 'test time': 4.47088623046875, 'acc_list': array([0.3691]), 'loss_list': array([2.93832041])}
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
evaluate mqb quantized model
{'mean_acc': 0.6835, 'qtl_acc': 0.6835, 'mean_loss': 1.4832341354104537, 'qtl_loss': 1.4832341354104537, 'test time': 4.449094295501709, 'acc_list': array([0.6835]), 'loss_list': array([1.48323414])}
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
evaluate mqb quantized model
{'mean_acc': 0.7235, 'qtl_acc': 0.7235, 'mean_loss': 1.2972044356261627, 'qtl_loss': 1.2972044356261627, 'test time': 4.451396942138672, 'acc_list': array([0.7235]), 'loss_list': array([1.29720444])}
[MQBENCH] INFO: Disable observer and Disable quantize.
input of  conv1  is  x_post_act_fake_quantizer
input of  layer1.0.conv1  is  relu_post_act_fake_quantizer
input of  layer1.0.conv2  is  layer1_0_relu_post_act_fake_quantizer
input of  layer1.1.conv1  is  layer1_0_relu_1_post_act_fake_quantizer
input of  layer1.1.conv2  is  layer1_1_relu_post_act_fake_quantizer
input of  layer1.2.conv1  is  layer1_1_relu_1_post_act_fake_quantizer
input of  layer1.2.conv2  is  layer1_2_relu_post_act_fake_quantizer
input of  layer1.3.conv1  is  layer1_2_relu_1_post_act_fake_quantizer
input of  layer1.3.conv2  is  layer1_3_relu_post_act_fake_quantizer
input of  layer1.4.conv1  is  layer1_3_relu_1_post_act_fake_quantizer
input of  layer1.4.conv2  is  layer1_4_relu_post_act_fake_quantizer
input of  layer1.5.conv1  is  layer1_4_relu_1_post_act_fake_quantizer
input of  layer1.5.conv2  is  layer1_5_relu_post_act_fake_quantizer
input of  layer1.6.conv1  is  layer1_5_relu_1_post_act_fake_quantizer
input of  layer1.6.conv2  is  layer1_6_relu_post_act_fake_quantizer
input of  layer1.7.conv1  is  layer1_6_relu_1_post_act_fake_quantizer
input of  layer1.7.conv2  is  layer1_7_relu_post_act_fake_quantizer
input of  layer1.8.conv1  is  layer1_7_relu_1_post_act_fake_quantizer
input of  layer1.8.conv2  is  layer1_8_relu_post_act_fake_quantizer
input of  layer2.0.conv1  is  layer1_8_relu_1_post_act_fake_quantizer
input of  layer2.0.conv2  is  layer2_0_relu_post_act_fake_quantizer
input of  layer2.0.downsample.0  is  layer1_8_relu_1_post_act_fake_quantizer
input of  layer2.1.conv1  is  layer2_0_relu_1_post_act_fake_quantizer
input of  layer2.1.conv2  is  layer2_1_relu_post_act_fake_quantizer
input of  layer2.2.conv1  is  layer2_1_relu_1_post_act_fake_quantizer
input of  layer2.2.conv2  is  layer2_2_relu_post_act_fake_quantizer
input of  layer2.3.conv1  is  layer2_2_relu_1_post_act_fake_quantizer
input of  layer2.3.conv2  is  layer2_3_relu_post_act_fake_quantizer
input of  layer2.4.conv1  is  layer2_3_relu_1_post_act_fake_quantizer
input of  layer2.4.conv2  is  layer2_4_relu_post_act_fake_quantizer
input of  layer2.5.conv1  is  layer2_4_relu_1_post_act_fake_quantizer
input of  layer2.5.conv2  is  layer2_5_relu_post_act_fake_quantizer
input of  layer2.6.conv1  is  layer2_5_relu_1_post_act_fake_quantizer
input of  layer2.6.conv2  is  layer2_6_relu_post_act_fake_quantizer
input of  layer2.7.conv1  is  layer2_6_relu_1_post_act_fake_quantizer
input of  layer2.7.conv2  is  layer2_7_relu_post_act_fake_quantizer
input of  layer2.8.conv1  is  layer2_7_relu_1_post_act_fake_quantizer
input of  layer2.8.conv2  is  layer2_8_relu_post_act_fake_quantizer
input of  layer3.0.conv1  is  layer2_8_relu_1_post_act_fake_quantizer
input of  layer3.0.conv2  is  layer3_0_relu_post_act_fake_quantizer
input of  layer3.0.downsample.0  is  layer2_8_relu_1_post_act_fake_quantizer
input of  layer3.1.conv1  is  layer3_0_relu_1_post_act_fake_quantizer
input of  layer3.1.conv2  is  layer3_1_relu_post_act_fake_quantizer
input of  layer3.2.conv1  is  layer3_1_relu_1_post_act_fake_quantizer
input of  layer3.2.conv2  is  layer3_2_relu_post_act_fake_quantizer
input of  layer3.3.conv1  is  layer3_2_relu_1_post_act_fake_quantizer
input of  layer3.3.conv2  is  layer3_3_relu_post_act_fake_quantizer
input of  layer3.4.conv1  is  layer3_3_relu_1_post_act_fake_quantizer
input of  layer3.4.conv2  is  layer3_4_relu_post_act_fake_quantizer
input of  layer3.5.conv1  is  layer3_4_relu_1_post_act_fake_quantizer
input of  layer3.5.conv2  is  layer3_5_relu_post_act_fake_quantizer
input of  layer3.6.conv1  is  layer3_5_relu_1_post_act_fake_quantizer
input of  layer3.6.conv2  is  layer3_6_relu_post_act_fake_quantizer
input of  layer3.7.conv1  is  layer3_6_relu_1_post_act_fake_quantizer
input of  layer3.7.conv2  is  layer3_7_relu_post_act_fake_quantizer
input of  layer3.8.conv1  is  layer3_7_relu_1_post_act_fake_quantizer
input of  layer3.8.conv2  is  layer3_8_relu_post_act_fake_quantizer
input of  fc  is  view_post_act_fake_quantizer
----------Gradient Caching----------
perturb layer layer1.0.conv1 to A2W2 p=2.0677347034215927
perturb layer layer1.0.conv1 to A2W4 p=1.2532820105552673
perturb layer layer1.0.conv1 to A2W8 p=1.2261190861463547
perturb layer layer1.0.conv1 to A4W2 p=0.9557577967643738
perturb layer layer1.0.conv1 to A4W4 p=0.1982286050915718
perturb layer layer1.0.conv1 to A4W8 p=0.21228326112031937
perturb layer layer1.0.conv1 to A8W2 p=0.7373148500919342
perturb layer layer1.0.conv1 to A8W4 p=-0.01243128627538681
perturb layer layer1.0.conv1 to A8W8 p=0.0032554790377616882
perturb layer layer1.0.conv1 to A2W2 and layer layer1.0.conv2 to A2W2 p=2.721445605158806
perturb layer layer1.0.conv1 to A2W2 and layer layer1.0.conv2 to A2W4 p=2.2810195833444595
perturb layer layer1.0.conv1 to A2W2 and layer layer1.0.conv2 to A2W8 p=2.381844088435173
perturb layer layer1.0.conv1 to A2W2 and layer layer1.0.conv2 to A4W2 p=2.4829693883657455
perturb layer layer1.0.conv1 to A2W2 and layer layer1.0.conv2 to A4W4 p=2.018380895256996
perturb layer layer1.0.conv1 to A2W2 and layer layer1.0.conv2 to A4W8 p=2.106217637658119
perturb layer layer1.0.conv1 to A2W2 and layer layer1.0.conv2 to A8W2 p=2.4513194113969803
perturb layer layer1.0.conv1 to A2W2 and layer layer1.0.conv2 to A8W4 p=1.9813603609800339
perturb layer layer1.0.conv1 to A2W2 and layer layer1.0.conv2 to A8W8 p=2.0661797672510147
perturb layer layer1.0.conv1 to A2W4 and layer layer1.0.conv2 to A2W2 p=1.867751196026802
perturb layer layer1.0.conv1 to A2W4 and layer layer1.0.conv2 to A2W4 p=1.5295309871435165
perturb layer layer1.0.conv1 to A2W4 and layer layer1.0.conv2 to A2W8 p=1.5955729335546494
perturb layer layer1.0.conv1 to A2W4 and layer layer1.0.conv2 to A4W2 p=1.6538758426904678
perturb layer layer1.0.conv1 to A2W4 and layer layer1.0.conv2 to A4W4 p=1.2519487142562866
perturb layer layer1.0.conv1 to A2W4 and layer layer1.0.conv2 to A4W8 p=1.2985300719738007
perturb layer layer1.0.conv1 to A2W4 and layer layer1.0.conv2 to A8W2 p=1.6091389507055283
perturb layer layer1.0.conv1 to A2W4 and layer layer1.0.conv2 to A8W4 p=1.2087247967720032
perturb layer layer1.0.conv1 to A2W4 and layer layer1.0.conv2 to A8W8 p=1.252239614725113
perturb layer layer1.0.conv1 to A2W8 and layer layer1.0.conv2 to A2W2 p=1.8157830387353897
perturb layer layer1.0.conv1 to A2W8 and layer layer1.0.conv2 to A2W4 p=1.4186512380838394
perturb layer layer1.0.conv1 to A2W8 and layer layer1.0.conv2 to A2W8 p=1.4743263274431229
perturb layer layer1.0.conv1 to A2W8 and layer layer1.0.conv2 to A4W2 p=1.5935325175523758
perturb layer layer1.0.conv1 to A2W8 and layer layer1.0.conv2 to A4W4 p=1.2188170552253723
perturb layer layer1.0.conv1 to A2W8 and layer layer1.0.conv2 to A4W8 p=1.2610695362091064
perturb layer layer1.0.conv1 to A2W8 and layer layer1.0.conv2 to A8W2 p=1.5784301310777664
perturb layer layer1.0.conv1 to A2W8 and layer layer1.0.conv2 to A8W4 p=1.184416115283966
perturb layer layer1.0.conv1 to A2W8 and layer layer1.0.conv2 to A8W8 p=1.225359782576561
perturb layer layer1.0.conv1 to A4W2 and layer layer1.0.conv2 to A2W2 p=1.4862065613269806
perturb layer layer1.0.conv1 to A4W2 and layer layer1.0.conv2 to A2W4 p=1.1313835233449936
perturb layer layer1.0.conv1 to A4W2 and layer layer1.0.conv2 to A2W8 p=1.188444584608078
perturb layer layer1.0.conv1 to A4W2 and layer layer1.0.conv2 to A4W2 p=1.334693267941475
perturb layer layer1.0.conv1 to A4W2 and layer layer1.0.conv2 to A4W4 p=0.9509547501802444
perturb layer layer1.0.conv1 to A4W2 and layer layer1.0.conv2 to A4W8 p=0.9996594041585922
perturb layer layer1.0.conv1 to A4W2 and layer layer1.0.conv2 to A8W2 p=1.3092796057462692
perturb layer layer1.0.conv1 to A4W2 and layer layer1.0.conv2 to A8W4 p=0.9144827872514725
perturb layer layer1.0.conv1 to A4W2 and layer layer1.0.conv2 to A8W8 p=0.9570579528808594
perturb layer layer1.0.conv1 to A4W4 and layer layer1.0.conv2 to A2W2 p=0.634871706366539
perturb layer layer1.0.conv1 to A4W4 and layer layer1.0.conv2 to A2W4 p=0.3782107010483742
perturb layer layer1.0.conv1 to A4W4 and layer layer1.0.conv2 to A2W8 p=0.4262266010046005
perturb layer layer1.0.conv1 to A4W4 and layer layer1.0.conv2 to A4W2 p=0.45990078151226044
perturb layer layer1.0.conv1 to A4W4 and layer layer1.0.conv2 to A4W4 p=0.19791001826524734
perturb layer layer1.0.conv1 to A4W4 and layer layer1.0.conv2 to A4W8 p=0.23133552074432373
perturb layer layer1.0.conv1 to A4W4 and layer layer1.0.conv2 to A8W2 p=0.4365658164024353
perturb layer layer1.0.conv1 to A4W4 and layer layer1.0.conv2 to A8W4 p=0.164888434112072
perturb layer layer1.0.conv1 to A4W4 and layer layer1.0.conv2 to A8W8 p=0.19825711101293564
perturb layer layer1.0.conv1 to A4W8 and layer layer1.0.conv2 to A2W2 p=0.5815133601427078
perturb layer layer1.0.conv1 to A4W8 and layer layer1.0.conv2 to A2W4 p=0.3156706318259239
perturb layer layer1.0.conv1 to A4W8 and layer layer1.0.conv2 to A2W8 p=0.3666146993637085
perturb layer layer1.0.conv1 to A4W8 and layer layer1.0.conv2 to A4W2 p=0.46090489625930786
perturb layer layer1.0.conv1 to A4W8 and layer layer1.0.conv2 to A4W4 p=0.19486360251903534
perturb layer layer1.0.conv1 to A4W8 and layer layer1.0.conv2 to A4W8 p=0.23812799155712128
perturb layer layer1.0.conv1 to A4W8 and layer layer1.0.conv2 to A8W2 p=0.43143709003925323
perturb layer layer1.0.conv1 to A4W8 and layer layer1.0.conv2 to A8W4 p=0.17460346966981888
perturb layer layer1.0.conv1 to A4W8 and layer layer1.0.conv2 to A8W8 p=0.21354493498802185
perturb layer layer1.0.conv1 to A8W2 and layer layer1.0.conv2 to A2W2 p=1.2699463814496994
perturb layer layer1.0.conv1 to A8W2 and layer layer1.0.conv2 to A2W4 p=0.9299195855855942
perturb layer layer1.0.conv1 to A8W2 and layer layer1.0.conv2 to A2W8 p=0.9786949753761292
perturb layer layer1.0.conv1 to A8W2 and layer layer1.0.conv2 to A4W2 p=1.1202739030122757
perturb layer layer1.0.conv1 to A8W2 and layer layer1.0.conv2 to A4W4 p=0.7427849322557449
perturb layer layer1.0.conv1 to A8W2 and layer layer1.0.conv2 to A4W8 p=0.7739996016025543
perturb layer layer1.0.conv1 to A8W2 and layer layer1.0.conv2 to A8W2 p=1.1009808778762817
perturb layer layer1.0.conv1 to A8W2 and layer layer1.0.conv2 to A8W4 p=0.7081582695245743
perturb layer layer1.0.conv1 to A8W2 and layer layer1.0.conv2 to A8W8 p=0.7379848510026932
perturb layer layer1.0.conv1 to A8W4 and layer layer1.0.conv2 to A2W2 p=0.4161796420812607
perturb layer layer1.0.conv1 to A8W4 and layer layer1.0.conv2 to A2W4 p=0.16651754826307297
perturb layer layer1.0.conv1 to A8W4 and layer layer1.0.conv2 to A2W8 p=0.20535019040107727
perturb layer layer1.0.conv1 to A8W4 and layer layer1.0.conv2 to A4W2 p=0.27323783189058304
perturb layer layer1.0.conv1 to A8W4 and layer layer1.0.conv2 to A4W4 p=-0.007792674005031586
perturb layer layer1.0.conv1 to A8W4 and layer layer1.0.conv2 to A4W8 p=0.014808423817157745
perturb layer layer1.0.conv1 to A8W4 and layer layer1.0.conv2 to A8W2 p=0.2543790861964226
perturb layer layer1.0.conv1 to A8W4 and layer layer1.0.conv2 to A8W4 p=-0.031188659369945526
perturb layer layer1.0.conv1 to A8W4 and layer layer1.0.conv2 to A8W8 p=-0.010512985289096832
perturb layer layer1.0.conv1 to A8W8 and layer layer1.0.conv2 to A2W2 p=0.3811802566051483
perturb layer layer1.0.conv1 to A8W8 and layer layer1.0.conv2 to A2W4 p=0.11047207564115524
perturb layer layer1.0.conv1 to A8W8 and layer layer1.0.conv2 to A2W8 p=0.15084685385227203
perturb layer layer1.0.conv1 to A8W8 and layer layer1.0.conv2 to A4W2 p=0.26146068423986435
perturb layer layer1.0.conv1 to A8W8 and layer layer1.0.conv2 to A4W4 p=-0.007899649441242218
perturb layer layer1.0.conv1 to A8W8 and layer layer1.0.conv2 to A4W8 p=0.030866719782352448
perturb layer layer1.0.conv1 to A8W8 and layer layer1.0.conv2 to A8W2 p=0.24386759847402573
perturb layer layer1.0.conv1 to A8W8 and layer layer1.0.conv2 to A8W4 p=-0.031490206718444824
perturb layer layer1.0.conv1 to A8W8 and layer layer1.0.conv2 to A8W8 p=0.0032774582505226135
perturb layer layer1.0.conv1 to A2W2 and layer layer1.1.conv1 to A2W2 p=3.2580629140138626
perturb layer layer1.0.conv1 to A2W2 and layer layer1.1.conv1 to A2W4 p=2.9713262766599655
perturb layer layer1.0.conv1 to A2W2 and layer layer1.1.conv1 to A2W8 p=2.93086139857769
perturb layer layer1.0.conv1 to A2W2 and layer layer1.1.conv1 to A4W2 p=2.482013627886772
perturb layer layer1.0.conv1 to A2W2 and layer layer1.1.conv1 to A4W4 p=2.2322850674390793
perturb layer layer1.0.conv1 to A2W2 and layer layer1.1.conv1 to A4W8 p=2.1977888494729996
perturb layer layer1.0.conv1 to A2W2 and layer layer1.1.conv1 to A8W2 p=2.369396075606346
perturb layer layer1.0.conv1 to A2W2 and layer layer1.1.conv1 to A8W4 p=2.098975583910942
perturb layer layer1.0.conv1 to A2W2 and layer layer1.1.conv1 to A8W8 p=2.0673050433397293
perturb layer layer1.0.conv1 to A2W4 and layer layer1.1.conv1 to A2W2 p=2.441559687256813
perturb layer layer1.0.conv1 to A2W4 and layer layer1.1.conv1 to A2W4 p=2.1682306081056595
perturb layer layer1.0.conv1 to A2W4 and layer layer1.1.conv1 to A2W8 p=2.149129644036293
perturb layer layer1.0.conv1 to A2W4 and layer layer1.1.conv1 to A4W2 p=1.5702540129423141
perturb layer layer1.0.conv1 to A2W4 and layer layer1.1.conv1 to A4W4 p=1.3381660133600235
perturb layer layer1.0.conv1 to A2W4 and layer layer1.1.conv1 to A4W8 p=1.3352418690919876
perturb layer layer1.0.conv1 to A2W4 and layer layer1.1.conv1 to A8W2 p=1.5017618983983994
perturb layer layer1.0.conv1 to A2W4 and layer layer1.1.conv1 to A8W4 p=1.2518054693937302
perturb layer layer1.0.conv1 to A2W4 and layer layer1.1.conv1 to A8W8 p=1.2512795180082321
perturb layer layer1.0.conv1 to A2W8 and layer layer1.1.conv1 to A2W2 p=2.3552112728357315
perturb layer layer1.0.conv1 to A2W8 and layer layer1.1.conv1 to A2W4 p=2.0958122462034225
perturb layer layer1.0.conv1 to A2W8 and layer layer1.1.conv1 to A2W8 p=2.079484209418297
perturb layer layer1.0.conv1 to A2W8 and layer layer1.1.conv1 to A4W2 p=1.535640463232994
perturb layer layer1.0.conv1 to A2W8 and layer layer1.1.conv1 to A4W4 p=1.3217171877622604
perturb layer layer1.0.conv1 to A2W8 and layer layer1.1.conv1 to A4W8 p=1.3179608434438705
perturb layer layer1.0.conv1 to A2W8 and layer layer1.1.conv1 to A8W2 p=1.4742775410413742
perturb layer layer1.0.conv1 to A2W8 and layer layer1.1.conv1 to A8W4 p=1.23671056330204
perturb layer layer1.0.conv1 to A2W8 and layer layer1.1.conv1 to A8W8 p=1.2269766479730606
perturb layer layer1.0.conv1 to A4W2 and layer layer1.1.conv1 to A2W2 p=2.1396607011556625
perturb layer layer1.0.conv1 to A4W2 and layer layer1.1.conv1 to A2W4 p=1.8968911319971085
perturb layer layer1.0.conv1 to A4W2 and layer layer1.1.conv1 to A2W8 p=1.860560491681099
perturb layer layer1.0.conv1 to A4W2 and layer layer1.1.conv1 to A4W2 p=1.2967277020215988
perturb layer layer1.0.conv1 to A4W2 and layer layer1.1.conv1 to A4W4 p=1.0737093687057495
perturb layer layer1.0.conv1 to A4W2 and layer layer1.1.conv1 to A4W8 p=1.0432290583848953
perturb layer layer1.0.conv1 to A4W2 and layer layer1.1.conv1 to A8W2 p=1.1916779577732086
perturb layer layer1.0.conv1 to A4W2 and layer layer1.1.conv1 to A8W4 p=0.9956789761781693
perturb layer layer1.0.conv1 to A4W2 and layer layer1.1.conv1 to A8W8 p=0.956738755106926
perturb layer layer1.0.conv1 to A4W4 and layer layer1.1.conv1 to A2W2 p=1.2450615465641022
perturb layer layer1.0.conv1 to A4W4 and layer layer1.1.conv1 to A2W4 p=1.0968296527862549
perturb layer layer1.0.conv1 to A4W4 and layer layer1.1.conv1 to A2W8 p=1.097360074520111
perturb layer layer1.0.conv1 to A4W4 and layer layer1.1.conv1 to A4W2 p=0.42919696867465973
perturb layer layer1.0.conv1 to A4W4 and layer layer1.1.conv1 to A4W4 p=0.2669845223426819
perturb layer layer1.0.conv1 to A4W4 and layer layer1.1.conv1 to A4W8 p=0.2640974894165993
perturb layer layer1.0.conv1 to A4W4 and layer layer1.1.conv1 to A8W2 p=0.3752347230911255
perturb layer layer1.0.conv1 to A4W4 and layer layer1.1.conv1 to A8W4 p=0.20293468981981277
perturb layer layer1.0.conv1 to A4W4 and layer layer1.1.conv1 to A8W8 p=0.19951943308115005
perturb layer layer1.0.conv1 to A4W8 and layer layer1.1.conv1 to A2W2 p=1.1794988214969635
perturb layer layer1.0.conv1 to A4W8 and layer layer1.1.conv1 to A2W4 p=1.0078157633543015
perturb layer layer1.0.conv1 to A4W8 and layer layer1.1.conv1 to A2W8 p=1.00232794880867
perturb layer layer1.0.conv1 to A4W8 and layer layer1.1.conv1 to A4W2 p=0.4584091752767563
perturb layer layer1.0.conv1 to A4W8 and layer layer1.1.conv1 to A4W4 p=0.28795718401670456
perturb layer layer1.0.conv1 to A4W8 and layer layer1.1.conv1 to A4W8 p=0.27575620263814926
perturb layer layer1.0.conv1 to A4W8 and layer layer1.1.conv1 to A8W2 p=0.4065573439002037
perturb layer layer1.0.conv1 to A4W8 and layer layer1.1.conv1 to A8W4 p=0.22184080630540848
perturb layer layer1.0.conv1 to A4W8 and layer layer1.1.conv1 to A8W8 p=0.21233484148979187
perturb layer layer1.0.conv1 to A8W2 and layer layer1.1.conv1 to A2W2 p=1.995369330048561
perturb layer layer1.0.conv1 to A8W2 and layer layer1.1.conv1 to A2W4 p=1.749739721417427
perturb layer layer1.0.conv1 to A8W2 and layer layer1.1.conv1 to A2W8 p=1.69823656976223
perturb layer layer1.0.conv1 to A8W2 and layer layer1.1.conv1 to A4W2 p=1.1468327045440674
perturb layer layer1.0.conv1 to A8W2 and layer layer1.1.conv1 to A4W4 p=0.9286542385816574
perturb layer layer1.0.conv1 to A8W2 and layer layer1.1.conv1 to A4W8 p=0.8795585185289383
perturb layer layer1.0.conv1 to A8W2 and layer layer1.1.conv1 to A8W2 p=1.0305660665035248
perturb layer layer1.0.conv1 to A8W2 and layer layer1.1.conv1 to A8W4 p=0.8023288249969482
perturb layer layer1.0.conv1 to A8W2 and layer layer1.1.conv1 to A8W8 p=0.7375531941652298
perturb layer layer1.0.conv1 to A8W4 and layer layer1.1.conv1 to A2W2 p=0.9908875226974487
perturb layer layer1.0.conv1 to A8W4 and layer layer1.1.conv1 to A2W4 p=0.7822272032499313
perturb layer layer1.0.conv1 to A8W4 and layer layer1.1.conv1 to A2W8 p=0.7815313637256622
perturb layer layer1.0.conv1 to A8W4 and layer layer1.1.conv1 to A4W2 p=0.28217746317386627
perturb layer layer1.0.conv1 to A8W4 and layer layer1.1.conv1 to A4W4 p=0.07336390763521194
perturb layer layer1.0.conv1 to A8W4 and layer layer1.1.conv1 to A4W8 p=0.06404979526996613
perturb layer layer1.0.conv1 to A8W4 and layer layer1.1.conv1 to A8W2 p=0.2232820987701416
perturb layer layer1.0.conv1 to A8W4 and layer layer1.1.conv1 to A8W4 p=0.005163416266441345
perturb layer layer1.0.conv1 to A8W4 and layer layer1.1.conv1 to A8W8 p=-0.012397445738315582
perturb layer layer1.0.conv1 to A8W8 and layer layer1.1.conv1 to A2W2 p=0.8984412997961044
perturb layer layer1.0.conv1 to A8W8 and layer layer1.1.conv1 to A2W4 p=0.7049484997987747
perturb layer layer1.0.conv1 to A8W8 and layer layer1.1.conv1 to A2W8 p=0.6957526206970215
perturb layer layer1.0.conv1 to A8W8 and layer layer1.1.conv1 to A4W2 p=0.3186589404940605
perturb layer layer1.0.conv1 to A8W8 and layer layer1.1.conv1 to A4W4 p=0.0975496843457222
perturb layer layer1.0.conv1 to A8W8 and layer layer1.1.conv1 to A4W8 p=0.08362967520952225
perturb layer layer1.0.conv1 to A8W8 and layer layer1.1.conv1 to A8W2 p=0.2638104557991028
perturb layer layer1.0.conv1 to A8W8 and layer layer1.1.conv1 to A8W4 p=0.02155163139104843
perturb layer layer1.0.conv1 to A8W8 and layer layer1.1.conv1 to A8W8 p=0.0032383501529693604
perturb layer layer1.0.conv1 to A2W2 and layer layer1.1.conv2 to A2W2 p=2.511740818619728
perturb layer layer1.0.conv1 to A2W2 and layer layer1.1.conv2 to A2W4 p=2.1644838601350784
perturb layer layer1.0.conv1 to A2W2 and layer layer1.1.conv2 to A2W8 p=2.2136319130659103
perturb layer layer1.0.conv1 to A2W2 and layer layer1.1.conv2 to A4W2 p=2.435838744044304
perturb layer layer1.0.conv1 to A2W2 and layer layer1.1.conv2 to A4W4 p=2.03530852496624
perturb layer layer1.0.conv1 to A2W2 and layer layer1.1.conv2 to A4W8 p=2.1014085561037064
perturb layer layer1.0.conv1 to A2W2 and layer layer1.1.conv2 to A8W2 p=2.4141208678483963
perturb layer layer1.0.conv1 to A2W2 and layer layer1.1.conv2 to A8W4 p=2.012070342898369
perturb layer layer1.0.conv1 to A2W2 and layer layer1.1.conv2 to A8W8 p=2.0680639296770096
perturb layer layer1.0.conv1 to A2W4 and layer layer1.1.conv2 to A2W2 p=1.5119715481996536
perturb layer layer1.0.conv1 to A2W4 and layer layer1.1.conv2 to A2W4 p=1.3015986382961273
perturb layer layer1.0.conv1 to A2W4 and layer layer1.1.conv2 to A2W8 p=1.3666029125452042
perturb layer layer1.0.conv1 to A2W4 and layer layer1.1.conv2 to A4W2 p=1.3987201303243637
perturb layer layer1.0.conv1 to A2W4 and layer layer1.1.conv2 to A4W4 p=1.2084436863660812
perturb layer layer1.0.conv1 to A2W4 and layer layer1.1.conv2 to A4W8 p=1.2767550945281982
perturb layer layer1.0.conv1 to A2W4 and layer layer1.1.conv2 to A8W2 p=1.378731057047844
perturb layer layer1.0.conv1 to A2W4 and layer layer1.1.conv2 to A8W4 p=1.1868613362312317
perturb layer layer1.0.conv1 to A2W4 and layer layer1.1.conv2 to A8W8 p=1.2534997016191483
perturb layer layer1.0.conv1 to A2W8 and layer layer1.1.conv2 to A2W2 p=1.5166474133729935
perturb layer layer1.0.conv1 to A2W8 and layer layer1.1.conv2 to A2W4 p=1.3166090548038483
perturb layer layer1.0.conv1 to A2W8 and layer layer1.1.conv2 to A2W8 p=1.370222568511963
perturb layer layer1.0.conv1 to A2W8 and layer layer1.1.conv2 to A4W2 p=1.382547751069069
perturb layer layer1.0.conv1 to A2W8 and layer layer1.1.conv2 to A4W4 p=1.1928371787071228
perturb layer layer1.0.conv1 to A2W8 and layer layer1.1.conv2 to A4W8 p=1.2425609230995178
perturb layer layer1.0.conv1 to A2W8 and layer layer1.1.conv2 to A8W2 p=1.367098405957222
perturb layer layer1.0.conv1 to A2W8 and layer layer1.1.conv2 to A8W4 p=1.1780330687761307
perturb layer layer1.0.conv1 to A2W8 and layer layer1.1.conv2 to A8W8 p=1.2257244735956192
perturb layer layer1.0.conv1 to A4W2 and layer layer1.1.conv2 to A2W2 p=1.2847742140293121
perturb layer layer1.0.conv1 to A4W2 and layer layer1.1.conv2 to A2W4 p=1.0370439141988754
perturb layer layer1.0.conv1 to A4W2 and layer layer1.1.conv2 to A2W8 p=1.026887908577919
perturb layer layer1.0.conv1 to A4W2 and layer layer1.1.conv2 to A4W2 p=1.262459173798561
perturb layer layer1.0.conv1 to A4W2 and layer layer1.1.conv2 to A4W4 p=0.992936760187149
perturb layer layer1.0.conv1 to A4W2 and layer layer1.1.conv2 to A4W8 p=0.9949721246957779
perturb layer layer1.0.conv1 to A4W2 and layer layer1.1.conv2 to A8W2 p=1.239995926618576
perturb layer layer1.0.conv1 to A4W2 and layer layer1.1.conv2 to A8W4 p=0.9602867513895035
perturb layer layer1.0.conv1 to A4W2 and layer layer1.1.conv2 to A8W8 p=0.9567676931619644
perturb layer layer1.0.conv1 to A4W4 and layer layer1.1.conv2 to A2W2 p=0.3954940438270569
perturb layer layer1.0.conv1 to A4W4 and layer layer1.1.conv2 to A2W4 p=0.2426936998963356
perturb layer layer1.0.conv1 to A4W4 and layer layer1.1.conv2 to A2W8 p=0.27643198519945145
perturb layer layer1.0.conv1 to A4W4 and layer layer1.1.conv2 to A4W2 p=0.30929458141326904
perturb layer layer1.0.conv1 to A4W4 and layer layer1.1.conv2 to A4W4 p=0.1743166297674179
perturb layer layer1.0.conv1 to A4W4 and layer layer1.1.conv2 to A4W8 p=0.21149307489395142
perturb layer layer1.0.conv1 to A4W4 and layer layer1.1.conv2 to A8W2 p=0.2983897849917412
perturb layer layer1.0.conv1 to A4W4 and layer layer1.1.conv2 to A8W4 p=0.1634758785367012
perturb layer layer1.0.conv1 to A4W4 and layer layer1.1.conv2 to A8W8 p=0.19845764338970184
perturb layer layer1.0.conv1 to A4W8 and layer layer1.1.conv2 to A2W2 p=0.4318143427371979
perturb layer layer1.0.conv1 to A4W8 and layer layer1.1.conv2 to A2W4 p=0.2923344299197197
perturb layer layer1.0.conv1 to A4W8 and layer layer1.1.conv2 to A2W8 p=0.3234372213482857
perturb layer layer1.0.conv1 to A4W8 and layer layer1.1.conv2 to A4W2 p=0.33842241019010544
perturb layer layer1.0.conv1 to A4W8 and layer layer1.1.conv2 to A4W4 p=0.19806454330682755
perturb layer layer1.0.conv1 to A4W8 and layer layer1.1.conv2 to A4W8 p=0.22391082346439362
perturb layer layer1.0.conv1 to A4W8 and layer layer1.1.conv2 to A8W2 p=0.3309745043516159
perturb layer layer1.0.conv1 to A4W8 and layer layer1.1.conv2 to A8W4 p=0.18705495446920395
perturb layer layer1.0.conv1 to A4W8 and layer layer1.1.conv2 to A8W8 p=0.2124067321419716
perturb layer layer1.0.conv1 to A8W2 and layer layer1.1.conv2 to A2W2 p=1.1334164589643478
perturb layer layer1.0.conv1 to A8W2 and layer layer1.1.conv2 to A2W4 p=0.8729849755764008
perturb layer layer1.0.conv1 to A8W2 and layer layer1.1.conv2 to A2W8 p=0.8416893482208252
perturb layer layer1.0.conv1 to A8W2 and layer layer1.1.conv2 to A4W2 p=1.091529220342636
perturb layer layer1.0.conv1 to A8W2 and layer layer1.1.conv2 to A4W4 p=0.7982467710971832
perturb layer layer1.0.conv1 to A8W2 and layer layer1.1.conv2 to A4W8 p=0.7697270512580872
perturb layer layer1.0.conv1 to A8W2 and layer layer1.1.conv2 to A8W2 p=1.0840457081794739
perturb layer layer1.0.conv1 to A8W2 and layer layer1.1.conv2 to A8W4 p=0.7765962481498718
perturb layer layer1.0.conv1 to A8W2 and layer layer1.1.conv2 to A8W8 p=0.7373391538858414
perturb layer layer1.0.conv1 to A8W4 and layer layer1.1.conv2 to A2W2 p=0.23544463515281677
perturb layer layer1.0.conv1 to A8W4 and layer layer1.1.conv2 to A2W4 p=0.05938065052032471
perturb layer layer1.0.conv1 to A8W4 and layer layer1.1.conv2 to A2W8 p=0.07725120335817337
perturb layer layer1.0.conv1 to A8W4 and layer layer1.1.conv2 to A4W2 p=0.15268318355083466
perturb layer layer1.0.conv1 to A8W4 and layer layer1.1.conv2 to A4W4 p=-0.011391177773475647
perturb layer layer1.0.conv1 to A8W4 and layer layer1.1.conv2 to A4W8 p=0.0071782395243644714
perturb layer layer1.0.conv1 to A8W4 and layer layer1.1.conv2 to A8W2 p=0.14186085015535355
perturb layer layer1.0.conv1 to A8W4 and layer layer1.1.conv2 to A8W4 p=-0.022996187210083008
perturb layer layer1.0.conv1 to A8W4 and layer layer1.1.conv2 to A8W8 p=-0.012363135814666748
perturb layer layer1.0.conv1 to A8W8 and layer layer1.1.conv2 to A2W2 p=0.2834707498550415
perturb layer layer1.0.conv1 to A8W8 and layer layer1.1.conv2 to A2W4 p=0.0934910848736763
perturb layer layer1.0.conv1 to A8W8 and layer layer1.1.conv2 to A2W8 p=0.10346158593893051
perturb layer layer1.0.conv1 to A8W8 and layer layer1.1.conv2 to A4W2 p=0.19545730203390121
perturb layer layer1.0.conv1 to A8W8 and layer layer1.1.conv2 to A4W4 p=0.002600446343421936
perturb layer layer1.0.conv1 to A8W8 and layer layer1.1.conv2 to A4W8 p=0.01054428517818451
perturb layer layer1.0.conv1 to A8W8 and layer layer1.1.conv2 to A8W2 p=0.18648941069841385
perturb layer layer1.0.conv1 to A8W8 and layer layer1.1.conv2 to A8W4 p=-0.0013860538601875305
perturb layer layer1.0.conv1 to A8W8 and layer layer1.1.conv2 to A8W8 p=0.003423541784286499
perturb layer layer1.0.conv1 to A2W2 and layer layer1.2.conv1 to A2W2 p=3.0453689843416214
perturb layer layer1.0.conv1 to A2W2 and layer layer1.2.conv1 to A2W4 p=3.2601544111967087
perturb layer layer1.0.conv1 to A2W2 and layer layer1.2.conv1 to A2W8 p=3.2582650929689407
perturb layer layer1.0.conv1 to A2W2 and layer layer1.2.conv1 to A4W2 p=1.9406694322824478
perturb layer layer1.0.conv1 to A2W2 and layer layer1.2.conv1 to A4W4 p=2.1748044937849045
perturb layer layer1.0.conv1 to A2W2 and layer layer1.2.conv1 to A4W8 p=2.170382186770439
perturb layer layer1.0.conv1 to A2W2 and layer layer1.2.conv1 to A8W2 p=1.8652839809656143
perturb layer layer1.0.conv1 to A2W2 and layer layer1.2.conv1 to A8W4 p=2.068617656826973
perturb layer layer1.0.conv1 to A2W2 and layer layer1.2.conv1 to A8W8 p=2.067672476172447
perturb layer layer1.0.conv1 to A2W4 and layer layer1.2.conv1 to A2W2 p=1.7808101922273636
perturb layer layer1.0.conv1 to A2W4 and layer layer1.2.conv1 to A2W4 p=1.9846544414758682
perturb layer layer1.0.conv1 to A2W4 and layer layer1.2.conv1 to A2W8 p=2.027749702334404
perturb layer layer1.0.conv1 to A2W4 and layer layer1.2.conv1 to A4W2 p=1.0874772369861603
perturb layer layer1.0.conv1 to A2W4 and layer layer1.2.conv1 to A4W4 p=1.2827353924512863
perturb layer layer1.0.conv1 to A2W4 and layer layer1.2.conv1 to A4W8 p=1.3193534016609192
perturb layer layer1.0.conv1 to A2W4 and layer layer1.2.conv1 to A8W2 p=1.0387267023324966
perturb layer layer1.0.conv1 to A2W4 and layer layer1.2.conv1 to A8W4 p=1.2184530347585678
perturb layer layer1.0.conv1 to A2W4 and layer layer1.2.conv1 to A8W8 p=1.2516245394945145
perturb layer layer1.0.conv1 to A2W8 and layer layer1.2.conv1 to A2W2 p=1.6539233475923538
perturb layer layer1.0.conv1 to A2W8 and layer layer1.2.conv1 to A2W4 p=1.88234643638134
perturb layer layer1.0.conv1 to A2W8 and layer layer1.2.conv1 to A2W8 p=1.932680293917656
perturb layer layer1.0.conv1 to A2W8 and layer layer1.2.conv1 to A4W2 p=1.0213015377521515
perturb layer layer1.0.conv1 to A2W8 and layer layer1.2.conv1 to A4W4 p=1.2427911013364792
perturb layer layer1.0.conv1 to A2W8 and layer layer1.2.conv1 to A4W8 p=1.276950478553772
perturb layer layer1.0.conv1 to A2W8 and layer layer1.2.conv1 to A8W2 p=0.9843330234289169
perturb layer layer1.0.conv1 to A2W8 and layer layer1.2.conv1 to A8W4 p=1.1924419552087784
perturb layer layer1.0.conv1 to A2W8 and layer layer1.2.conv1 to A8W8 p=1.2266056835651398
perturb layer layer1.0.conv1 to A4W2 and layer layer1.2.conv1 to A2W2 p=1.771086648106575
perturb layer layer1.0.conv1 to A4W2 and layer layer1.2.conv1 to A2W4 p=1.8465321213006973
perturb layer layer1.0.conv1 to A4W2 and layer layer1.2.conv1 to A2W8 p=1.8295825272798538
perturb layer layer1.0.conv1 to A4W2 and layer layer1.2.conv1 to A4W2 p=0.9653325825929642
perturb layer layer1.0.conv1 to A4W2 and layer layer1.2.conv1 to A4W4 p=1.028523325920105
perturb layer layer1.0.conv1 to A4W2 and layer layer1.2.conv1 to A4W8 p=1.01856729388237
perturb layer layer1.0.conv1 to A4W2 and layer layer1.2.conv1 to A8W2 p=0.9218342453241348
perturb layer layer1.0.conv1 to A4W2 and layer layer1.2.conv1 to A8W4 p=0.9673875868320465
perturb layer layer1.0.conv1 to A4W2 and layer layer1.2.conv1 to A8W8 p=0.9562063813209534
perturb layer layer1.0.conv1 to A4W4 and layer layer1.2.conv1 to A2W2 p=0.7263430953025818
perturb layer layer1.0.conv1 to A4W4 and layer layer1.2.conv1 to A2W4 p=0.8577768951654434
perturb layer layer1.0.conv1 to A4W4 and layer layer1.2.conv1 to A2W8 p=0.8755571395158768
perturb layer layer1.0.conv1 to A4W4 and layer layer1.2.conv1 to A4W2 p=0.14334122091531754
perturb layer layer1.0.conv1 to A4W4 and layer layer1.2.conv1 to A4W4 p=0.22814517468214035
perturb layer layer1.0.conv1 to A4W4 and layer layer1.2.conv1 to A4W8 p=0.2426304742693901
perturb layer layer1.0.conv1 to A4W4 and layer layer1.2.conv1 to A8W2 p=0.10768979787826538
perturb layer layer1.0.conv1 to A4W4 and layer layer1.2.conv1 to A8W4 p=0.18476268649101257
perturb layer layer1.0.conv1 to A4W4 and layer layer1.2.conv1 to A8W8 p=0.19934191554784775
perturb layer layer1.0.conv1 to A4W8 and layer layer1.2.conv1 to A2W2 p=0.5715361535549164
perturb layer layer1.0.conv1 to A4W8 and layer layer1.2.conv1 to A2W4 p=0.7357789278030396
perturb layer layer1.0.conv1 to A4W8 and layer layer1.2.conv1 to A2W8 p=0.7630036175251007
perturb layer layer1.0.conv1 to A4W8 and layer layer1.2.conv1 to A4W2 p=0.138186976313591
perturb layer layer1.0.conv1 to A4W8 and layer layer1.2.conv1 to A4W4 p=0.24259605258703232
perturb layer layer1.0.conv1 to A4W8 and layer layer1.2.conv1 to A4W8 p=0.25936754792928696
perturb layer layer1.0.conv1 to A4W8 and layer layer1.2.conv1 to A8W2 p=0.09717819094657898
perturb layer layer1.0.conv1 to A4W8 and layer layer1.2.conv1 to A8W4 p=0.19481679797172546
perturb layer layer1.0.conv1 to A4W8 and layer layer1.2.conv1 to A8W8 p=0.21197176724672318
perturb layer layer1.0.conv1 to A8W2 and layer layer1.2.conv1 to A2W2 p=1.563637986779213
perturb layer layer1.0.conv1 to A8W2 and layer layer1.2.conv1 to A2W4 p=1.6205352395772934
perturb layer layer1.0.conv1 to A8W2 and layer layer1.2.conv1 to A2W8 p=1.6101877838373184
perturb layer layer1.0.conv1 to A8W2 and layer layer1.2.conv1 to A4W2 p=0.8295726180076599
perturb layer layer1.0.conv1 to A8W2 and layer layer1.2.conv1 to A4W4 p=0.8429525792598724
perturb layer layer1.0.conv1 to A8W2 and layer layer1.2.conv1 to A4W8 p=0.8281145989894867
perturb layer layer1.0.conv1 to A8W2 and layer layer1.2.conv1 to A8W2 p=0.7695588618516922
perturb layer layer1.0.conv1 to A8W2 and layer layer1.2.conv1 to A8W4 p=0.7549115866422653
perturb layer layer1.0.conv1 to A8W2 and layer layer1.2.conv1 to A8W8 p=0.7386098504066467
perturb layer layer1.0.conv1 to A8W4 and layer layer1.2.conv1 to A2W2 p=0.4757698029279709
perturb layer layer1.0.conv1 to A8W4 and layer layer1.2.conv1 to A2W4 p=0.5621999651193619
perturb layer layer1.0.conv1 to A8W4 and layer layer1.2.conv1 to A2W8 p=0.5837260037660599
perturb layer layer1.0.conv1 to A8W4 and layer layer1.2.conv1 to A4W2 p=-0.027061693370342255
perturb layer layer1.0.conv1 to A8W4 and layer layer1.2.conv1 to A4W4 p=0.024092473089694977
perturb layer layer1.0.conv1 to A8W4 and layer layer1.2.conv1 to A4W8 p=0.034962430596351624
perturb layer layer1.0.conv1 to A8W4 and layer layer1.2.conv1 to A8W2 p=-0.037612833082675934
perturb layer layer1.0.conv1 to A8W4 and layer layer1.2.conv1 to A8W4 p=-0.01856371760368347
perturb layer layer1.0.conv1 to A8W4 and layer layer1.2.conv1 to A8W8 p=-0.012853972613811493
perturb layer layer1.0.conv1 to A8W8 and layer layer1.2.conv1 to A2W2 p=0.3947119116783142
perturb layer layer1.0.conv1 to A8W8 and layer layer1.2.conv1 to A2W4 p=0.5149757713079453
perturb layer layer1.0.conv1 to A8W8 and layer layer1.2.conv1 to A2W8 p=0.542099192738533
perturb layer layer1.0.conv1 to A8W8 and layer layer1.2.conv1 to A4W2 p=-0.04514387249946594
perturb layer layer1.0.conv1 to A8W8 and layer layer1.2.conv1 to A4W4 p=0.024088196456432343
perturb layer layer1.0.conv1 to A8W8 and layer layer1.2.conv1 to A4W8 p=0.04193482547998428
perturb layer layer1.0.conv1 to A8W8 and layer layer1.2.conv1 to A8W2 p=-0.05599651485681534
perturb layer layer1.0.conv1 to A8W8 and layer layer1.2.conv1 to A8W4 p=-0.00882246345281601
perturb layer layer1.0.conv1 to A8W8 and layer layer1.2.conv1 to A8W8 p=0.00457330048084259
perturb layer layer1.0.conv1 to A2W2 and layer layer1.2.conv2 to A2W2 p=1.993699923157692
perturb layer layer1.0.conv1 to A2W2 and layer layer1.2.conv2 to A2W4 p=1.9998492151498795
perturb layer layer1.0.conv1 to A2W2 and layer layer1.2.conv2 to A2W8 p=2.0883219093084335
perturb layer layer1.0.conv1 to A2W2 and layer layer1.2.conv2 to A4W2 p=1.9894524961709976
perturb layer layer1.0.conv1 to A2W2 and layer layer1.2.conv2 to A4W4 p=1.984771654009819
perturb layer layer1.0.conv1 to A2W2 and layer layer1.2.conv2 to A4W8 p=2.081073597073555
perturb layer layer1.0.conv1 to A2W2 and layer layer1.2.conv2 to A8W2 p=1.9806432574987411
perturb layer layer1.0.conv1 to A2W2 and layer layer1.2.conv2 to A8W4 p=1.9714874178171158
perturb layer layer1.0.conv1 to A2W2 and layer layer1.2.conv2 to A8W8 p=2.0668440014123917
perturb layer layer1.0.conv1 to A2W4 and layer layer1.2.conv2 to A2W2 p=1.1505056470632553
perturb layer layer1.0.conv1 to A2W4 and layer layer1.2.conv2 to A2W4 p=1.194731429219246
perturb layer layer1.0.conv1 to A2W4 and layer layer1.2.conv2 to A2W8 p=1.2484413832426071
perturb layer layer1.0.conv1 to A2W4 and layer layer1.2.conv2 to A4W2 p=1.1557059288024902
perturb layer layer1.0.conv1 to A2W4 and layer layer1.2.conv2 to A4W4 p=1.207057386636734
perturb layer layer1.0.conv1 to A2W4 and layer layer1.2.conv2 to A4W8 p=1.2638590037822723
perturb layer layer1.0.conv1 to A2W4 and layer layer1.2.conv2 to A8W2 p=1.1509135216474533
perturb layer layer1.0.conv1 to A2W4 and layer layer1.2.conv2 to A8W4 p=1.1979819983243942
perturb layer layer1.0.conv1 to A2W4 and layer layer1.2.conv2 to A8W8 p=1.2530202865600586
perturb layer layer1.0.conv1 to A2W8 and layer layer1.2.conv2 to A2W2 p=1.135642483830452
perturb layer layer1.0.conv1 to A2W8 and layer layer1.2.conv2 to A2W4 p=1.1748802810907364
perturb layer layer1.0.conv1 to A2W8 and layer layer1.2.conv2 to A2W8 p=1.230151727795601
perturb layer layer1.0.conv1 to A2W8 and layer layer1.2.conv2 to A4W2 p=1.1328182220458984
perturb layer layer1.0.conv1 to A2W8 and layer layer1.2.conv2 to A4W4 p=1.179992988705635
perturb layer layer1.0.conv1 to A2W8 and layer layer1.2.conv2 to A4W8 p=1.2367175966501236
perturb layer layer1.0.conv1 to A2W8 and layer layer1.2.conv2 to A8W2 p=1.1272147595882416
perturb layer layer1.0.conv1 to A2W8 and layer layer1.2.conv2 to A8W4 p=1.1694861948490143
perturb layer layer1.0.conv1 to A2W8 and layer layer1.2.conv2 to A8W8 p=1.225913554430008
perturb layer layer1.0.conv1 to A4W2 and layer layer1.2.conv2 to A2W2 p=0.9107467383146286
perturb layer layer1.0.conv1 to A4W2 and layer layer1.2.conv2 to A2W4 p=0.8927788734436035
perturb layer layer1.0.conv1 to A4W2 and layer layer1.2.conv2 to A2W8 p=0.9420260339975357
perturb layer layer1.0.conv1 to A4W2 and layer layer1.2.conv2 to A4W2 p=0.9318715780973434
perturb layer layer1.0.conv1 to A4W2 and layer layer1.2.conv2 to A4W4 p=0.9133256375789642
perturb layer layer1.0.conv1 to A4W2 and layer layer1.2.conv2 to A4W8 p=0.9706484824419022
perturb layer layer1.0.conv1 to A4W2 and layer layer1.2.conv2 to A8W2 p=0.9195625483989716
perturb layer layer1.0.conv1 to A4W2 and layer layer1.2.conv2 to A8W4 p=0.9013186991214752
perturb layer layer1.0.conv1 to A4W2 and layer layer1.2.conv2 to A8W8 p=0.9553116858005524
perturb layer layer1.0.conv1 to A4W4 and layer layer1.2.conv2 to A2W2 p=0.13599758595228195
perturb layer layer1.0.conv1 to A4W4 and layer layer1.2.conv2 to A2W4 p=0.16423433274030685
perturb layer layer1.0.conv1 to A4W4 and layer layer1.2.conv2 to A2W8 p=0.1945585086941719
perturb layer layer1.0.conv1 to A4W4 and layer layer1.2.conv2 to A4W2 p=0.1427188739180565
perturb layer layer1.0.conv1 to A4W4 and layer layer1.2.conv2 to A4W4 p=0.1677272468805313
perturb layer layer1.0.conv1 to A4W4 and layer layer1.2.conv2 to A4W8 p=0.20180294662714005
perturb layer layer1.0.conv1 to A4W4 and layer layer1.2.conv2 to A8W2 p=0.14140388369560242
perturb layer layer1.0.conv1 to A4W4 and layer layer1.2.conv2 to A8W4 p=0.16437777131795883
perturb layer layer1.0.conv1 to A4W4 and layer layer1.2.conv2 to A8W8 p=0.19849322736263275
perturb layer layer1.0.conv1 to A4W8 and layer layer1.2.conv2 to A2W2 p=0.13620568066835403
perturb layer layer1.0.conv1 to A4W8 and layer layer1.2.conv2 to A2W4 p=0.16092874109745026
perturb layer layer1.0.conv1 to A4W8 and layer layer1.2.conv2 to A2W8 p=0.19344405829906464
perturb layer layer1.0.conv1 to A4W8 and layer layer1.2.conv2 to A4W2 p=0.14996018260717392
perturb layer layer1.0.conv1 to A4W8 and layer layer1.2.conv2 to A4W4 p=0.17949853092432022
perturb layer layer1.0.conv1 to A4W8 and layer layer1.2.conv2 to A4W8 p=0.2188192531466484
perturb layer layer1.0.conv1 to A4W8 and layer layer1.2.conv2 to A8W2 p=0.14870883524417877
perturb layer layer1.0.conv1 to A4W8 and layer layer1.2.conv2 to A8W4 p=0.17557281255722046
perturb layer layer1.0.conv1 to A4W8 and layer layer1.2.conv2 to A8W8 p=0.21279897540807724
perturb layer layer1.0.conv1 to A8W2 and layer layer1.2.conv2 to A2W2 p=0.7337920963764191
perturb layer layer1.0.conv1 to A8W2 and layer layer1.2.conv2 to A2W4 p=0.7109566479921341
perturb layer layer1.0.conv1 to A8W2 and layer layer1.2.conv2 to A2W8 p=0.7566946893930435
perturb layer layer1.0.conv1 to A8W2 and layer layer1.2.conv2 to A4W2 p=0.7220171093940735
perturb layer layer1.0.conv1 to A8W2 and layer layer1.2.conv2 to A4W4 p=0.6981498450040817
perturb layer layer1.0.conv1 to A8W2 and layer layer1.2.conv2 to A4W8 p=0.7402154207229614
perturb layer layer1.0.conv1 to A8W2 and layer layer1.2.conv2 to A8W2 p=0.7244475185871124
perturb layer layer1.0.conv1 to A8W2 and layer layer1.2.conv2 to A8W4 p=0.696030780673027
perturb layer layer1.0.conv1 to A8W2 and layer layer1.2.conv2 to A8W8 p=0.7372823804616928
perturb layer layer1.0.conv1 to A8W4 and layer layer1.2.conv2 to A2W2 p=-0.04441153258085251
perturb layer layer1.0.conv1 to A8W4 and layer layer1.2.conv2 to A2W4 p=-0.03242065757513046
perturb layer layer1.0.conv1 to A8W4 and layer layer1.2.conv2 to A2W8 p=-0.010490715503692627
perturb layer layer1.0.conv1 to A8W4 and layer layer1.2.conv2 to A4W2 p=-0.04365725815296173
perturb layer layer1.0.conv1 to A8W4 and layer layer1.2.conv2 to A4W4 p=-0.03175970911979675
perturb layer layer1.0.conv1 to A8W4 and layer layer1.2.conv2 to A4W8 p=-0.004776470363140106
perturb layer layer1.0.conv1 to A8W4 and layer layer1.2.conv2 to A8W2 p=-0.0474044606089592
perturb layer layer1.0.conv1 to A8W4 and layer layer1.2.conv2 to A8W4 p=-0.03704587370157242
perturb layer layer1.0.conv1 to A8W4 and layer layer1.2.conv2 to A8W8 p=-0.012540772557258606
perturb layer layer1.0.conv1 to A8W8 and layer layer1.2.conv2 to A2W2 p=-0.042217500507831573
perturb layer layer1.0.conv1 to A8W8 and layer layer1.2.conv2 to A2W4 p=-0.026268884539604187
perturb layer layer1.0.conv1 to A8W8 and layer layer1.2.conv2 to A2W8 p=-0.0009199604392051697
perturb layer layer1.0.conv1 to A8W8 and layer layer1.2.conv2 to A4W2 p=-0.03991732746362686
perturb layer layer1.0.conv1 to A8W8 and layer layer1.2.conv2 to A4W4 p=-0.021372951567173004
perturb layer layer1.0.conv1 to A8W8 and layer layer1.2.conv2 to A4W8 p=0.007645256817340851
perturb layer layer1.0.conv1 to A8W8 and layer layer1.2.conv2 to A8W2 p=-0.04179324954748154
perturb layer layer1.0.conv1 to A8W8 and layer layer1.2.conv2 to A8W4 p=-0.025130972266197205
perturb layer layer1.0.conv1 to A8W8 and layer layer1.2.conv2 to A8W8 p=0.0035417452454566956
perturb layer layer1.0.conv1 to A2W2 and layer layer1.3.conv1 to A2W2 p=3.493069276213646
perturb layer layer1.0.conv1 to A2W2 and layer layer1.3.conv1 to A2W4 p=3.2460393458604813
perturb layer layer1.0.conv1 to A2W2 and layer layer1.3.conv1 to A2W8 p=3.1851079910993576
perturb layer layer1.0.conv1 to A2W2 and layer layer1.3.conv1 to A4W2 p=2.6135004311800003
perturb layer layer1.0.conv1 to A2W2 and layer layer1.3.conv1 to A4W4 p=2.243649885058403
perturb layer layer1.0.conv1 to A2W2 and layer layer1.3.conv1 to A4W8 p=2.1632044166326523
perturb layer layer1.0.conv1 to A2W2 and layer layer1.3.conv1 to A8W2 p=2.5470246225595474
perturb layer layer1.0.conv1 to A2W2 and layer layer1.3.conv1 to A8W4 p=2.1405126601457596
perturb layer layer1.0.conv1 to A2W2 and layer layer1.3.conv1 to A8W8 p=2.06840480864048
perturb layer layer1.0.conv1 to A2W4 and layer layer1.3.conv1 to A2W2 p=2.227991506457329
perturb layer layer1.0.conv1 to A2W4 and layer layer1.3.conv1 to A2W4 p=2.1430379897356033
perturb layer layer1.0.conv1 to A2W4 and layer layer1.3.conv1 to A2W8 p=2.0937229841947556
perturb layer layer1.0.conv1 to A2W4 and layer layer1.3.conv1 to A4W2 p=1.3581677079200745
perturb layer layer1.0.conv1 to A2W4 and layer layer1.3.conv1 to A4W4 p=1.3315594643354416
perturb layer layer1.0.conv1 to A2W4 and layer layer1.3.conv1 to A4W8 p=1.2950757592916489
perturb layer layer1.0.conv1 to A2W4 and layer layer1.3.conv1 to A8W2 p=1.3328577131032944
perturb layer layer1.0.conv1 to A2W4 and layer layer1.3.conv1 to A8W4 p=1.2889156937599182
perturb layer layer1.0.conv1 to A2W4 and layer layer1.3.conv1 to A8W8 p=1.2537432760000229
perturb layer layer1.0.conv1 to A2W8 and layer layer1.3.conv1 to A2W2 p=2.0181779712438583
perturb layer layer1.0.conv1 to A2W8 and layer layer1.3.conv1 to A2W4 p=2.0217339247465134
perturb layer layer1.0.conv1 to A2W8 and layer layer1.3.conv1 to A2W8 p=1.9978106170892715
perturb layer layer1.0.conv1 to A2W8 and layer layer1.3.conv1 to A4W2 p=1.302093505859375
perturb layer layer1.0.conv1 to A2W8 and layer layer1.3.conv1 to A4W4 p=1.2984920889139175
perturb layer layer1.0.conv1 to A2W8 and layer layer1.3.conv1 to A4W8 p=1.2840254455804825
perturb layer layer1.0.conv1 to A2W8 and layer layer1.3.conv1 to A8W2 p=1.2510357648134232
perturb layer layer1.0.conv1 to A2W8 and layer layer1.3.conv1 to A8W4 p=1.2411381751298904
perturb layer layer1.0.conv1 to A2W8 and layer layer1.3.conv1 to A8W8 p=1.2257633358240128
perturb layer layer1.0.conv1 to A4W2 and layer layer1.3.conv1 to A2W2 p=2.2729258984327316
perturb layer layer1.0.conv1 to A4W2 and layer layer1.3.conv1 to A2W4 p=1.9460186213254929
perturb layer layer1.0.conv1 to A4W2 and layer layer1.3.conv1 to A2W8 p=1.896447792649269
perturb layer layer1.0.conv1 to A4W2 and layer layer1.3.conv1 to A4W2 p=1.47628453373909
perturb layer layer1.0.conv1 to A4W2 and layer layer1.3.conv1 to A4W4 p=1.075230062007904
perturb layer layer1.0.conv1 to A4W2 and layer layer1.3.conv1 to A4W8 p=1.0160174369812012
perturb layer layer1.0.conv1 to A4W2 and layer layer1.3.conv1 to A8W2 p=1.4269500523805618
perturb layer layer1.0.conv1 to A4W2 and layer layer1.3.conv1 to A8W4 p=1.015082910656929
perturb layer layer1.0.conv1 to A4W2 and layer layer1.3.conv1 to A8W8 p=0.9571278542280197
perturb layer layer1.0.conv1 to A4W4 and layer layer1.3.conv1 to A2W2 p=1.1479410082101822
perturb layer layer1.0.conv1 to A4W4 and layer layer1.3.conv1 to A2W4 p=0.9006170928478241
perturb layer layer1.0.conv1 to A4W4 and layer layer1.3.conv1 to A2W8 p=0.8627655208110809
perturb layer layer1.0.conv1 to A4W4 and layer layer1.3.conv1 to A4W2 p=0.42662447690963745
perturb layer layer1.0.conv1 to A4W4 and layer layer1.3.conv1 to A4W4 p=0.2487221509218216
perturb layer layer1.0.conv1 to A4W4 and layer layer1.3.conv1 to A4W8 p=0.22459129989147186
perturb layer layer1.0.conv1 to A4W4 and layer layer1.3.conv1 to A8W2 p=0.39777522534132004
perturb layer layer1.0.conv1 to A4W4 and layer layer1.3.conv1 to A8W4 p=0.2211342751979828
perturb layer layer1.0.conv1 to A4W4 and layer layer1.3.conv1 to A8W8 p=0.1973961889743805
perturb layer layer1.0.conv1 to A4W8 and layer layer1.3.conv1 to A2W2 p=1.018106386065483
perturb layer layer1.0.conv1 to A4W8 and layer layer1.3.conv1 to A2W4 p=0.8332135677337646
perturb layer layer1.0.conv1 to A4W8 and layer layer1.3.conv1 to A2W8 p=0.8165015876293182
perturb layer layer1.0.conv1 to A4W8 and layer layer1.3.conv1 to A4W2 p=0.3810978978872299
perturb layer layer1.0.conv1 to A4W8 and layer layer1.3.conv1 to A4W4 p=0.2643315717577934
perturb layer layer1.0.conv1 to A4W8 and layer layer1.3.conv1 to A4W8 p=0.2523968666791916
perturb layer layer1.0.conv1 to A4W8 and layer layer1.3.conv1 to A8W2 p=0.3463316783308983
perturb layer layer1.0.conv1 to A4W8 and layer layer1.3.conv1 to A8W4 p=0.22478676587343216
perturb layer layer1.0.conv1 to A4W8 and layer layer1.3.conv1 to A8W8 p=0.213101826608181
perturb layer layer1.0.conv1 to A8W2 and layer layer1.3.conv1 to A2W2 p=2.006541535258293
perturb layer layer1.0.conv1 to A8W2 and layer layer1.3.conv1 to A2W4 p=1.6808338016271591
perturb layer layer1.0.conv1 to A8W2 and layer layer1.3.conv1 to A2W8 p=1.640021339058876
perturb layer layer1.0.conv1 to A8W2 and layer layer1.3.conv1 to A4W2 p=1.3024927526712418
perturb layer layer1.0.conv1 to A8W2 and layer layer1.3.conv1 to A4W4 p=0.8696191310882568
perturb layer layer1.0.conv1 to A8W2 and layer layer1.3.conv1 to A4W8 p=0.813460573554039
perturb layer layer1.0.conv1 to A8W2 and layer layer1.3.conv1 to A8W2 p=1.2476924359798431
perturb layer layer1.0.conv1 to A8W2 and layer layer1.3.conv1 to A8W4 p=0.7916943579912186
perturb layer layer1.0.conv1 to A8W2 and layer layer1.3.conv1 to A8W8 p=0.7356804758310318
perturb layer layer1.0.conv1 to A8W4 and layer layer1.3.conv1 to A2W2 p=0.8864917159080505
perturb layer layer1.0.conv1 to A8W4 and layer layer1.3.conv1 to A2W4 p=0.6468282341957092
perturb layer layer1.0.conv1 to A8W4 and layer layer1.3.conv1 to A2W8 p=0.6150717437267303
perturb layer layer1.0.conv1 to A8W4 and layer layer1.3.conv1 to A4W2 p=0.20595161616802216
perturb layer layer1.0.conv1 to A8W4 and layer layer1.3.conv1 to A4W4 p=0.055447399616241455
perturb layer layer1.0.conv1 to A8W4 and layer layer1.3.conv1 to A4W8 p=0.037647925317287445
perturb layer layer1.0.conv1 to A8W4 and layer layer1.3.conv1 to A8W2 p=0.17530574649572372
perturb layer layer1.0.conv1 to A8W4 and layer layer1.3.conv1 to A8W4 p=0.00847066193819046
perturb layer layer1.0.conv1 to A8W4 and layer layer1.3.conv1 to A8W8 p=-0.010841786861419678
perturb layer layer1.0.conv1 to A8W8 and layer layer1.3.conv1 to A2W2 p=0.7732470482587814
perturb layer layer1.0.conv1 to A8W8 and layer layer1.3.conv1 to A2W4 p=0.5989174544811249
perturb layer layer1.0.conv1 to A8W8 and layer layer1.3.conv1 to A2W8 p=0.5802464634180069
perturb layer layer1.0.conv1 to A8W8 and layer layer1.3.conv1 to A4W2 p=0.15916141867637634
perturb layer layer1.0.conv1 to A8W8 and layer layer1.3.conv1 to A4W4 p=0.053122855722904205
perturb layer layer1.0.conv1 to A8W8 and layer layer1.3.conv1 to A4W8 p=0.04409961402416229
perturb layer layer1.0.conv1 to A8W8 and layer layer1.3.conv1 to A8W2 p=0.13325686752796173
perturb layer layer1.0.conv1 to A8W8 and layer layer1.3.conv1 to A8W4 p=0.014134280383586884
perturb layer layer1.0.conv1 to A8W8 and layer layer1.3.conv1 to A8W8 p=0.003705933690071106
perturb layer layer1.0.conv1 to A2W2 and layer layer1.3.conv2 to A2W2 p=2.3809972256422043
perturb layer layer1.0.conv1 to A2W2 and layer layer1.3.conv2 to A2W4 p=2.102595940232277
perturb layer layer1.0.conv1 to A2W2 and layer layer1.3.conv2 to A2W8 p=2.1233761459589005
perturb layer layer1.0.conv1 to A2W2 and layer layer1.3.conv2 to A4W2 p=2.3526905924081802
perturb layer layer1.0.conv1 to A2W2 and layer layer1.3.conv2 to A4W4 p=2.057668164372444
perturb layer layer1.0.conv1 to A2W2 and layer layer1.3.conv2 to A4W8 p=2.0747873336076736
perturb layer layer1.0.conv1 to A2W2 and layer layer1.3.conv2 to A8W2 p=2.349115625023842
perturb layer layer1.0.conv1 to A2W2 and layer layer1.3.conv2 to A8W4 p=2.0502673536539078
perturb layer layer1.0.conv1 to A2W2 and layer layer1.3.conv2 to A8W8 p=2.067119285464287
perturb layer layer1.0.conv1 to A2W4 and layer layer1.3.conv2 to A2W2 p=1.3493255525827408
perturb layer layer1.0.conv1 to A2W4 and layer layer1.3.conv2 to A2W4 p=1.3103805929422379
perturb layer layer1.0.conv1 to A2W4 and layer layer1.3.conv2 to A2W8 p=1.301905795931816
perturb layer layer1.0.conv1 to A2W4 and layer layer1.3.conv2 to A4W2 p=1.3216619938611984
perturb layer layer1.0.conv1 to A2W4 and layer layer1.3.conv2 to A4W4 p=1.2669296264648438
perturb layer layer1.0.conv1 to A2W4 and layer layer1.3.conv2 to A4W8 p=1.2589096873998642
perturb layer layer1.0.conv1 to A2W4 and layer layer1.3.conv2 to A8W2 p=1.321463719010353
perturb layer layer1.0.conv1 to A2W4 and layer layer1.3.conv2 to A8W4 p=1.2618767768144608
perturb layer layer1.0.conv1 to A2W4 and layer layer1.3.conv2 to A8W8 p=1.2529070526361465
perturb layer layer1.0.conv1 to A2W8 and layer layer1.3.conv2 to A2W2 p=1.2796426862478256
perturb layer layer1.0.conv1 to A2W8 and layer layer1.3.conv2 to A2W4 p=1.268824264407158
perturb layer layer1.0.conv1 to A2W8 and layer layer1.3.conv2 to A2W8 p=1.262282371520996
perturb layer layer1.0.conv1 to A2W8 and layer layer1.3.conv2 to A4W2 p=1.2634973973035812
perturb layer layer1.0.conv1 to A2W8 and layer layer1.3.conv2 to A4W4 p=1.240111529827118
perturb layer layer1.0.conv1 to A2W8 and layer layer1.3.conv2 to A4W8 p=1.2288267016410828
perturb layer layer1.0.conv1 to A2W8 and layer layer1.3.conv2 to A8W2 p=1.262021854519844
perturb layer layer1.0.conv1 to A2W8 and layer layer1.3.conv2 to A8W4 p=1.2387422174215317
perturb layer layer1.0.conv1 to A2W8 and layer layer1.3.conv2 to A8W8 p=1.2266684025526047
perturb layer layer1.0.conv1 to A4W2 and layer layer1.3.conv2 to A2W2 p=1.137261226773262
perturb layer layer1.0.conv1 to A4W2 and layer layer1.3.conv2 to A2W4 p=0.9477556049823761
perturb layer layer1.0.conv1 to A4W2 and layer layer1.3.conv2 to A2W8 p=0.9661750346422195
perturb layer layer1.0.conv1 to A4W2 and layer layer1.3.conv2 to A4W2 p=1.136959284543991
perturb layer layer1.0.conv1 to A4W2 and layer layer1.3.conv2 to A4W4 p=0.9431386440992355
perturb layer layer1.0.conv1 to A4W2 and layer layer1.3.conv2 to A4W8 p=0.9588636159896851
perturb layer layer1.0.conv1 to A4W2 and layer layer1.3.conv2 to A8W2 p=1.1335233002901077
perturb layer layer1.0.conv1 to A4W2 and layer layer1.3.conv2 to A8W4 p=0.9396529942750931
perturb layer layer1.0.conv1 to A4W2 and layer layer1.3.conv2 to A8W8 p=0.9558358788490295
perturb layer layer1.0.conv1 to A4W4 and layer layer1.3.conv2 to A2W2 p=0.30181314796209335
perturb layer layer1.0.conv1 to A4W4 and layer layer1.3.conv2 to A2W4 p=0.22152527421712875
perturb layer layer1.0.conv1 to A4W4 and layer layer1.3.conv2 to A2W8 p=0.21857766062021255
perturb layer layer1.0.conv1 to A4W4 and layer layer1.3.conv2 to A4W2 p=0.29014933854341507
perturb layer layer1.0.conv1 to A4W4 and layer layer1.3.conv2 to A4W4 p=0.20389389991760254
perturb layer layer1.0.conv1 to A4W4 and layer layer1.3.conv2 to A4W8 p=0.19990497827529907
perturb layer layer1.0.conv1 to A4W4 and layer layer1.3.conv2 to A8W2 p=0.2890807315707207
perturb layer layer1.0.conv1 to A4W4 and layer layer1.3.conv2 to A8W4 p=0.20297575742006302
perturb layer layer1.0.conv1 to A4W4 and layer layer1.3.conv2 to A8W8 p=0.19818463921546936
perturb layer layer1.0.conv1 to A4W8 and layer layer1.3.conv2 to A2W2 p=0.30070308595895767
perturb layer layer1.0.conv1 to A4W8 and layer layer1.3.conv2 to A2W4 p=0.24587289988994598
perturb layer layer1.0.conv1 to A4W8 and layer layer1.3.conv2 to A2W8 p=0.23783552646636963
perturb layer layer1.0.conv1 to A4W8 and layer layer1.3.conv2 to A4W2 p=0.2861420214176178
perturb layer layer1.0.conv1 to A4W8 and layer layer1.3.conv2 to A4W4 p=0.22784212231636047
perturb layer layer1.0.conv1 to A4W8 and layer layer1.3.conv2 to A4W8 p=0.21602052450180054
perturb layer layer1.0.conv1 to A4W8 and layer layer1.3.conv2 to A8W2 p=0.28523190319538116
perturb layer layer1.0.conv1 to A4W8 and layer layer1.3.conv2 to A8W4 p=0.2239483818411827
perturb layer layer1.0.conv1 to A4W8 and layer layer1.3.conv2 to A8W8 p=0.21224020421504974
perturb layer layer1.0.conv1 to A8W2 and layer layer1.3.conv2 to A2W2 p=0.9372288286685944
perturb layer layer1.0.conv1 to A8W2 and layer layer1.3.conv2 to A2W4 p=0.7587720602750778
perturb layer layer1.0.conv1 to A8W2 and layer layer1.3.conv2 to A2W8 p=0.7700742036104202
perturb layer layer1.0.conv1 to A8W2 and layer layer1.3.conv2 to A4W2 p=0.9251480996608734
perturb layer layer1.0.conv1 to A8W2 and layer layer1.3.conv2 to A4W4 p=0.7340064197778702
perturb layer layer1.0.conv1 to A8W2 and layer layer1.3.conv2 to A4W8 p=0.7441674917936325
perturb layer layer1.0.conv1 to A8W2 and layer layer1.3.conv2 to A8W2 p=0.9185599684715271
perturb layer layer1.0.conv1 to A8W2 and layer layer1.3.conv2 to A8W4 p=0.7250457406044006
perturb layer layer1.0.conv1 to A8W2 and layer layer1.3.conv2 to A8W8 p=0.7370462268590927
perturb layer layer1.0.conv1 to A8W4 and layer layer1.3.conv2 to A2W2 p=0.06443484127521515
perturb layer layer1.0.conv1 to A8W4 and layer layer1.3.conv2 to A2W4 p=0.009770028293132782
perturb layer layer1.0.conv1 to A8W4 and layer layer1.3.conv2 to A2W8 p=0.006676875054836273
perturb layer layer1.0.conv1 to A8W4 and layer layer1.3.conv2 to A4W2 p=0.052762165665626526
perturb layer layer1.0.conv1 to A8W4 and layer layer1.3.conv2 to A4W4 p=-0.007075168192386627
perturb layer layer1.0.conv1 to A8W4 and layer layer1.3.conv2 to A4W8 p=-0.013411574065685272
perturb layer layer1.0.conv1 to A8W4 and layer layer1.3.conv2 to A8W2 p=0.05357605218887329
perturb layer layer1.0.conv1 to A8W4 and layer layer1.3.conv2 to A8W4 p=-0.006994493305683136
perturb layer layer1.0.conv1 to A8W4 and layer layer1.3.conv2 to A8W8 p=-0.012489117681980133
perturb layer layer1.0.conv1 to A8W8 and layer layer1.3.conv2 to A2W2 p=0.061794087290763855
perturb layer layer1.0.conv1 to A8W8 and layer layer1.3.conv2 to A2W4 p=0.029935285449028015
perturb layer layer1.0.conv1 to A8W8 and layer layer1.3.conv2 to A2W8 p=0.023348115384578705
perturb layer layer1.0.conv1 to A8W8 and layer layer1.3.conv2 to A4W2 p=0.0531601756811142
perturb layer layer1.0.conv1 to A8W8 and layer layer1.3.conv2 to A4W4 p=0.013834647834300995
perturb layer layer1.0.conv1 to A8W8 and layer layer1.3.conv2 to A4W8 p=0.007692389190196991
perturb layer layer1.0.conv1 to A8W8 and layer layer1.3.conv2 to A8W2 p=0.0514974519610405
perturb layer layer1.0.conv1 to A8W8 and layer layer1.3.conv2 to A8W4 p=0.010390132665634155
perturb layer layer1.0.conv1 to A8W8 and layer layer1.3.conv2 to A8W8 p=0.003598161041736603
perturb layer layer1.0.conv1 to A2W2 and layer layer1.4.conv1 to A2W2 p=3.4545815736055374
perturb layer layer1.0.conv1 to A2W2 and layer layer1.4.conv1 to A2W4 p=3.1900923997163773
perturb layer layer1.0.conv1 to A2W2 and layer layer1.4.conv1 to A2W8 p=3.1296599954366684
perturb layer layer1.0.conv1 to A2W2 and layer layer1.4.conv1 to A4W2 p=2.4853586703538895
perturb layer layer1.0.conv1 to A2W2 and layer layer1.4.conv1 to A4W4 p=2.196605995297432
perturb layer layer1.0.conv1 to A2W2 and layer layer1.4.conv1 to A4W8 p=2.131550297141075
perturb layer layer1.0.conv1 to A2W2 and layer layer1.4.conv1 to A8W2 p=2.4184813052415848
perturb layer layer1.0.conv1 to A2W2 and layer layer1.4.conv1 to A8W4 p=2.1292026191949844
perturb layer layer1.0.conv1 to A2W2 and layer layer1.4.conv1 to A8W8 p=2.06822307407856
perturb layer layer1.0.conv1 to A2W4 and layer layer1.4.conv1 to A2W2 p=2.395227864384651
perturb layer layer1.0.conv1 to A2W4 and layer layer1.4.conv1 to A2W4 p=2.2693232148885727
perturb layer layer1.0.conv1 to A2W4 and layer layer1.4.conv1 to A2W8 p=2.24292354285717
perturb layer layer1.0.conv1 to A2W4 and layer layer1.4.conv1 to A4W2 p=1.4432563930749893
perturb layer layer1.0.conv1 to A2W4 and layer layer1.4.conv1 to A4W4 p=1.3303038626909256
perturb layer layer1.0.conv1 to A2W4 and layer layer1.4.conv1 to A4W8 p=1.3111553192138672
perturb layer layer1.0.conv1 to A2W4 and layer layer1.4.conv1 to A8W2 p=1.3820588737726212
perturb layer layer1.0.conv1 to A2W4 and layer layer1.4.conv1 to A8W4 p=1.2771311551332474
perturb layer layer1.0.conv1 to A2W4 and layer layer1.4.conv1 to A8W8 p=1.2547655254602432
perturb layer layer1.0.conv1 to A2W8 and layer layer1.4.conv1 to A2W2 p=2.270474150776863
perturb layer layer1.0.conv1 to A2W8 and layer layer1.4.conv1 to A2W4 p=2.178901121020317
perturb layer layer1.0.conv1 to A2W8 and layer layer1.4.conv1 to A2W8 p=2.1525086611509323
perturb layer layer1.0.conv1 to A2W8 and layer layer1.4.conv1 to A4W2 p=1.3538804799318314
perturb layer layer1.0.conv1 to A2W8 and layer layer1.4.conv1 to A4W4 p=1.2861549854278564
perturb layer layer1.0.conv1 to A2W8 and layer layer1.4.conv1 to A4W8 p=1.2669252157211304
perturb layer layer1.0.conv1 to A2W8 and layer layer1.4.conv1 to A8W2 p=1.313805177807808
perturb layer layer1.0.conv1 to A2W8 and layer layer1.4.conv1 to A8W4 p=1.246243804693222
perturb layer layer1.0.conv1 to A2W8 and layer layer1.4.conv1 to A8W8 p=1.227050468325615
perturb layer layer1.0.conv1 to A4W2 and layer layer1.4.conv1 to A2W2 p=2.084730848670006
perturb layer layer1.0.conv1 to A4W2 and layer layer1.4.conv1 to A2W4 p=1.8737942725419998
perturb layer layer1.0.conv1 to A4W2 and layer layer1.4.conv1 to A2W8 p=1.8335008770227432
perturb layer layer1.0.conv1 to A4W2 and layer layer1.4.conv1 to A4W2 p=1.310040220618248
perturb layer layer1.0.conv1 to A4W2 and layer layer1.4.conv1 to A4W4 p=1.0464486479759216
perturb layer layer1.0.conv1 to A4W2 and layer layer1.4.conv1 to A4W8 p=0.9885344058275223
perturb layer layer1.0.conv1 to A4W2 and layer layer1.4.conv1 to A8W2 p=1.2828550338745117
perturb layer layer1.0.conv1 to A4W2 and layer layer1.4.conv1 to A8W4 p=1.019382730126381
perturb layer layer1.0.conv1 to A4W2 and layer layer1.4.conv1 to A8W8 p=0.9576892256736755
perturb layer layer1.0.conv1 to A4W4 and layer layer1.4.conv1 to A2W2 p=1.0461048185825348
perturb layer layer1.0.conv1 to A4W4 and layer layer1.4.conv1 to A2W4 p=0.9516808539628983
perturb layer layer1.0.conv1 to A4W4 and layer layer1.4.conv1 to A2W8 p=0.918993666768074
perturb layer layer1.0.conv1 to A4W4 and layer layer1.4.conv1 to A4W2 p=0.3576089069247246
perturb layer layer1.0.conv1 to A4W4 and layer layer1.4.conv1 to A4W4 p=0.2511618584394455
perturb layer layer1.0.conv1 to A4W4 and layer layer1.4.conv1 to A4W8 p=0.2298235446214676
perturb layer layer1.0.conv1 to A4W4 and layer layer1.4.conv1 to A8W2 p=0.31624461710453033
perturb layer layer1.0.conv1 to A4W4 and layer layer1.4.conv1 to A8W4 p=0.22029465436935425
perturb layer layer1.0.conv1 to A4W4 and layer layer1.4.conv1 to A8W8 p=0.19879978895187378
perturb layer layer1.0.conv1 to A4W8 and layer layer1.4.conv1 to A2W2 p=1.011913225054741
perturb layer layer1.0.conv1 to A4W8 and layer layer1.4.conv1 to A2W4 p=0.9413157105445862
perturb layer layer1.0.conv1 to A4W8 and layer layer1.4.conv1 to A2W8 p=0.9097874909639359
perturb layer layer1.0.conv1 to A4W8 and layer layer1.4.conv1 to A4W2 p=0.31486545503139496
perturb layer layer1.0.conv1 to A4W8 and layer layer1.4.conv1 to A4W4 p=0.27126676589250565
perturb layer layer1.0.conv1 to A4W8 and layer layer1.4.conv1 to A4W8 p=0.25838013738393784
perturb layer layer1.0.conv1 to A4W8 and layer layer1.4.conv1 to A8W2 p=0.28128521144390106
perturb layer layer1.0.conv1 to A4W8 and layer layer1.4.conv1 to A8W4 p=0.2272953987121582
perturb layer layer1.0.conv1 to A4W8 and layer layer1.4.conv1 to A8W8 p=0.21163631975650787
perturb layer layer1.0.conv1 to A8W2 and layer layer1.4.conv1 to A2W2 p=1.85968579351902
perturb layer layer1.0.conv1 to A8W2 and layer layer1.4.conv1 to A2W4 p=1.6569694429636002
perturb layer layer1.0.conv1 to A8W2 and layer layer1.4.conv1 to A2W8 p=1.617703452706337
perturb layer layer1.0.conv1 to A8W2 and layer layer1.4.conv1 to A4W2 p=1.1158380806446075
perturb layer layer1.0.conv1 to A8W2 and layer layer1.4.conv1 to A4W4 p=0.8345582038164139
perturb layer layer1.0.conv1 to A8W2 and layer layer1.4.conv1 to A4W8 p=0.7715201377868652
perturb layer layer1.0.conv1 to A8W2 and layer layer1.4.conv1 to A8W2 p=1.0835063308477402
perturb layer layer1.0.conv1 to A8W2 and layer layer1.4.conv1 to A8W4 p=0.7994886934757233
perturb layer layer1.0.conv1 to A8W2 and layer layer1.4.conv1 to A8W8 p=0.7380049526691437
perturb layer layer1.0.conv1 to A8W4 and layer layer1.4.conv1 to A2W2 p=0.7776716649532318
perturb layer layer1.0.conv1 to A8W4 and layer layer1.4.conv1 to A2W4 p=0.6954424232244492
perturb layer layer1.0.conv1 to A8W4 and layer layer1.4.conv1 to A2W8 p=0.6726874560117722
perturb layer layer1.0.conv1 to A8W4 and layer layer1.4.conv1 to A4W2 p=0.14188382774591446
perturb layer layer1.0.conv1 to A8W4 and layer layer1.4.conv1 to A4W4 p=0.042481936514377594
perturb layer layer1.0.conv1 to A8W4 and layer layer1.4.conv1 to A4W8 p=0.017826564610004425
perturb layer layer1.0.conv1 to A8W4 and layer layer1.4.conv1 to A8W2 p=0.11512377113103867
perturb layer layer1.0.conv1 to A8W4 and layer layer1.4.conv1 to A8W4 p=0.010312683880329132
perturb layer layer1.0.conv1 to A8W4 and layer layer1.4.conv1 to A8W8 p=-0.012122169137001038
perturb layer layer1.0.conv1 to A8W8 and layer layer1.4.conv1 to A2W2 p=0.7252651453018188
perturb layer layer1.0.conv1 to A8W8 and layer layer1.4.conv1 to A2W4 p=0.6769046783447266
perturb layer layer1.0.conv1 to A8W8 and layer layer1.4.conv1 to A2W8 p=0.653775542974472
perturb layer layer1.0.conv1 to A8W8 and layer layer1.4.conv1 to A4W2 p=0.12011279910802841
perturb layer layer1.0.conv1 to A8W8 and layer layer1.4.conv1 to A4W4 p=0.05142618715763092
perturb layer layer1.0.conv1 to A8W8 and layer layer1.4.conv1 to A4W8 p=0.033856578171253204
perturb layer layer1.0.conv1 to A8W8 and layer layer1.4.conv1 to A8W2 p=0.09295737743377686
perturb layer layer1.0.conv1 to A8W8 and layer layer1.4.conv1 to A8W4 p=0.018593356013298035
perturb layer layer1.0.conv1 to A8W8 and layer layer1.4.conv1 to A8W8 p=0.002307906746864319
perturb layer layer1.0.conv1 to A2W2 and layer layer1.4.conv2 to A2W2 p=2.542758598923683
perturb layer layer1.0.conv1 to A2W2 and layer layer1.4.conv2 to A2W4 p=2.0998873561620712
perturb layer layer1.0.conv1 to A2W2 and layer layer1.4.conv2 to A2W8 p=2.147910252213478
perturb layer layer1.0.conv1 to A2W2 and layer layer1.4.conv2 to A4W2 p=2.478472664952278
perturb layer layer1.0.conv1 to A2W2 and layer layer1.4.conv2 to A4W4 p=2.019551232457161
perturb layer layer1.0.conv1 to A2W2 and layer layer1.4.conv2 to A4W8 p=2.070410057902336
perturb layer layer1.0.conv1 to A2W2 and layer layer1.4.conv2 to A8W2 p=2.480815038084984
perturb layer layer1.0.conv1 to A2W2 and layer layer1.4.conv2 to A8W4 p=2.0164215713739395
perturb layer layer1.0.conv1 to A2W2 and layer layer1.4.conv2 to A8W8 p=2.0676132291555405
perturb layer layer1.0.conv1 to A2W4 and layer layer1.4.conv2 to A2W2 p=1.4765035510063171
perturb layer layer1.0.conv1 to A2W4 and layer layer1.4.conv2 to A2W4 p=1.273890182375908
perturb layer layer1.0.conv1 to A2W4 and layer layer1.4.conv2 to A2W8 p=1.308595061302185
perturb layer layer1.0.conv1 to A2W4 and layer layer1.4.conv2 to A4W2 p=1.4357806891202927
perturb layer layer1.0.conv1 to A2W4 and layer layer1.4.conv2 to A4W4 p=1.2212365418672562
perturb layer layer1.0.conv1 to A2W4 and layer layer1.4.conv2 to A4W8 p=1.254881739616394
perturb layer layer1.0.conv1 to A2W4 and layer layer1.4.conv2 to A8W2 p=1.4364652335643768
perturb layer layer1.0.conv1 to A2W4 and layer layer1.4.conv2 to A8W4 p=1.2202966213226318
perturb layer layer1.0.conv1 to A2W4 and layer layer1.4.conv2 to A8W8 p=1.253404900431633
perturb layer layer1.0.conv1 to A2W8 and layer layer1.4.conv2 to A2W2 p=1.4133236855268478
perturb layer layer1.0.conv1 to A2W8 and layer layer1.4.conv2 to A2W4 p=1.2366245687007904
perturb layer layer1.0.conv1 to A2W8 and layer layer1.4.conv2 to A2W8 p=1.2684675753116608
perturb layer layer1.0.conv1 to A2W8 and layer layer1.4.conv2 to A4W2 p=1.3834710717201233
perturb layer layer1.0.conv1 to A2W8 and layer layer1.4.conv2 to A4W4 p=1.1946410089731216
perturb layer layer1.0.conv1 to A2W8 and layer layer1.4.conv2 to A4W8 p=1.2265847772359848
perturb layer layer1.0.conv1 to A2W8 and layer layer1.4.conv2 to A8W2 p=1.3852374702692032
perturb layer layer1.0.conv1 to A2W8 and layer layer1.4.conv2 to A8W4 p=1.1940267533063889
perturb layer layer1.0.conv1 to A2W8 and layer layer1.4.conv2 to A8W8 p=1.2261775434017181
perturb layer layer1.0.conv1 to A4W2 and layer layer1.4.conv2 to A2W2 p=1.3213624507188797
perturb layer layer1.0.conv1 to A4W2 and layer layer1.4.conv2 to A2W4 p=0.9812049269676208
perturb layer layer1.0.conv1 to A4W2 and layer layer1.4.conv2 to A2W8 p=1.0066763013601303
perturb layer layer1.0.conv1 to A4W2 and layer layer1.4.conv2 to A4W2 p=1.287386268377304
perturb layer layer1.0.conv1 to A4W2 and layer layer1.4.conv2 to A4W4 p=0.9345012009143829
perturb layer layer1.0.conv1 to A4W2 and layer layer1.4.conv2 to A4W8 p=0.9588574171066284
perturb layer layer1.0.conv1 to A4W2 and layer layer1.4.conv2 to A8W2 p=1.2851359993219376
perturb layer layer1.0.conv1 to A4W2 and layer layer1.4.conv2 to A8W4 p=0.9310364425182343
perturb layer layer1.0.conv1 to A4W2 and layer layer1.4.conv2 to A8W8 p=0.9555017054080963
perturb layer layer1.0.conv1 to A4W4 and layer layer1.4.conv2 to A2W2 p=0.39072369784116745
perturb layer layer1.0.conv1 to A4W4 and layer layer1.4.conv2 to A2W4 p=0.22410812228918076
perturb layer layer1.0.conv1 to A4W4 and layer layer1.4.conv2 to A2W8 p=0.2462426945567131
perturb layer layer1.0.conv1 to A4W4 and layer layer1.4.conv2 to A4W2 p=0.3599264472723007
perturb layer layer1.0.conv1 to A4W4 and layer layer1.4.conv2 to A4W4 p=0.17861127853393555
perturb layer layer1.0.conv1 to A4W4 and layer layer1.4.conv2 to A4W8 p=0.20145396143198013
perturb layer layer1.0.conv1 to A4W4 and layer layer1.4.conv2 to A8W2 p=0.35841204226017
perturb layer layer1.0.conv1 to A4W4 and layer layer1.4.conv2 to A8W4 p=0.17523600161075592
perturb layer layer1.0.conv1 to A4W4 and layer layer1.4.conv2 to A8W8 p=0.1982531175017357
perturb layer layer1.0.conv1 to A4W8 and layer layer1.4.conv2 to A2W2 p=0.3739155903458595
perturb layer layer1.0.conv1 to A4W8 and layer layer1.4.conv2 to A2W4 p=0.228242427110672
perturb layer layer1.0.conv1 to A4W8 and layer layer1.4.conv2 to A2W8 p=0.2505265548825264
perturb layer layer1.0.conv1 to A4W8 and layer layer1.4.conv2 to A4W2 p=0.3509877622127533
perturb layer layer1.0.conv1 to A4W8 and layer layer1.4.conv2 to A4W4 p=0.19237349182367325
perturb layer layer1.0.conv1 to A4W8 and layer layer1.4.conv2 to A4W8 p=0.21551242470741272
perturb layer layer1.0.conv1 to A4W8 and layer layer1.4.conv2 to A8W2 p=0.34850122034549713
perturb layer layer1.0.conv1 to A4W8 and layer layer1.4.conv2 to A8W4 p=0.18917754292488098
perturb layer layer1.0.conv1 to A4W8 and layer layer1.4.conv2 to A8W8 p=0.21236969530582428
perturb layer layer1.0.conv1 to A8W2 and layer layer1.4.conv2 to A2W2 p=1.0385652333498
perturb layer layer1.0.conv1 to A8W2 and layer layer1.4.conv2 to A2W4 p=0.7381978929042816
perturb layer layer1.0.conv1 to A8W2 and layer layer1.4.conv2 to A2W8 p=0.7588956505060196
perturb layer layer1.0.conv1 to A8W2 and layer layer1.4.conv2 to A4W2 p=1.0391487032175064
perturb layer layer1.0.conv1 to A8W2 and layer layer1.4.conv2 to A4W4 p=0.7205842733383179
perturb layer layer1.0.conv1 to A8W2 and layer layer1.4.conv2 to A4W8 p=0.7415234744548798
perturb layer layer1.0.conv1 to A8W2 and layer layer1.4.conv2 to A8W2 p=1.0374394208192825
perturb layer layer1.0.conv1 to A8W2 and layer layer1.4.conv2 to A8W4 p=0.7176675051450729
perturb layer layer1.0.conv1 to A8W2 and layer layer1.4.conv2 to A8W8 p=0.7372695505619049
perturb layer layer1.0.conv1 to A8W4 and layer layer1.4.conv2 to A2W2 p=0.1322672739624977
perturb layer layer1.0.conv1 to A8W4 and layer layer1.4.conv2 to A2W4 p=0.00750906765460968
perturb layer layer1.0.conv1 to A8W4 and layer layer1.4.conv2 to A2W8 p=0.023664481937885284
perturb layer layer1.0.conv1 to A8W4 and layer layer1.4.conv2 to A4W2 p=0.10921724140644073
perturb layer layer1.0.conv1 to A8W4 and layer layer1.4.conv2 to A4W4 p=-0.02587559074163437
perturb layer layer1.0.conv1 to A8W4 and layer layer1.4.conv2 to A4W8 p=-0.01051969826221466
perturb layer layer1.0.conv1 to A8W4 and layer layer1.4.conv2 to A8W2 p=0.10836033523082733
perturb layer layer1.0.conv1 to A8W4 and layer layer1.4.conv2 to A8W4 p=-0.02773059904575348
perturb layer layer1.0.conv1 to A8W4 and layer layer1.4.conv2 to A8W8 p=-0.012526646256446838
perturb layer layer1.0.conv1 to A8W8 and layer layer1.4.conv2 to A2W2 p=0.11937959492206573
perturb layer layer1.0.conv1 to A8W8 and layer layer1.4.conv2 to A2W4 p=0.011440277099609375
perturb layer layer1.0.conv1 to A8W8 and layer layer1.4.conv2 to A2W8 p=0.028627783060073853
perturb layer layer1.0.conv1 to A8W8 and layer layer1.4.conv2 to A4W2 p=0.1027953252196312
perturb layer layer1.0.conv1 to A8W8 and layer layer1.4.conv2 to A4W4 p=-0.01211712509393692
perturb layer layer1.0.conv1 to A8W8 and layer layer1.4.conv2 to A4W8 p=0.003372281789779663
perturb layer layer1.0.conv1 to A8W8 and layer layer1.4.conv2 to A8W2 p=0.10388574004173279
perturb layer layer1.0.conv1 to A8W8 and layer layer1.4.conv2 to A8W4 p=-0.012092620134353638
perturb layer layer1.0.conv1 to A8W8 and layer layer1.4.conv2 to A8W8 p=0.0032409951090812683
perturb layer layer1.0.conv1 to A2W2 and layer layer1.5.conv1 to A2W2 p=3.0734798461198807
perturb layer layer1.0.conv1 to A2W2 and layer layer1.5.conv1 to A2W4 p=3.106039509177208
perturb layer layer1.0.conv1 to A2W2 and layer layer1.5.conv1 to A2W8 p=3.041763409972191
perturb layer layer1.0.conv1 to A2W2 and layer layer1.5.conv1 to A4W2 p=2.2012548595666885
perturb layer layer1.0.conv1 to A2W2 and layer layer1.5.conv1 to A4W4 p=2.2050630897283554
perturb layer layer1.0.conv1 to A2W2 and layer layer1.5.conv1 to A4W8 p=2.1135407835245132
perturb layer layer1.0.conv1 to A2W2 and layer layer1.5.conv1 to A8W2 p=2.168699786067009
perturb layer layer1.0.conv1 to A2W2 and layer layer1.5.conv1 to A8W4 p=2.167090818285942
perturb layer layer1.0.conv1 to A2W2 and layer layer1.5.conv1 to A8W8 p=2.069333866238594
perturb layer layer1.0.conv1 to A2W4 and layer layer1.5.conv1 to A2W2 p=2.1302549690008163
perturb layer layer1.0.conv1 to A2W4 and layer layer1.5.conv1 to A2W4 p=2.2349311858415604
perturb layer layer1.0.conv1 to A2W4 and layer layer1.5.conv1 to A2W8 p=2.235112026333809
perturb layer layer1.0.conv1 to A2W4 and layer layer1.5.conv1 to A4W2 p=1.2266847789287567
perturb layer layer1.0.conv1 to A2W4 and layer layer1.5.conv1 to A4W4 p=1.3202967792749405
perturb layer layer1.0.conv1 to A2W4 and layer layer1.5.conv1 to A4W8 p=1.3042732924222946
perturb layer layer1.0.conv1 to A2W4 and layer layer1.5.conv1 to A8W2 p=1.1853930354118347
perturb layer layer1.0.conv1 to A2W4 and layer layer1.5.conv1 to A8W4 p=1.2693517953157425
perturb layer layer1.0.conv1 to A2W4 and layer layer1.5.conv1 to A8W8 p=1.254645437002182
perturb layer layer1.0.conv1 to A2W8 and layer layer1.5.conv1 to A2W2 p=2.0378104001283646
perturb layer layer1.0.conv1 to A2W8 and layer layer1.5.conv1 to A2W4 p=2.1509777158498764
perturb layer layer1.0.conv1 to A2W8 and layer layer1.5.conv1 to A2W8 p=2.1447478979825974
perturb layer layer1.0.conv1 to A2W8 and layer layer1.5.conv1 to A4W2 p=1.197058081626892
perturb layer layer1.0.conv1 to A2W8 and layer layer1.5.conv1 to A4W4 p=1.2871719151735306
perturb layer layer1.0.conv1 to A2W8 and layer layer1.5.conv1 to A4W8 p=1.2699078470468521
perturb layer layer1.0.conv1 to A2W8 and layer layer1.5.conv1 to A8W2 p=1.1658898144960403
perturb layer layer1.0.conv1 to A2W8 and layer layer1.5.conv1 to A8W4 p=1.249989852309227
perturb layer layer1.0.conv1 to A2W8 and layer layer1.5.conv1 to A8W8 p=1.2278325110673904
perturb layer layer1.0.conv1 to A4W2 and layer layer1.5.conv1 to A2W2 p=1.8336584717035294
perturb layer layer1.0.conv1 to A4W2 and layer layer1.5.conv1 to A2W4 p=1.7671770006418228
perturb layer layer1.0.conv1 to A4W2 and layer layer1.5.conv1 to A2W8 p=1.735800787806511
perturb layer layer1.0.conv1 to A4W2 and layer layer1.5.conv1 to A4W2 p=1.0807578265666962
perturb layer layer1.0.conv1 to A4W2 and layer layer1.5.conv1 to A4W4 p=1.0392486900091171
perturb layer layer1.0.conv1 to A4W2 and layer layer1.5.conv1 to A4W8 p=0.9761916995048523
perturb layer layer1.0.conv1 to A4W2 and layer layer1.5.conv1 to A8W2 p=1.0580961257219315
perturb layer layer1.0.conv1 to A4W2 and layer layer1.5.conv1 to A8W4 p=1.021425798535347
perturb layer layer1.0.conv1 to A4W2 and layer layer1.5.conv1 to A8W8 p=0.957368403673172
perturb layer layer1.0.conv1 to A4W4 and layer layer1.5.conv1 to A2W2 p=0.9315252304077148
perturb layer layer1.0.conv1 to A4W4 and layer layer1.5.conv1 to A2W4 p=0.9629959762096405
perturb layer layer1.0.conv1 to A4W4 and layer layer1.5.conv1 to A2W8 p=0.9419948011636734
perturb layer layer1.0.conv1 to A4W4 and layer layer1.5.conv1 to A4W2 p=0.22882315516471863
perturb layer layer1.0.conv1 to A4W4 and layer layer1.5.conv1 to A4W4 p=0.25874532759189606
perturb layer layer1.0.conv1 to A4W4 and layer layer1.5.conv1 to A4W8 p=0.22601332515478134
perturb layer layer1.0.conv1 to A4W4 and layer layer1.5.conv1 to A8W2 p=0.20072557032108307
perturb layer layer1.0.conv1 to A4W4 and layer layer1.5.conv1 to A8W4 p=0.23052721470594406
perturb layer layer1.0.conv1 to A4W4 and layer layer1.5.conv1 to A8W8 p=0.19650182127952576
perturb layer layer1.0.conv1 to A4W8 and layer layer1.5.conv1 to A2W2 p=0.9093099236488342
perturb layer layer1.0.conv1 to A4W8 and layer layer1.5.conv1 to A2W4 p=0.9593529999256134
perturb layer layer1.0.conv1 to A4W8 and layer layer1.5.conv1 to A2W8 p=0.9282365590333939
perturb layer layer1.0.conv1 to A4W8 and layer layer1.5.conv1 to A4W2 p=0.22519533336162567
perturb layer layer1.0.conv1 to A4W8 and layer layer1.5.conv1 to A4W4 p=0.2712692990899086
perturb layer layer1.0.conv1 to A4W8 and layer layer1.5.conv1 to A4W8 p=0.23054584115743637
perturb layer layer1.0.conv1 to A4W8 and layer layer1.5.conv1 to A8W2 p=0.20437398552894592
perturb layer layer1.0.conv1 to A4W8 and layer layer1.5.conv1 to A8W4 p=0.25434982776641846
perturb layer layer1.0.conv1 to A4W8 and layer layer1.5.conv1 to A8W8 p=0.2125949114561081
perturb layer layer1.0.conv1 to A8W2 and layer layer1.5.conv1 to A2W2 p=1.6014645546674728
perturb layer layer1.0.conv1 to A8W2 and layer layer1.5.conv1 to A2W4 p=1.5205651074647903
perturb layer layer1.0.conv1 to A8W2 and layer layer1.5.conv1 to A2W8 p=1.4852824807167053
perturb layer layer1.0.conv1 to A8W2 and layer layer1.5.conv1 to A4W2 p=0.8607934713363647
perturb layer layer1.0.conv1 to A8W2 and layer layer1.5.conv1 to A4W4 p=0.8109157085418701
perturb layer layer1.0.conv1 to A8W2 and layer layer1.5.conv1 to A4W8 p=0.7523273825645447
perturb layer layer1.0.conv1 to A8W2 and layer layer1.5.conv1 to A8W2 p=0.8472422808408737
perturb layer layer1.0.conv1 to A8W2 and layer layer1.5.conv1 to A8W4 p=0.7921691387891769
perturb layer layer1.0.conv1 to A8W2 and layer layer1.5.conv1 to A8W8 p=0.7359778583049774
perturb layer layer1.0.conv1 to A8W4 and layer layer1.5.conv1 to A2W2 p=0.6345772445201874
perturb layer layer1.0.conv1 to A8W4 and layer layer1.5.conv1 to A2W4 p=0.6321735382080078
perturb layer layer1.0.conv1 to A8W4 and layer layer1.5.conv1 to A2W8 p=0.6166912615299225
perturb layer layer1.0.conv1 to A8W4 and layer layer1.5.conv1 to A4W2 p=0.021540410816669464
perturb layer layer1.0.conv1 to A8W4 and layer layer1.5.conv1 to A4W4 p=0.03294917196035385
perturb layer layer1.0.conv1 to A8W4 and layer layer1.5.conv1 to A4W8 p=0.005680002272129059
perturb layer layer1.0.conv1 to A8W4 and layer layer1.5.conv1 to A8W2 p=0.008690871298313141
perturb layer layer1.0.conv1 to A8W4 and layer layer1.5.conv1 to A8W4 p=0.015156082808971405
perturb layer layer1.0.conv1 to A8W4 and layer layer1.5.conv1 to A8W8 p=-0.011496715247631073
perturb layer layer1.0.conv1 to A8W8 and layer layer1.5.conv1 to A2W2 p=0.6297261416912079
perturb layer layer1.0.conv1 to A8W8 and layer layer1.5.conv1 to A2W4 p=0.6406124830245972
perturb layer layer1.0.conv1 to A8W8 and layer layer1.5.conv1 to A2W8 p=0.618961438536644
perturb layer layer1.0.conv1 to A8W8 and layer layer1.5.conv1 to A4W2 p=0.025310605764389038
perturb layer layer1.0.conv1 to A8W8 and layer layer1.5.conv1 to A4W4 p=0.04916691035032272
perturb layer layer1.0.conv1 to A8W8 and layer layer1.5.conv1 to A4W8 p=0.015425756573677063
perturb layer layer1.0.conv1 to A8W8 and layer layer1.5.conv1 to A8W2 p=0.009833723306655884
perturb layer layer1.0.conv1 to A8W8 and layer layer1.5.conv1 to A8W4 p=0.03496430069208145
perturb layer layer1.0.conv1 to A8W8 and layer layer1.5.conv1 to A8W8 p=0.003281906247138977
perturb layer layer1.0.conv1 to A2W2 and layer layer1.5.conv2 to A2W2 p=2.058878317475319
perturb layer layer1.0.conv1 to A2W2 and layer layer1.5.conv2 to A2W4 p=2.0294341892004013
perturb layer layer1.0.conv1 to A2W2 and layer layer1.5.conv2 to A2W8 p=2.1119513660669327
perturb layer layer1.0.conv1 to A2W2 and layer layer1.5.conv2 to A4W2 p=2.0374169796705246
perturb layer layer1.0.conv1 to A2W2 and layer layer1.5.conv2 to A4W4 p=1.9888890236616135
perturb layer layer1.0.conv1 to A2W2 and layer layer1.5.conv2 to A4W8 p=2.07223017513752
perturb layer layer1.0.conv1 to A2W2 and layer layer1.5.conv2 to A8W2 p=2.035536751151085
perturb layer layer1.0.conv1 to A2W2 and layer layer1.5.conv2 to A8W4 p=1.9844045490026474
perturb layer layer1.0.conv1 to A2W2 and layer layer1.5.conv2 to A8W8 p=2.0678083449602127
perturb layer layer1.0.conv1 to A2W4 and layer layer1.5.conv2 to A2W2 p=1.0884030759334564
perturb layer layer1.0.conv1 to A2W4 and layer layer1.5.conv2 to A2W4 p=1.229919359087944
perturb layer layer1.0.conv1 to A2W4 and layer layer1.5.conv2 to A2W8 p=1.289270669221878
perturb layer layer1.0.conv1 to A2W4 and layer layer1.5.conv2 to A4W2 p=1.065354585647583
perturb layer layer1.0.conv1 to A2W4 and layer layer1.5.conv2 to A4W4 p=1.2011436522006989
perturb layer layer1.0.conv1 to A2W4 and layer layer1.5.conv2 to A4W8 p=1.2593261450529099
perturb layer layer1.0.conv1 to A2W4 and layer layer1.5.conv2 to A8W2 p=1.0617251843214035
perturb layer layer1.0.conv1 to A2W4 and layer layer1.5.conv2 to A8W4 p=1.195012852549553
perturb layer layer1.0.conv1 to A2W4 and layer layer1.5.conv2 to A8W8 p=1.2534981667995453
perturb layer layer1.0.conv1 to A2W8 and layer layer1.5.conv2 to A2W2 p=1.0274901539087296
perturb layer layer1.0.conv1 to A2W8 and layer layer1.5.conv2 to A2W4 p=1.1961309760808945
perturb layer layer1.0.conv1 to A2W8 and layer layer1.5.conv2 to A2W8 p=1.2553068697452545
perturb layer layer1.0.conv1 to A2W8 and layer layer1.5.conv2 to A4W2 p=1.008038118481636
perturb layer layer1.0.conv1 to A2W8 and layer layer1.5.conv2 to A4W4 p=1.1706508547067642
perturb layer layer1.0.conv1 to A2W8 and layer layer1.5.conv2 to A4W8 p=1.2275485545396805
perturb layer layer1.0.conv1 to A2W8 and layer layer1.5.conv2 to A8W2 p=1.0075026750564575
perturb layer layer1.0.conv1 to A2W8 and layer layer1.5.conv2 to A8W4 p=1.1695054173469543
perturb layer layer1.0.conv1 to A2W8 and layer layer1.5.conv2 to A8W8 p=1.226439818739891
perturb layer layer1.0.conv1 to A4W2 and layer layer1.5.conv2 to A2W2 p=0.9940213710069656
perturb layer layer1.0.conv1 to A4W2 and layer layer1.5.conv2 to A2W4 p=0.9203235059976578
perturb layer layer1.0.conv1 to A4W2 and layer layer1.5.conv2 to A2W8 p=0.968375101685524
perturb layer layer1.0.conv1 to A4W2 and layer layer1.5.conv2 to A4W2 p=0.994151771068573
perturb layer layer1.0.conv1 to A4W2 and layer layer1.5.conv2 to A4W4 p=0.9133957475423813
perturb layer layer1.0.conv1 to A4W2 and layer layer1.5.conv2 to A4W8 p=0.9623504728078842
perturb layer layer1.0.conv1 to A4W2 and layer layer1.5.conv2 to A8W2 p=0.9910342395305634
perturb layer layer1.0.conv1 to A4W2 and layer layer1.5.conv2 to A8W4 p=0.9081249833106995
perturb layer layer1.0.conv1 to A4W2 and layer layer1.5.conv2 to A8W8 p=0.955841451883316
perturb layer layer1.0.conv1 to A4W4 and layer layer1.5.conv2 to A2W2 p=0.1363758146762848
perturb layer layer1.0.conv1 to A4W4 and layer layer1.5.conv2 to A2W4 p=0.17798027396202087
perturb layer layer1.0.conv1 to A4W4 and layer layer1.5.conv2 to A2W8 p=0.207976333796978
perturb layer layer1.0.conv1 to A4W4 and layer layer1.5.conv2 to A4W2 p=0.13237757980823517
perturb layer layer1.0.conv1 to A4W4 and layer layer1.5.conv2 to A4W4 p=0.17024914175271988
perturb layer layer1.0.conv1 to A4W4 and layer layer1.5.conv2 to A4W8 p=0.2008206620812416
perturb layer layer1.0.conv1 to A4W4 and layer layer1.5.conv2 to A8W2 p=0.13133696466684341
perturb layer layer1.0.conv1 to A4W4 and layer layer1.5.conv2 to A8W4 p=0.16762127727270126
perturb layer layer1.0.conv1 to A4W4 and layer layer1.5.conv2 to A8W8 p=0.19824770838022232
perturb layer layer1.0.conv1 to A4W8 and layer layer1.5.conv2 to A2W2 p=0.1162404716014862
perturb layer layer1.0.conv1 to A4W8 and layer layer1.5.conv2 to A2W4 p=0.18328911066055298
perturb layer layer1.0.conv1 to A4W8 and layer layer1.5.conv2 to A2W8 p=0.21471886336803436
perturb layer layer1.0.conv1 to A4W8 and layer layer1.5.conv2 to A4W2 p=0.11202595382928848
perturb layer layer1.0.conv1 to A4W8 and layer layer1.5.conv2 to A4W4 p=0.18016429245471954
perturb layer layer1.0.conv1 to A4W8 and layer layer1.5.conv2 to A4W8 p=0.21375449001789093
perturb layer layer1.0.conv1 to A4W8 and layer layer1.5.conv2 to A8W2 p=0.11115317791700363
perturb layer layer1.0.conv1 to A4W8 and layer layer1.5.conv2 to A8W4 p=0.17896947264671326
perturb layer layer1.0.conv1 to A4W8 and layer layer1.5.conv2 to A8W8 p=0.21251516789197922
perturb layer layer1.0.conv1 to A8W2 and layer layer1.5.conv2 to A2W2 p=0.817193403840065
perturb layer layer1.0.conv1 to A8W2 and layer layer1.5.conv2 to A2W4 p=0.7151435911655426
perturb layer layer1.0.conv1 to A8W2 and layer layer1.5.conv2 to A2W8 p=0.7539169043302536
perturb layer layer1.0.conv1 to A8W2 and layer layer1.5.conv2 to A4W2 p=0.8113637119531631
perturb layer layer1.0.conv1 to A8W2 and layer layer1.5.conv2 to A4W4 p=0.6967878639698029
perturb layer layer1.0.conv1 to A8W2 and layer layer1.5.conv2 to A4W8 p=0.7352916747331619
perturb layer layer1.0.conv1 to A8W2 and layer layer1.5.conv2 to A8W2 p=0.8130120933055878
perturb layer layer1.0.conv1 to A8W2 and layer layer1.5.conv2 to A8W4 p=0.6978040635585785
perturb layer layer1.0.conv1 to A8W2 and layer layer1.5.conv2 to A8W8 p=0.7373886406421661
perturb layer layer1.0.conv1 to A8W4 and layer layer1.5.conv2 to A2W2 p=-0.04276452586054802
perturb layer layer1.0.conv1 to A8W4 and layer layer1.5.conv2 to A2W4 p=-0.031851910054683685
perturb layer layer1.0.conv1 to A8W4 and layer layer1.5.conv2 to A2W8 p=-0.008577495813369751
perturb layer layer1.0.conv1 to A8W4 and layer layer1.5.conv2 to A4W2 p=-0.04714358225464821
perturb layer layer1.0.conv1 to A8W4 and layer layer1.5.conv2 to A4W4 p=-0.03762675076723099
perturb layer layer1.0.conv1 to A8W4 and layer layer1.5.conv2 to A4W8 p=-0.013246625661849976
perturb layer layer1.0.conv1 to A8W4 and layer layer1.5.conv2 to A8W2 p=-0.04609545320272446
perturb layer layer1.0.conv1 to A8W4 and layer layer1.5.conv2 to A8W4 p=-0.037093065679073334
perturb layer layer1.0.conv1 to A8W4 and layer layer1.5.conv2 to A8W8 p=-0.01255796104669571
perturb layer layer1.0.conv1 to A8W8 and layer layer1.5.conv2 to A2W2 p=-0.060126177966594696
perturb layer layer1.0.conv1 to A8W8 and layer layer1.5.conv2 to A2W4 p=-0.028630010783672333
perturb layer layer1.0.conv1 to A8W8 and layer layer1.5.conv2 to A2W8 p=-0.0014669448137283325
perturb layer layer1.0.conv1 to A8W8 and layer layer1.5.conv2 to A4W2 p=-0.060143109411001205
perturb layer layer1.0.conv1 to A8W8 and layer layer1.5.conv2 to A4W4 p=-0.02634299546480179
perturb layer layer1.0.conv1 to A8W8 and layer layer1.5.conv2 to A4W8 p=0.003201231360435486
perturb layer layer1.0.conv1 to A8W8 and layer layer1.5.conv2 to A8W2 p=-0.06080988049507141
perturb layer layer1.0.conv1 to A8W8 and layer layer1.5.conv2 to A8W4 p=-0.026442445814609528
perturb layer layer1.0.conv1 to A8W8 and layer layer1.5.conv2 to A8W8 p=0.0033687055110931396
perturb layer layer1.0.conv1 to A2W2 and layer layer1.6.conv1 to A2W2 p=3.1816265136003494
perturb layer layer1.0.conv1 to A2W2 and layer layer1.6.conv1 to A2W4 p=2.958331361413002
perturb layer layer1.0.conv1 to A2W2 and layer layer1.6.conv1 to A2W8 p=2.9148112684488297
perturb layer layer1.0.conv1 to A2W2 and layer layer1.6.conv1 to A4W2 p=2.277474418282509
perturb layer layer1.0.conv1 to A2W2 and layer layer1.6.conv1 to A4W4 p=2.0886839479207993
perturb layer layer1.0.conv1 to A2W2 and layer layer1.6.conv1 to A4W8 p=2.0868849009275436
perturb layer layer1.0.conv1 to A2W2 and layer layer1.6.conv1 to A8W2 p=2.2483281046152115
perturb layer layer1.0.conv1 to A2W2 and layer layer1.6.conv1 to A8W4 p=2.065054401755333
perturb layer layer1.0.conv1 to A2W2 and layer layer1.6.conv1 to A8W8 p=2.067638114094734
perturb layer layer1.0.conv1 to A2W4 and layer layer1.6.conv1 to A2W2 p=2.333073601126671
perturb layer layer1.0.conv1 to A2W4 and layer layer1.6.conv1 to A2W4 p=2.1751584261655807
perturb layer layer1.0.conv1 to A2W4 and layer layer1.6.conv1 to A2W8 p=2.142777666449547
perturb layer layer1.0.conv1 to A2W4 and layer layer1.6.conv1 to A4W2 p=1.405919387936592
perturb layer layer1.0.conv1 to A2W4 and layer layer1.6.conv1 to A4W4 p=1.286771535873413
perturb layer layer1.0.conv1 to A2W4 and layer layer1.6.conv1 to A4W8 p=1.2826904505491257
perturb layer layer1.0.conv1 to A2W4 and layer layer1.6.conv1 to A8W2 p=1.3656056076288223
perturb layer layer1.0.conv1 to A2W4 and layer layer1.6.conv1 to A8W4 p=1.255319356918335
perturb layer layer1.0.conv1 to A2W4 and layer layer1.6.conv1 to A8W8 p=1.2533337771892548
perturb layer layer1.0.conv1 to A2W8 and layer layer1.6.conv1 to A2W2 p=2.243038550019264
perturb layer layer1.0.conv1 to A2W8 and layer layer1.6.conv1 to A2W4 p=2.060171589255333
perturb layer layer1.0.conv1 to A2W8 and layer layer1.6.conv1 to A2W8 p=2.03428815305233
perturb layer layer1.0.conv1 to A2W8 and layer layer1.6.conv1 to A4W2 p=1.3348590582609177
perturb layer layer1.0.conv1 to A2W8 and layer layer1.6.conv1 to A4W4 p=1.2338939756155014
perturb layer layer1.0.conv1 to A2W8 and layer layer1.6.conv1 to A4W8 p=1.2350006103515625
perturb layer layer1.0.conv1 to A2W8 and layer layer1.6.conv1 to A8W2 p=1.3106702715158463
perturb layer layer1.0.conv1 to A2W8 and layer layer1.6.conv1 to A8W4 p=1.219760149717331
perturb layer layer1.0.conv1 to A2W8 and layer layer1.6.conv1 to A8W8 p=1.2263168692588806
perturb layer layer1.0.conv1 to A4W2 and layer layer1.6.conv1 to A2W2 p=1.745914801955223
perturb layer layer1.0.conv1 to A4W2 and layer layer1.6.conv1 to A2W4 p=1.5492528825998306
perturb layer layer1.0.conv1 to A4W2 and layer layer1.6.conv1 to A2W8 p=1.5183784812688828
perturb layer layer1.0.conv1 to A4W2 and layer layer1.6.conv1 to A4W2 p=1.1009392738342285
perturb layer layer1.0.conv1 to A4W2 and layer layer1.6.conv1 to A4W4 p=0.9706779718399048
perturb layer layer1.0.conv1 to A4W2 and layer layer1.6.conv1 to A4W8 p=0.9715866446495056
perturb layer layer1.0.conv1 to A4W2 and layer layer1.6.conv1 to A8W2 p=1.0878315418958664
perturb layer layer1.0.conv1 to A4W2 and layer layer1.6.conv1 to A8W4 p=0.9541416615247726
perturb layer layer1.0.conv1 to A4W2 and layer layer1.6.conv1 to A8W8 p=0.9550248384475708
perturb layer layer1.0.conv1 to A4W4 and layer layer1.6.conv1 to A2W2 p=0.9934644103050232
perturb layer layer1.0.conv1 to A4W4 and layer layer1.6.conv1 to A2W4 p=0.8120643794536591
perturb layer layer1.0.conv1 to A4W4 and layer layer1.6.conv1 to A2W8 p=0.791032075881958
perturb layer layer1.0.conv1 to A4W4 and layer layer1.6.conv1 to A4W2 p=0.3064187690615654
perturb layer layer1.0.conv1 to A4W4 and layer layer1.6.conv1 to A4W4 p=0.2003365308046341
perturb layer layer1.0.conv1 to A4W4 and layer layer1.6.conv1 to A4W8 p=0.20440666377544403
perturb layer layer1.0.conv1 to A4W4 and layer layer1.6.conv1 to A8W2 p=0.2997085750102997
perturb layer layer1.0.conv1 to A4W4 and layer layer1.6.conv1 to A8W4 p=0.19301601499319077
perturb layer layer1.0.conv1 to A4W4 and layer layer1.6.conv1 to A8W8 p=0.19871465861797333
perturb layer layer1.0.conv1 to A4W8 and layer layer1.6.conv1 to A2W2 p=0.999230831861496
perturb layer layer1.0.conv1 to A4W8 and layer layer1.6.conv1 to A2W4 p=0.7885634154081345
perturb layer layer1.0.conv1 to A4W8 and layer layer1.6.conv1 to A2W8 p=0.7766842097043991
perturb layer layer1.0.conv1 to A4W8 and layer layer1.6.conv1 to A4W2 p=0.3079193979501724
perturb layer layer1.0.conv1 to A4W8 and layer layer1.6.conv1 to A4W4 p=0.20688608288764954
perturb layer layer1.0.conv1 to A4W8 and layer layer1.6.conv1 to A4W8 p=0.21440277248620987
perturb layer layer1.0.conv1 to A4W8 and layer layer1.6.conv1 to A8W2 p=0.30216851085424423
perturb layer layer1.0.conv1 to A4W8 and layer layer1.6.conv1 to A8W4 p=0.20154740661382675
perturb layer layer1.0.conv1 to A4W8 and layer layer1.6.conv1 to A8W8 p=0.21129941940307617
perturb layer layer1.0.conv1 to A8W2 and layer layer1.6.conv1 to A2W2 p=1.4202067703008652
perturb layer layer1.0.conv1 to A8W2 and layer layer1.6.conv1 to A2W4 p=1.2683316320180893
perturb layer layer1.0.conv1 to A8W2 and layer layer1.6.conv1 to A2W8 p=1.2357601821422577
perturb layer layer1.0.conv1 to A8W2 and layer layer1.6.conv1 to A4W2 p=0.8245956003665924
perturb layer layer1.0.conv1 to A8W2 and layer layer1.6.conv1 to A4W4 p=0.7268186956644058
perturb layer layer1.0.conv1 to A8W2 and layer layer1.6.conv1 to A4W8 p=0.7285639941692352
perturb layer layer1.0.conv1 to A8W2 and layer layer1.6.conv1 to A8W2 p=0.8227752596139908
perturb layer layer1.0.conv1 to A8W2 and layer layer1.6.conv1 to A8W4 p=0.7330686151981354
perturb layer layer1.0.conv1 to A8W2 and layer layer1.6.conv1 to A8W8 p=0.7376776039600372
perturb layer layer1.0.conv1 to A8W4 and layer layer1.6.conv1 to A2W2 p=0.6557785421609879
perturb layer layer1.0.conv1 to A8W4 and layer layer1.6.conv1 to A2W4 p=0.5056454241275787
perturb layer layer1.0.conv1 to A8W4 and layer layer1.6.conv1 to A2W8 p=0.4850282371044159
perturb layer layer1.0.conv1 to A8W4 and layer layer1.6.conv1 to A4W2 p=0.048755280673503876
perturb layer layer1.0.conv1 to A8W4 and layer layer1.6.conv1 to A4W4 p=-0.014541856944561005
perturb layer layer1.0.conv1 to A8W4 and layer layer1.6.conv1 to A4W8 p=-0.010953545570373535
perturb layer layer1.0.conv1 to A8W4 and layer layer1.6.conv1 to A8W2 p=0.040908291935920715
perturb layer layer1.0.conv1 to A8W4 and layer layer1.6.conv1 to A8W4 p=-0.016971930861473083
perturb layer layer1.0.conv1 to A8W4 and layer layer1.6.conv1 to A8W8 p=-0.013045050203800201
perturb layer layer1.0.conv1 to A8W8 and layer layer1.6.conv1 to A2W2 p=0.5988315045833588
perturb layer layer1.0.conv1 to A8W8 and layer layer1.6.conv1 to A2W4 p=0.44475263357162476
perturb layer layer1.0.conv1 to A8W8 and layer layer1.6.conv1 to A2W8 p=0.4329918622970581
perturb layer layer1.0.conv1 to A8W8 and layer layer1.6.conv1 to A4W2 p=0.037309497594833374
perturb layer layer1.0.conv1 to A8W8 and layer layer1.6.conv1 to A4W4 p=-0.014612823724746704
perturb layer layer1.0.conv1 to A8W8 and layer layer1.6.conv1 to A4W8 p=-0.007358945906162262
perturb layer layer1.0.conv1 to A8W8 and layer layer1.6.conv1 to A8W2 p=0.03775106370449066
perturb layer layer1.0.conv1 to A8W8 and layer layer1.6.conv1 to A8W4 p=-0.006526395678520203
perturb layer layer1.0.conv1 to A8W8 and layer layer1.6.conv1 to A8W8 p=0.0034420564770698547
perturb layer layer1.0.conv1 to A2W2 and layer layer1.6.conv2 to A2W2 p=2.260136142373085
perturb layer layer1.0.conv1 to A2W2 and layer layer1.6.conv2 to A2W4 p=2.088414564728737
perturb layer layer1.0.conv1 to A2W2 and layer layer1.6.conv2 to A2W8 p=2.0766420513391495
perturb layer layer1.0.conv1 to A2W2 and layer layer1.6.conv2 to A4W2 p=2.247632548213005
perturb layer layer1.0.conv1 to A2W2 and layer layer1.6.conv2 to A4W4 p=2.077351376414299
perturb layer layer1.0.conv1 to A2W2 and layer layer1.6.conv2 to A4W8 p=2.0662959665060043
perturb layer layer1.0.conv1 to A2W2 and layer layer1.6.conv2 to A8W2 p=2.247516855597496
perturb layer layer1.0.conv1 to A2W2 and layer layer1.6.conv2 to A8W4 p=2.0781316310167313
perturb layer layer1.0.conv1 to A2W2 and layer layer1.6.conv2 to A8W8 p=2.067899838089943
perturb layer layer1.0.conv1 to A2W4 and layer layer1.6.conv2 to A2W2 p=1.2658069878816605
perturb layer layer1.0.conv1 to A2W4 and layer layer1.6.conv2 to A2W4 p=1.259909376502037
perturb layer layer1.0.conv1 to A2W4 and layer layer1.6.conv2 to A2W8 p=1.2616154253482819
perturb layer layer1.0.conv1 to A2W4 and layer layer1.6.conv2 to A4W2 p=1.2538885474205017
perturb layer layer1.0.conv1 to A2W4 and layer layer1.6.conv2 to A4W4 p=1.249384343624115
perturb layer layer1.0.conv1 to A2W4 and layer layer1.6.conv2 to A4W8 p=1.2526465952396393
perturb layer layer1.0.conv1 to A2W4 and layer layer1.6.conv2 to A8W2 p=1.2536950409412384
perturb layer layer1.0.conv1 to A2W4 and layer layer1.6.conv2 to A8W4 p=1.249434381723404
perturb layer layer1.0.conv1 to A2W4 and layer layer1.6.conv2 to A8W8 p=1.2531677037477493
perturb layer layer1.0.conv1 to A2W8 and layer layer1.6.conv2 to A2W2 p=1.2389808744192123
perturb layer layer1.0.conv1 to A2W8 and layer layer1.6.conv2 to A2W4 p=1.2354558557271957
perturb layer layer1.0.conv1 to A2W8 and layer layer1.6.conv2 to A2W8 p=1.237623170018196
perturb layer layer1.0.conv1 to A2W8 and layer layer1.6.conv2 to A4W2 p=1.2210678309202194
perturb layer layer1.0.conv1 to A2W8 and layer layer1.6.conv2 to A4W4 p=1.2232081145048141
perturb layer layer1.0.conv1 to A2W8 and layer layer1.6.conv2 to A4W8 p=1.2249772995710373
perturb layer layer1.0.conv1 to A2W8 and layer layer1.6.conv2 to A8W2 p=1.2227286845445633
perturb layer layer1.0.conv1 to A2W8 and layer layer1.6.conv2 to A8W4 p=1.2243734449148178
perturb layer layer1.0.conv1 to A2W8 and layer layer1.6.conv2 to A8W8 p=1.2261979430913925
perturb layer layer1.0.conv1 to A4W2 and layer layer1.6.conv2 to A2W2 p=1.091314435005188
perturb layer layer1.0.conv1 to A4W2 and layer layer1.6.conv2 to A2W4 p=0.9668923169374466
perturb layer layer1.0.conv1 to A4W2 and layer layer1.6.conv2 to A2W8 p=0.963738203048706
perturb layer layer1.0.conv1 to A4W2 and layer layer1.6.conv2 to A4W2 p=1.0804110914468765
perturb layer layer1.0.conv1 to A4W2 and layer layer1.6.conv2 to A4W4 p=0.9564235955476761
perturb layer layer1.0.conv1 to A4W2 and layer layer1.6.conv2 to A4W8 p=0.9541344195604324
perturb layer layer1.0.conv1 to A4W2 and layer layer1.6.conv2 to A8W2 p=1.0824381113052368
perturb layer layer1.0.conv1 to A4W2 and layer layer1.6.conv2 to A8W4 p=0.9583120793104172
perturb layer layer1.0.conv1 to A4W2 and layer layer1.6.conv2 to A8W8 p=0.9557297974824905
perturb layer layer1.0.conv1 to A4W4 and layer layer1.6.conv2 to A2W2 p=0.26487914472818375
perturb layer layer1.0.conv1 to A4W4 and layer layer1.6.conv2 to A2W4 p=0.2056218385696411
perturb layer layer1.0.conv1 to A4W4 and layer layer1.6.conv2 to A2W8 p=0.20494237542152405
perturb layer layer1.0.conv1 to A4W4 and layer layer1.6.conv2 to A4W2 p=0.2568288668990135
perturb layer layer1.0.conv1 to A4W4 and layer layer1.6.conv2 to A4W4 p=0.19831609725952148
perturb layer layer1.0.conv1 to A4W4 and layer layer1.6.conv2 to A4W8 p=0.19888953119516373
perturb layer layer1.0.conv1 to A4W4 and layer layer1.6.conv2 to A8W2 p=0.25555218756198883
perturb layer layer1.0.conv1 to A4W4 and layer layer1.6.conv2 to A8W4 p=0.19752438366413116
perturb layer layer1.0.conv1 to A4W4 and layer layer1.6.conv2 to A8W8 p=0.19816988706588745
perturb layer layer1.0.conv1 to A4W8 and layer layer1.6.conv2 to A2W2 p=0.2686101868748665
perturb layer layer1.0.conv1 to A4W8 and layer layer1.6.conv2 to A2W4 p=0.2172711044549942
perturb layer layer1.0.conv1 to A4W8 and layer layer1.6.conv2 to A2W8 p=0.21710509061813354
perturb layer layer1.0.conv1 to A4W8 and layer layer1.6.conv2 to A4W2 p=0.2613753527402878
perturb layer layer1.0.conv1 to A4W8 and layer layer1.6.conv2 to A4W4 p=0.21202197670936584
perturb layer layer1.0.conv1 to A4W8 and layer layer1.6.conv2 to A4W8 p=0.212498739361763
perturb layer layer1.0.conv1 to A4W8 and layer layer1.6.conv2 to A8W2 p=0.2605733647942543
perturb layer layer1.0.conv1 to A4W8 and layer layer1.6.conv2 to A8W4 p=0.2112293839454651
perturb layer layer1.0.conv1 to A4W8 and layer layer1.6.conv2 to A8W8 p=0.21214131265878677
perturb layer layer1.0.conv1 to A8W2 and layer layer1.6.conv2 to A2W2 p=0.8578173220157623
perturb layer layer1.0.conv1 to A8W2 and layer layer1.6.conv2 to A2W4 p=0.7484820038080215
perturb layer layer1.0.conv1 to A8W2 and layer layer1.6.conv2 to A2W8 p=0.7452119439840317
perturb layer layer1.0.conv1 to A8W2 and layer layer1.6.conv2 to A4W2 p=0.8494881689548492
perturb layer layer1.0.conv1 to A8W2 and layer layer1.6.conv2 to A4W4 p=0.7398693114519119
perturb layer layer1.0.conv1 to A8W2 and layer layer1.6.conv2 to A4W8 p=0.7377432584762573
perturb layer layer1.0.conv1 to A8W2 and layer layer1.6.conv2 to A8W2 p=0.8501798361539841
perturb layer layer1.0.conv1 to A8W2 and layer layer1.6.conv2 to A8W4 p=0.7394630014896393
perturb layer layer1.0.conv1 to A8W2 and layer layer1.6.conv2 to A8W8 p=0.737624928355217
perturb layer layer1.0.conv1 to A8W4 and layer layer1.6.conv2 to A2W2 p=0.019725695252418518
perturb layer layer1.0.conv1 to A8W4 and layer layer1.6.conv2 to A2W4 p=-0.011059597134590149
perturb layer layer1.0.conv1 to A8W4 and layer layer1.6.conv2 to A2W8 p=-0.00908748060464859
perturb layer layer1.0.conv1 to A8W4 and layer layer1.6.conv2 to A4W2 p=0.013066060841083527
perturb layer layer1.0.conv1 to A8W4 and layer layer1.6.conv2 to A4W4 p=-0.017053820192813873
perturb layer layer1.0.conv1 to A8W4 and layer layer1.6.conv2 to A4W8 p=-0.01458374410867691
perturb layer layer1.0.conv1 to A8W4 and layer layer1.6.conv2 to A8W2 p=0.01375986635684967
perturb layer layer1.0.conv1 to A8W4 and layer layer1.6.conv2 to A8W4 p=-0.015170976519584656
perturb layer layer1.0.conv1 to A8W4 and layer layer1.6.conv2 to A8W8 p=-0.01246960461139679
perturb layer layer1.0.conv1 to A8W8 and layer layer1.6.conv2 to A2W2 p=0.02769222855567932
perturb layer layer1.0.conv1 to A8W8 and layer layer1.6.conv2 to A2W4 p=0.002894394099712372
perturb layer layer1.0.conv1 to A8W8 and layer layer1.6.conv2 to A2W8 p=0.005952261388301849
perturb layer layer1.0.conv1 to A8W8 and layer layer1.6.conv2 to A4W2 p=0.01877458393573761
perturb layer layer1.0.conv1 to A8W8 and layer layer1.6.conv2 to A4W4 p=-0.001890026032924652
perturb layer layer1.0.conv1 to A8W8 and layer layer1.6.conv2 to A4W8 p=0.0026365667581558228
perturb layer layer1.0.conv1 to A8W8 and layer layer1.6.conv2 to A8W2 p=0.017663680016994476
perturb layer layer1.0.conv1 to A8W8 and layer layer1.6.conv2 to A8W4 p=-0.0014780014753341675
perturb layer layer1.0.conv1 to A8W8 and layer layer1.6.conv2 to A8W8 p=0.0032491907477378845
perturb layer layer1.0.conv1 to A2W2 and layer layer1.7.conv1 to A2W2 p=2.886746898293495
perturb layer layer1.0.conv1 to A2W2 and layer layer1.7.conv1 to A2W4 p=2.746970519423485
perturb layer layer1.0.conv1 to A2W2 and layer layer1.7.conv1 to A2W8 p=2.799645647406578
perturb layer layer1.0.conv1 to A2W2 and layer layer1.7.conv1 to A4W2 p=2.2407229095697403
perturb layer layer1.0.conv1 to A2W2 and layer layer1.7.conv1 to A4W4 p=2.0466033071279526
perturb layer layer1.0.conv1 to A2W2 and layer layer1.7.conv1 to A4W8 p=2.0884036868810654
perturb layer layer1.0.conv1 to A2W2 and layer layer1.7.conv1 to A8W2 p=2.226940795779228
perturb layer layer1.0.conv1 to A2W2 and layer layer1.7.conv1 to A8W4 p=2.0258541852235794
perturb layer layer1.0.conv1 to A2W2 and layer layer1.7.conv1 to A8W8 p=2.070465549826622
perturb layer layer1.0.conv1 to A2W4 and layer layer1.7.conv1 to A2W2 p=2.167223170399666
perturb layer layer1.0.conv1 to A2W4 and layer layer1.7.conv1 to A2W4 p=2.1000010520219803
perturb layer layer1.0.conv1 to A2W4 and layer layer1.7.conv1 to A2W8 p=2.130918338894844
perturb layer layer1.0.conv1 to A2W4 and layer layer1.7.conv1 to A4W2 p=1.3124034106731415
perturb layer layer1.0.conv1 to A2W4 and layer layer1.7.conv1 to A4W4 p=1.2587535083293915
perturb layer layer1.0.conv1 to A2W4 and layer layer1.7.conv1 to A4W8 p=1.288974329829216
perturb layer layer1.0.conv1 to A2W4 and layer layer1.7.conv1 to A8W2 p=1.285718858242035
perturb layer layer1.0.conv1 to A2W4 and layer layer1.7.conv1 to A8W4 p=1.222289353609085
perturb layer layer1.0.conv1 to A2W4 and layer layer1.7.conv1 to A8W8 p=1.252465695142746
perturb layer layer1.0.conv1 to A2W8 and layer layer1.7.conv1 to A2W2 p=2.0670137256383896
perturb layer layer1.0.conv1 to A2W8 and layer layer1.7.conv1 to A2W4 p=2.017303928732872
perturb layer layer1.0.conv1 to A2W8 and layer layer1.7.conv1 to A2W8 p=2.0470812171697617
perturb layer layer1.0.conv1 to A2W8 and layer layer1.7.conv1 to A4W2 p=1.2814024835824966
perturb layer layer1.0.conv1 to A2W8 and layer layer1.7.conv1 to A4W4 p=1.2267947047948837
perturb layer layer1.0.conv1 to A2W8 and layer layer1.7.conv1 to A4W8 p=1.257433921098709
perturb layer layer1.0.conv1 to A2W8 and layer layer1.7.conv1 to A8W2 p=1.252832904458046
perturb layer layer1.0.conv1 to A2W8 and layer layer1.7.conv1 to A8W4 p=1.1952079832553864
perturb layer layer1.0.conv1 to A2W8 and layer layer1.7.conv1 to A8W8 p=1.2255131155252457
perturb layer layer1.0.conv1 to A4W2 and layer layer1.7.conv1 to A2W2 p=1.5958401411771774
perturb layer layer1.0.conv1 to A4W2 and layer layer1.7.conv1 to A2W4 p=1.4259081929922104
perturb layer layer1.0.conv1 to A4W2 and layer layer1.7.conv1 to A2W8 p=1.4423483461141586
perturb layer layer1.0.conv1 to A4W2 and layer layer1.7.conv1 to A4W2 p=1.108498066663742
perturb layer layer1.0.conv1 to A4W2 and layer layer1.7.conv1 to A4W4 p=0.9238881170749664
perturb layer layer1.0.conv1 to A4W2 and layer layer1.7.conv1 to A4W8 p=0.9443011432886124
perturb layer layer1.0.conv1 to A4W2 and layer layer1.7.conv1 to A8W2 p=1.1190527379512787
perturb layer layer1.0.conv1 to A4W2 and layer layer1.7.conv1 to A8W4 p=0.9372653961181641
perturb layer layer1.0.conv1 to A4W2 and layer layer1.7.conv1 to A8W8 p=0.9551794230937958
perturb layer layer1.0.conv1 to A4W4 and layer layer1.7.conv1 to A2W2 p=0.7962623685598373
perturb layer layer1.0.conv1 to A4W4 and layer layer1.7.conv1 to A2W4 p=0.7577788382768631
perturb layer layer1.0.conv1 to A4W4 and layer layer1.7.conv1 to A2W8 p=0.7799953669309616
perturb layer layer1.0.conv1 to A4W4 and layer layer1.7.conv1 to A4W2 p=0.1978830322623253
perturb layer layer1.0.conv1 to A4W4 and layer layer1.7.conv1 to A4W4 p=0.1807251274585724
perturb layer layer1.0.conv1 to A4W4 and layer layer1.7.conv1 to A4W8 p=0.2009412720799446
perturb layer layer1.0.conv1 to A4W4 and layer layer1.7.conv1 to A8W2 p=0.19167723506689072
perturb layer layer1.0.conv1 to A4W4 and layer layer1.7.conv1 to A8W4 p=0.1769249439239502
perturb layer layer1.0.conv1 to A4W4 and layer layer1.7.conv1 to A8W8 p=0.19805069267749786
perturb layer layer1.0.conv1 to A4W8 and layer layer1.7.conv1 to A2W2 p=0.7556804120540619
perturb layer layer1.0.conv1 to A4W8 and layer layer1.7.conv1 to A2W4 p=0.7297663688659668
perturb layer layer1.0.conv1 to A4W8 and layer layer1.7.conv1 to A2W8 p=0.7560988366603851
perturb layer layer1.0.conv1 to A4W8 and layer layer1.7.conv1 to A4W2 p=0.1931675225496292
perturb layer layer1.0.conv1 to A4W8 and layer layer1.7.conv1 to A4W4 p=0.19303368031978607
perturb layer layer1.0.conv1 to A4W8 and layer layer1.7.conv1 to A4W8 p=0.21895965933799744
perturb layer layer1.0.conv1 to A4W8 and layer layer1.7.conv1 to A8W2 p=0.1832306981086731
perturb layer layer1.0.conv1 to A4W8 and layer layer1.7.conv1 to A8W4 p=0.18324536830186844
perturb layer layer1.0.conv1 to A4W8 and layer layer1.7.conv1 to A8W8 p=0.2110007181763649
perturb layer layer1.0.conv1 to A8W2 and layer layer1.7.conv1 to A2W2 p=1.3967712819576263
perturb layer layer1.0.conv1 to A8W2 and layer layer1.7.conv1 to A2W4 p=1.202951654791832
perturb layer layer1.0.conv1 to A8W2 and layer layer1.7.conv1 to A2W8 p=1.212749183177948
perturb layer layer1.0.conv1 to A8W2 and layer layer1.7.conv1 to A4W2 p=0.9505407512187958
perturb layer layer1.0.conv1 to A8W2 and layer layer1.7.conv1 to A4W4 p=0.7188579589128494
perturb layer layer1.0.conv1 to A8W2 and layer layer1.7.conv1 to A4W8 p=0.7244343161582947
perturb layer layer1.0.conv1 to A8W2 and layer layer1.7.conv1 to A8W2 p=0.9769658893346786
perturb layer layer1.0.conv1 to A8W2 and layer layer1.7.conv1 to A8W4 p=0.7328915297985077
perturb layer layer1.0.conv1 to A8W2 and layer layer1.7.conv1 to A8W8 p=0.7383691519498825
perturb layer layer1.0.conv1 to A8W4 and layer layer1.7.conv1 to A2W2 p=0.5029045939445496
perturb layer layer1.0.conv1 to A8W4 and layer layer1.7.conv1 to A2W4 p=0.43361760675907135
perturb layer layer1.0.conv1 to A8W4 and layer layer1.7.conv1 to A2W8 p=0.44696682691574097
perturb layer layer1.0.conv1 to A8W4 and layer layer1.7.conv1 to A4W2 p=0.03456643968820572
perturb layer layer1.0.conv1 to A8W4 and layer layer1.7.conv1 to A4W4 p=-0.016495704650878906
perturb layer layer1.0.conv1 to A8W4 and layer layer1.7.conv1 to A4W8 p=-0.005181901156902313
perturb layer layer1.0.conv1 to A8W4 and layer layer1.7.conv1 to A8W2 p=0.03066662698984146
perturb layer layer1.0.conv1 to A8W4 and layer layer1.7.conv1 to A8W4 p=-0.024537324905395508
perturb layer layer1.0.conv1 to A8W4 and layer layer1.7.conv1 to A8W8 p=-0.01220092922449112
perturb layer layer1.0.conv1 to A8W8 and layer layer1.7.conv1 to A2W2 p=0.4416240304708481
perturb layer layer1.0.conv1 to A8W8 and layer layer1.7.conv1 to A2W4 p=0.38966046273708344
perturb layer layer1.0.conv1 to A8W8 and layer layer1.7.conv1 to A2W8 p=0.4084852635860443
perturb layer layer1.0.conv1 to A8W8 and layer layer1.7.conv1 to A4W2 p=0.02310408651828766
perturb layer layer1.0.conv1 to A8W8 and layer layer1.7.conv1 to A4W4 p=-0.01996607333421707
perturb layer layer1.0.conv1 to A8W8 and layer layer1.7.conv1 to A4W8 p=-0.00467362254858017
perturb layer layer1.0.conv1 to A8W8 and layer layer1.7.conv1 to A8W2 p=0.0281587615609169
perturb layer layer1.0.conv1 to A8W8 and layer layer1.7.conv1 to A8W4 p=-0.014790616929531097
perturb layer layer1.0.conv1 to A8W8 and layer layer1.7.conv1 to A8W8 p=0.0029485970735549927
perturb layer layer1.0.conv1 to A2W2 and layer layer1.7.conv2 to A2W2 p=2.1088527888059616
perturb layer layer1.0.conv1 to A2W2 and layer layer1.7.conv2 to A2W4 p=2.05660043656826
perturb layer layer1.0.conv1 to A2W2 and layer layer1.7.conv2 to A2W8 p=2.087350144982338
perturb layer layer1.0.conv1 to A2W2 and layer layer1.7.conv2 to A4W2 p=2.089293196797371
perturb layer layer1.0.conv1 to A2W2 and layer layer1.7.conv2 to A4W4 p=2.0360017865896225
perturb layer layer1.0.conv1 to A2W2 and layer layer1.7.conv2 to A4W8 p=2.0664482563734055
perturb layer layer1.0.conv1 to A2W2 and layer layer1.7.conv2 to A8W2 p=2.0889483243227005
perturb layer layer1.0.conv1 to A2W2 and layer layer1.7.conv2 to A8W4 p=2.037077561020851
perturb layer layer1.0.conv1 to A2W2 and layer layer1.7.conv2 to A8W8 p=2.0677462071180344
perturb layer layer1.0.conv1 to A2W4 and layer layer1.7.conv2 to A2W2 p=1.1804045885801315
perturb layer layer1.0.conv1 to A2W4 and layer layer1.7.conv2 to A2W4 p=1.2403860837221146
perturb layer layer1.0.conv1 to A2W4 and layer layer1.7.conv2 to A2W8 p=1.2633684128522873
perturb layer layer1.0.conv1 to A2W4 and layer layer1.7.conv2 to A4W2 p=1.1706354320049286
perturb layer layer1.0.conv1 to A2W4 and layer layer1.7.conv2 to A4W4 p=1.2300048768520355
perturb layer layer1.0.conv1 to A2W4 and layer layer1.7.conv2 to A4W8 p=1.2536868005990982
perturb layer layer1.0.conv1 to A2W4 and layer layer1.7.conv2 to A8W2 p=1.169339120388031
perturb layer layer1.0.conv1 to A2W4 and layer layer1.7.conv2 to A8W4 p=1.22996287047863
perturb layer layer1.0.conv1 to A2W4 and layer layer1.7.conv2 to A8W8 p=1.2532029747962952
perturb layer layer1.0.conv1 to A2W8 and layer layer1.7.conv2 to A2W2 p=1.151423156261444
perturb layer layer1.0.conv1 to A2W8 and layer layer1.7.conv2 to A2W4 p=1.2134458273649216
perturb layer layer1.0.conv1 to A2W8 and layer layer1.7.conv2 to A2W8 p=1.2349733412265778
perturb layer layer1.0.conv1 to A2W8 and layer layer1.7.conv2 to A4W2 p=1.14150732755661
perturb layer layer1.0.conv1 to A2W8 and layer layer1.7.conv2 to A4W4 p=1.2051787674427032
perturb layer layer1.0.conv1 to A2W8 and layer layer1.7.conv2 to A4W8 p=1.2270729839801788
perturb layer layer1.0.conv1 to A2W8 and layer layer1.7.conv2 to A8W2 p=1.1410077810287476
perturb layer layer1.0.conv1 to A2W8 and layer layer1.7.conv2 to A8W4 p=1.2043026983737946
perturb layer layer1.0.conv1 to A2W8 and layer layer1.7.conv2 to A8W8 p=1.2261791229248047
perturb layer layer1.0.conv1 to A4W2 and layer layer1.7.conv2 to A2W2 p=1.0069053769111633
perturb layer layer1.0.conv1 to A4W2 and layer layer1.7.conv2 to A2W4 p=0.9494224339723587
perturb layer layer1.0.conv1 to A4W2 and layer layer1.7.conv2 to A2W8 p=0.9714093655347824
perturb layer layer1.0.conv1 to A4W2 and layer layer1.7.conv2 to A4W2 p=0.9953871518373489
perturb layer layer1.0.conv1 to A4W2 and layer layer1.7.conv2 to A4W4 p=0.9367608726024628
perturb layer layer1.0.conv1 to A4W2 and layer layer1.7.conv2 to A4W8 p=0.9578025043010712
perturb layer layer1.0.conv1 to A4W2 and layer layer1.7.conv2 to A8W2 p=0.9953784495592117
perturb layer layer1.0.conv1 to A4W2 and layer layer1.7.conv2 to A8W4 p=0.9351277947425842
perturb layer layer1.0.conv1 to A4W2 and layer layer1.7.conv2 to A8W8 p=0.9558471590280533
perturb layer layer1.0.conv1 to A4W4 and layer layer1.7.conv2 to A2W2 p=0.15505351126194
perturb layer layer1.0.conv1 to A4W4 and layer layer1.7.conv2 to A2W4 p=0.1888050064444542
perturb layer layer1.0.conv1 to A4W4 and layer layer1.7.conv2 to A2W8 p=0.20485315471887589
perturb layer layer1.0.conv1 to A4W4 and layer layer1.7.conv2 to A4W2 p=0.14685547351837158
perturb layer layer1.0.conv1 to A4W4 and layer layer1.7.conv2 to A4W4 p=0.18272311985492706
perturb layer layer1.0.conv1 to A4W4 and layer layer1.7.conv2 to A4W8 p=0.19878684729337692
perturb layer layer1.0.conv1 to A4W4 and layer layer1.7.conv2 to A8W2 p=0.14650410413742065
perturb layer layer1.0.conv1 to A4W4 and layer layer1.7.conv2 to A8W4 p=0.18224773555994034
perturb layer layer1.0.conv1 to A4W4 and layer layer1.7.conv2 to A8W8 p=0.19819581508636475
perturb layer layer1.0.conv1 to A4W8 and layer layer1.7.conv2 to A2W2 p=0.15238571166992188
perturb layer layer1.0.conv1 to A4W8 and layer layer1.7.conv2 to A2W4 p=0.19522255659103394
perturb layer layer1.0.conv1 to A4W8 and layer layer1.7.conv2 to A2W8 p=0.21074584871530533
perturb layer layer1.0.conv1 to A4W8 and layer layer1.7.conv2 to A4W2 p=0.14700829982757568
perturb layer layer1.0.conv1 to A4W8 and layer layer1.7.conv2 to A4W4 p=0.19529582560062408
perturb layer layer1.0.conv1 to A4W8 and layer layer1.7.conv2 to A4W8 p=0.21172251552343369
perturb layer layer1.0.conv1 to A4W8 and layer layer1.7.conv2 to A8W2 p=0.1468997299671173
perturb layer layer1.0.conv1 to A4W8 and layer layer1.7.conv2 to A8W4 p=0.1955471932888031
perturb layer layer1.0.conv1 to A4W8 and layer layer1.7.conv2 to A8W8 p=0.2122838795185089
perturb layer layer1.0.conv1 to A8W2 and layer layer1.7.conv2 to A2W2 p=0.825057864189148
perturb layer layer1.0.conv1 to A8W2 and layer layer1.7.conv2 to A2W4 p=0.7293944656848907
perturb layer layer1.0.conv1 to A8W2 and layer layer1.7.conv2 to A2W8 p=0.7443369179964066
perturb layer layer1.0.conv1 to A8W2 and layer layer1.7.conv2 to A4W2 p=0.8236169517040253
perturb layer layer1.0.conv1 to A8W2 and layer layer1.7.conv2 to A4W4 p=0.7250822633504868
perturb layer layer1.0.conv1 to A8W2 and layer layer1.7.conv2 to A4W8 p=0.738414004445076
perturb layer layer1.0.conv1 to A8W2 and layer layer1.7.conv2 to A8W2 p=0.8224347233772278
perturb layer layer1.0.conv1 to A8W2 and layer layer1.7.conv2 to A8W4 p=0.7238979041576385
perturb layer layer1.0.conv1 to A8W2 and layer layer1.7.conv2 to A8W8 p=0.7372113168239594
perturb layer layer1.0.conv1 to A8W4 and layer layer1.7.conv2 to A2W2 p=-0.028616011142730713
perturb layer layer1.0.conv1 to A8W4 and layer layer1.7.conv2 to A2W4 p=-0.020236268639564514
perturb layer layer1.0.conv1 to A8W4 and layer layer1.7.conv2 to A2W8 p=-0.009024955332279205
perturb layer layer1.0.conv1 to A8W4 and layer layer1.7.conv2 to A4W2 p=-0.03095310926437378
perturb layer layer1.0.conv1 to A8W4 and layer layer1.7.conv2 to A4W4 p=-0.02349156141281128
perturb layer layer1.0.conv1 to A8W4 and layer layer1.7.conv2 to A4W8 p=-0.01161552220582962
perturb layer layer1.0.conv1 to A8W4 and layer layer1.7.conv2 to A8W2 p=-0.031507693231105804
perturb layer layer1.0.conv1 to A8W4 and layer layer1.7.conv2 to A8W4 p=-0.024239346385002136
perturb layer layer1.0.conv1 to A8W4 and layer layer1.7.conv2 to A8W8 p=-0.012411892414093018
perturb layer layer1.0.conv1 to A8W8 and layer layer1.7.conv2 to A2W2 p=-0.03228776901960373
perturb layer layer1.0.conv1 to A8W8 and layer layer1.7.conv2 to A2W4 p=-0.01235373318195343
perturb layer layer1.0.conv1 to A8W8 and layer layer1.7.conv2 to A2W8 p=0.00121249258518219
perturb layer layer1.0.conv1 to A8W8 and layer layer1.7.conv2 to A4W2 p=-0.03352687507867813
perturb layer layer1.0.conv1 to A8W8 and layer layer1.7.conv2 to A4W4 p=-0.011462636291980743
perturb layer layer1.0.conv1 to A8W8 and layer layer1.7.conv2 to A4W8 p=0.001916743814945221
perturb layer layer1.0.conv1 to A8W8 and layer layer1.7.conv2 to A8W2 p=-0.03241247683763504
perturb layer layer1.0.conv1 to A8W8 and layer layer1.7.conv2 to A8W4 p=-0.009967967867851257
perturb layer layer1.0.conv1 to A8W8 and layer layer1.7.conv2 to A8W8 p=0.0033962875604629517
perturb layer layer1.0.conv1 to A2W2 and layer layer1.8.conv1 to A2W2 p=2.776184394955635
perturb layer layer1.0.conv1 to A2W2 and layer layer1.8.conv1 to A2W4 p=2.3867154866456985
perturb layer layer1.0.conv1 to A2W2 and layer layer1.8.conv1 to A2W8 p=2.6766438633203506
perturb layer layer1.0.conv1 to A2W2 and layer layer1.8.conv1 to A4W2 p=2.2342356592416763
perturb layer layer1.0.conv1 to A2W2 and layer layer1.8.conv1 to A4W4 p=1.7595546692609787
perturb layer layer1.0.conv1 to A2W2 and layer layer1.8.conv1 to A4W8 p=2.0564779192209244
perturb layer layer1.0.conv1 to A2W2 and layer layer1.8.conv1 to A8W2 p=2.248239740729332
perturb layer layer1.0.conv1 to A2W2 and layer layer1.8.conv1 to A8W4 p=1.7646660655736923
perturb layer layer1.0.conv1 to A2W2 and layer layer1.8.conv1 to A8W8 p=2.0674528032541275
perturb layer layer1.0.conv1 to A2W4 and layer layer1.8.conv1 to A2W2 p=1.9950322657823563
perturb layer layer1.0.conv1 to A2W4 and layer layer1.8.conv1 to A2W4 p=1.9013279527425766
perturb layer layer1.0.conv1 to A2W4 and layer layer1.8.conv1 to A2W8 p=2.1074121445417404
perturb layer layer1.0.conv1 to A2W4 and layer layer1.8.conv1 to A4W2 p=1.38881815969944
perturb layer layer1.0.conv1 to A2W4 and layer layer1.8.conv1 to A4W4 p=1.0921616107225418
perturb layer layer1.0.conv1 to A2W4 and layer layer1.8.conv1 to A4W8 p=1.270391270518303
perturb layer layer1.0.conv1 to A2W4 and layer layer1.8.conv1 to A8W2 p=1.3972709327936172
perturb layer layer1.0.conv1 to A2W4 and layer layer1.8.conv1 to A8W4 p=1.0764267891645432
perturb layer layer1.0.conv1 to A2W4 and layer layer1.8.conv1 to A8W8 p=1.2541590332984924
perturb layer layer1.0.conv1 to A2W8 and layer layer1.8.conv1 to A2W2 p=1.8895786553621292
perturb layer layer1.0.conv1 to A2W8 and layer layer1.8.conv1 to A2W4 p=1.8122966438531876
perturb layer layer1.0.conv1 to A2W8 and layer layer1.8.conv1 to A2W8 p=2.0079128593206406
perturb layer layer1.0.conv1 to A2W8 and layer layer1.8.conv1 to A4W2 p=1.322968229651451
perturb layer layer1.0.conv1 to A2W8 and layer layer1.8.conv1 to A4W4 p=1.0670195072889328
perturb layer layer1.0.conv1 to A2W8 and layer layer1.8.conv1 to A4W8 p=1.2427456378936768
perturb layer layer1.0.conv1 to A2W8 and layer layer1.8.conv1 to A8W2 p=1.330086499452591
perturb layer layer1.0.conv1 to A2W8 and layer layer1.8.conv1 to A8W4 p=1.0581155866384506
perturb layer layer1.0.conv1 to A2W8 and layer layer1.8.conv1 to A8W8 p=1.2259187251329422
perturb layer layer1.0.conv1 to A4W2 and layer layer1.8.conv1 to A2W2 p=1.802584633231163
perturb layer layer1.0.conv1 to A4W2 and layer layer1.8.conv1 to A2W4 p=1.2759823501110077
perturb layer layer1.0.conv1 to A4W2 and layer layer1.8.conv1 to A2W8 p=1.4080288857221603
perturb layer layer1.0.conv1 to A4W2 and layer layer1.8.conv1 to A4W2 p=1.230653464794159
perturb layer layer1.0.conv1 to A4W2 and layer layer1.8.conv1 to A4W4 p=0.740497037768364
perturb layer layer1.0.conv1 to A4W2 and layer layer1.8.conv1 to A4W8 p=0.9272580146789551
perturb layer layer1.0.conv1 to A4W2 and layer layer1.8.conv1 to A8W2 p=1.2475747466087341
perturb layer layer1.0.conv1 to A4W2 and layer layer1.8.conv1 to A8W4 p=0.7601994872093201
perturb layer layer1.0.conv1 to A4W2 and layer layer1.8.conv1 to A8W8 p=0.953914538025856
perturb layer layer1.0.conv1 to A4W4 and layer layer1.8.conv1 to A2W2 p=0.8204443901777267
perturb layer layer1.0.conv1 to A4W4 and layer layer1.8.conv1 to A2W4 p=0.6158072054386139
perturb layer layer1.0.conv1 to A4W4 and layer layer1.8.conv1 to A2W8 p=0.7395201027393341
perturb layer layer1.0.conv1 to A4W4 and layer layer1.8.conv1 to A4W2 p=0.25267764925956726
perturb layer layer1.0.conv1 to A4W4 and layer layer1.8.conv1 to A4W4 p=0.07529929280281067
perturb layer layer1.0.conv1 to A4W4 and layer layer1.8.conv1 to A4W8 p=0.19518472254276276
perturb layer layer1.0.conv1 to A4W4 and layer layer1.8.conv1 to A8W2 p=0.2421201765537262
perturb layer layer1.0.conv1 to A4W4 and layer layer1.8.conv1 to A8W4 p=0.07658496499061584
perturb layer layer1.0.conv1 to A4W4 and layer layer1.8.conv1 to A8W8 p=0.1971917450428009
perturb layer layer1.0.conv1 to A4W8 and layer layer1.8.conv1 to A2W2 p=0.7225600928068161
perturb layer layer1.0.conv1 to A4W8 and layer layer1.8.conv1 to A2W4 p=0.5877453833818436
perturb layer layer1.0.conv1 to A4W8 and layer layer1.8.conv1 to A2W8 p=0.7093644738197327
perturb layer layer1.0.conv1 to A4W8 and layer layer1.8.conv1 to A4W2 p=0.18949396163225174
perturb layer layer1.0.conv1 to A4W8 and layer layer1.8.conv1 to A4W4 p=0.08955013006925583
perturb layer layer1.0.conv1 to A4W8 and layer layer1.8.conv1 to A4W8 p=0.21286630630493164
perturb layer layer1.0.conv1 to A4W8 and layer layer1.8.conv1 to A8W2 p=0.19089219719171524
perturb layer layer1.0.conv1 to A4W8 and layer layer1.8.conv1 to A8W4 p=0.09284666180610657
perturb layer layer1.0.conv1 to A4W8 and layer layer1.8.conv1 to A8W8 p=0.21195482462644577
perturb layer layer1.0.conv1 to A8W2 and layer layer1.8.conv1 to A2W2 p=1.7182755321264267
perturb layer layer1.0.conv1 to A8W2 and layer layer1.8.conv1 to A2W4 p=1.07977956533432
perturb layer layer1.0.conv1 to A8W2 and layer layer1.8.conv1 to A2W8 p=1.163870558142662
perturb layer layer1.0.conv1 to A8W2 and layer layer1.8.conv1 to A4W2 p=1.1710099428892136
perturb layer layer1.0.conv1 to A8W2 and layer layer1.8.conv1 to A4W4 p=0.5859231352806091
perturb layer layer1.0.conv1 to A8W2 and layer layer1.8.conv1 to A4W8 p=0.7223194241523743
perturb layer layer1.0.conv1 to A8W2 and layer layer1.8.conv1 to A8W2 p=1.1846620291471481
perturb layer layer1.0.conv1 to A8W2 and layer layer1.8.conv1 to A8W4 p=0.597937285900116
perturb layer layer1.0.conv1 to A8W2 and layer layer1.8.conv1 to A8W8 p=0.7377738207578659
perturb layer layer1.0.conv1 to A8W4 and layer layer1.8.conv1 to A2W2 p=0.6155389845371246
perturb layer layer1.0.conv1 to A8W4 and layer layer1.8.conv1 to A2W4 p=0.3381778299808502
perturb layer layer1.0.conv1 to A8W4 and layer layer1.8.conv1 to A2W8 p=0.4149925112724304
perturb layer layer1.0.conv1 to A8W4 and layer layer1.8.conv1 to A4W2 p=0.14979375898838043
perturb layer layer1.0.conv1 to A8W4 and layer layer1.8.conv1 to A4W4 p=-0.0847383588552475
perturb layer layer1.0.conv1 to A8W4 and layer layer1.8.conv1 to A4W8 p=-0.016089893877506256
perturb layer layer1.0.conv1 to A8W4 and layer layer1.8.conv1 to A8W2 p=0.14709196239709854
perturb layer layer1.0.conv1 to A8W4 and layer layer1.8.conv1 to A8W4 p=-0.07601390779018402
perturb layer layer1.0.conv1 to A8W4 and layer layer1.8.conv1 to A8W8 p=-0.01198866218328476
perturb layer layer1.0.conv1 to A8W8 and layer layer1.8.conv1 to A2W2 p=0.5491907894611359
perturb layer layer1.0.conv1 to A8W8 and layer layer1.8.conv1 to A2W4 p=0.3058674931526184
perturb layer layer1.0.conv1 to A8W8 and layer layer1.8.conv1 to A2W8 p=0.3946458250284195
perturb layer layer1.0.conv1 to A8W8 and layer layer1.8.conv1 to A4W2 p=0.1220429390668869
perturb layer layer1.0.conv1 to A8W8 and layer layer1.8.conv1 to A4W4 p=-0.07043194025754929
perturb layer layer1.0.conv1 to A8W8 and layer layer1.8.conv1 to A4W8 p=3.571808338165283e-05
perturb layer layer1.0.conv1 to A8W8 and layer layer1.8.conv1 to A8W2 p=0.11591498553752899
perturb layer layer1.0.conv1 to A8W8 and layer layer1.8.conv1 to A8W4 p=-0.06254248321056366
perturb layer layer1.0.conv1 to A8W8 and layer layer1.8.conv1 to A8W8 p=0.0038330554962158203
perturb layer layer1.0.conv1 to A2W2 and layer layer1.8.conv2 to A2W2 p=1.9877311736345291
perturb layer layer1.0.conv1 to A2W2 and layer layer1.8.conv2 to A2W4 p=1.9290376752614975
perturb layer layer1.0.conv1 to A2W2 and layer layer1.8.conv2 to A2W8 p=1.9749210625886917
perturb layer layer1.0.conv1 to A2W2 and layer layer1.8.conv2 to A4W2 p=2.0528113692998886
perturb layer layer1.0.conv1 to A2W2 and layer layer1.8.conv2 to A4W4 p=2.002141907811165
perturb layer layer1.0.conv1 to A2W2 and layer layer1.8.conv2 to A4W8 p=2.055771693587303
perturb layer layer1.0.conv1 to A2W2 and layer layer1.8.conv2 to A8W2 p=2.0585123747587204
perturb layer layer1.0.conv1 to A2W2 and layer layer1.8.conv2 to A8W4 p=2.0133132487535477
perturb layer layer1.0.conv1 to A2W2 and layer layer1.8.conv2 to A8W8 p=2.0679298490285873
perturb layer layer1.0.conv1 to A2W4 and layer layer1.8.conv2 to A2W2 p=0.9479964226484299
perturb layer layer1.0.conv1 to A2W4 and layer layer1.8.conv2 to A2W4 p=1.1279507130384445
perturb layer layer1.0.conv1 to A2W4 and layer layer1.8.conv2 to A2W8 p=1.178671047091484
perturb layer layer1.0.conv1 to A2W4 and layer layer1.8.conv2 to A4W2 p=0.9830458611249924
perturb layer layer1.0.conv1 to A2W4 and layer layer1.8.conv2 to A4W4 p=1.18461774289608
perturb layer layer1.0.conv1 to A2W4 and layer layer1.8.conv2 to A4W8 p=1.243503749370575
perturb layer layer1.0.conv1 to A2W4 and layer layer1.8.conv2 to A8W2 p=0.9860591143369675
perturb layer layer1.0.conv1 to A2W4 and layer layer1.8.conv2 to A8W4 p=1.19213205575943
perturb layer layer1.0.conv1 to A2W4 and layer layer1.8.conv2 to A8W8 p=1.2528550028800964
perturb layer layer1.0.conv1 to A2W8 and layer layer1.8.conv2 to A2W2 p=0.8931891918182373
perturb layer layer1.0.conv1 to A2W8 and layer layer1.8.conv2 to A2W4 p=1.1003879755735397
perturb layer layer1.0.conv1 to A2W8 and layer layer1.8.conv2 to A2W8 p=1.1540264636278152
perturb layer layer1.0.conv1 to A2W8 and layer layer1.8.conv2 to A4W2 p=0.9318829327821732
perturb layer layer1.0.conv1 to A2W8 and layer layer1.8.conv2 to A4W4 p=1.1580117046833038
perturb layer layer1.0.conv1 to A2W8 and layer layer1.8.conv2 to A4W8 p=1.2189225405454636
perturb layer layer1.0.conv1 to A2W8 and layer layer1.8.conv2 to A8W2 p=0.9347690045833588
perturb layer layer1.0.conv1 to A2W8 and layer layer1.8.conv2 to A8W4 p=1.165832445025444
perturb layer layer1.0.conv1 to A2W8 and layer layer1.8.conv2 to A8W8 p=1.2263905853033066
perturb layer layer1.0.conv1 to A4W2 and layer layer1.8.conv2 to A2W2 p=1.0338249057531357
perturb layer layer1.0.conv1 to A4W2 and layer layer1.8.conv2 to A2W4 p=0.8731181919574738
perturb layer layer1.0.conv1 to A4W2 and layer layer1.8.conv2 to A2W8 p=0.8975686132907867
perturb layer layer1.0.conv1 to A4W2 and layer layer1.8.conv2 to A4W2 p=1.0708919912576675
perturb layer layer1.0.conv1 to A4W2 and layer layer1.8.conv2 to A4W4 p=0.9194653630256653
perturb layer layer1.0.conv1 to A4W2 and layer layer1.8.conv2 to A4W8 p=0.9515815824270248
perturb layer layer1.0.conv1 to A4W2 and layer layer1.8.conv2 to A8W2 p=1.0728528499603271
perturb layer layer1.0.conv1 to A4W2 and layer layer1.8.conv2 to A8W4 p=0.9235912412405014
perturb layer layer1.0.conv1 to A4W2 and layer layer1.8.conv2 to A8W8 p=0.9556325227022171
perturb layer layer1.0.conv1 to A4W4 and layer layer1.8.conv2 to A2W2 p=0.08696288615465164
perturb layer layer1.0.conv1 to A4W4 and layer layer1.8.conv2 to A2W4 p=0.11158139258623123
perturb layer layer1.0.conv1 to A4W4 and layer layer1.8.conv2 to A2W8 p=0.14280935376882553
perturb layer layer1.0.conv1 to A4W4 and layer layer1.8.conv2 to A4W2 p=0.11449382454156876
perturb layer layer1.0.conv1 to A4W4 and layer layer1.8.conv2 to A4W4 p=0.1539418324828148
perturb layer layer1.0.conv1 to A4W4 and layer layer1.8.conv2 to A4W8 p=0.191765196621418
perturb layer layer1.0.conv1 to A4W4 and layer layer1.8.conv2 to A8W2 p=0.11726038157939911
perturb layer layer1.0.conv1 to A4W4 and layer layer1.8.conv2 to A8W4 p=0.15895093232393265
perturb layer layer1.0.conv1 to A4W4 and layer layer1.8.conv2 to A8W8 p=0.19807059317827225
perturb layer layer1.0.conv1 to A4W8 and layer layer1.8.conv2 to A2W2 p=0.04228159785270691
perturb layer layer1.0.conv1 to A4W8 and layer layer1.8.conv2 to A2W4 p=0.11223797500133514
perturb layer layer1.0.conv1 to A4W8 and layer layer1.8.conv2 to A2W8 p=0.14736659079790115
perturb layer layer1.0.conv1 to A4W8 and layer layer1.8.conv2 to A4W2 p=0.07726755738258362
perturb layer layer1.0.conv1 to A4W8 and layer layer1.8.conv2 to A4W4 p=0.16465944796800613
perturb layer layer1.0.conv1 to A4W8 and layer layer1.8.conv2 to A4W8 p=0.2058727890253067
perturb layer layer1.0.conv1 to A4W8 and layer layer1.8.conv2 to A8W2 p=0.08091109246015549
perturb layer layer1.0.conv1 to A4W8 and layer layer1.8.conv2 to A8W4 p=0.17063041776418686
perturb layer layer1.0.conv1 to A4W8 and layer layer1.8.conv2 to A8W8 p=0.21208221465349197
perturb layer layer1.0.conv1 to A8W2 and layer layer1.8.conv2 to A2W2 p=0.8954042494297028
perturb layer layer1.0.conv1 to A8W2 and layer layer1.8.conv2 to A2W4 p=0.686857059597969
perturb layer layer1.0.conv1 to A8W2 and layer layer1.8.conv2 to A2W8 p=0.7007217258214951
perturb layer layer1.0.conv1 to A8W2 and layer layer1.8.conv2 to A4W2 p=0.9163578450679779
perturb layer layer1.0.conv1 to A8W2 and layer layer1.8.conv2 to A4W4 p=0.7160775065422058
perturb layer layer1.0.conv1 to A8W2 and layer layer1.8.conv2 to A4W8 p=0.7345249205827713
perturb layer layer1.0.conv1 to A8W2 and layer layer1.8.conv2 to A8W2 p=0.9177337139844894
perturb layer layer1.0.conv1 to A8W2 and layer layer1.8.conv2 to A8W4 p=0.7166562080383301
perturb layer layer1.0.conv1 to A8W2 and layer layer1.8.conv2 to A8W8 p=0.7370893806219101
perturb layer layer1.0.conv1 to A8W4 and layer layer1.8.conv2 to A2W2 p=-0.0744844600558281
perturb layer layer1.0.conv1 to A8W4 and layer layer1.8.conv2 to A2W4 p=-0.07253627479076385
perturb layer layer1.0.conv1 to A8W4 and layer layer1.8.conv2 to A2W8 p=-0.053281255066394806
perturb layer layer1.0.conv1 to A8W4 and layer layer1.8.conv2 to A4W2 p=-0.04749318212270737
perturb layer layer1.0.conv1 to A8W4 and layer layer1.8.conv2 to A4W4 p=-0.037336938083171844
perturb layer layer1.0.conv1 to A8W4 and layer layer1.8.conv2 to A4W8 p=-0.013124652206897736
perturb layer layer1.0.conv1 to A8W4 and layer layer1.8.conv2 to A8W2 p=-0.04661949723958969
perturb layer layer1.0.conv1 to A8W4 and layer layer1.8.conv2 to A8W4 p=-0.03693404048681259
perturb layer layer1.0.conv1 to A8W4 and layer layer1.8.conv2 to A8W8 p=-0.012305982410907745
perturb layer layer1.0.conv1 to A8W8 and layer layer1.8.conv2 to A2W2 p=-0.11116310209035873
perturb layer layer1.0.conv1 to A8W8 and layer layer1.8.conv2 to A2W4 p=-0.06589126586914062
perturb layer layer1.0.conv1 to A8W8 and layer layer1.8.conv2 to A2W8 p=-0.043920665979385376
perturb layer layer1.0.conv1 to A8W8 and layer layer1.8.conv2 to A4W2 p=-0.08513785153627396
perturb layer layer1.0.conv1 to A8W8 and layer layer1.8.conv2 to A4W4 p=-0.03010193258523941
perturb layer layer1.0.conv1 to A8W8 and layer layer1.8.conv2 to A4W8 p=-0.003291577100753784
perturb layer layer1.0.conv1 to A8W8 and layer layer1.8.conv2 to A8W2 p=-0.08172287046909332
perturb layer layer1.0.conv1 to A8W8 and layer layer1.8.conv2 to A8W4 p=-0.024570882320404053
perturb layer layer1.0.conv1 to A8W8 and layer layer1.8.conv2 to A8W8 p=0.003364667296409607
perturb layer layer1.0.conv1 to A2W2 and layer layer2.0.conv1 to A2W2 p=5.956168040633202
perturb layer layer1.0.conv1 to A2W2 and layer layer2.0.conv1 to A2W4 p=3.0051891058683395
perturb layer layer1.0.conv1 to A2W2 and layer layer2.0.conv1 to A2W8 p=2.661538854241371
perturb layer layer1.0.conv1 to A2W2 and layer layer2.0.conv1 to A4W2 p=5.011188134551048
perturb layer layer1.0.conv1 to A2W2 and layer layer2.0.conv1 to A4W4 p=2.3351838141679764
perturb layer layer1.0.conv1 to A2W2 and layer layer2.0.conv1 to A4W8 p=2.0575352758169174
perturb layer layer1.0.conv1 to A2W2 and layer layer2.0.conv1 to A8W2 p=5.001107081770897
perturb layer layer1.0.conv1 to A2W2 and layer layer2.0.conv1 to A8W4 p=2.3456036299467087
perturb layer layer1.0.conv1 to A2W2 and layer layer2.0.conv1 to A8W8 p=2.068107947707176
perturb layer layer1.0.conv1 to A2W4 and layer layer2.0.conv1 to A2W2 p=5.49699954688549
perturb layer layer1.0.conv1 to A2W4 and layer layer2.0.conv1 to A2W4 p=2.6173811703920364
perturb layer layer1.0.conv1 to A2W4 and layer layer2.0.conv1 to A2W8 p=2.234055832028389
perturb layer layer1.0.conv1 to A2W4 and layer layer2.0.conv1 to A4W2 p=4.06049869954586
perturb layer layer1.0.conv1 to A2W4 and layer layer2.0.conv1 to A4W4 p=1.5586100965738297
perturb layer layer1.0.conv1 to A2W4 and layer layer2.0.conv1 to A4W8 p=1.2770900875329971
perturb layer layer1.0.conv1 to A2W4 and layer layer2.0.conv1 to A8W2 p=4.019685253500938
perturb layer layer1.0.conv1 to A2W4 and layer layer2.0.conv1 to A8W4 p=1.5270347744226456
perturb layer layer1.0.conv1 to A2W4 and layer layer2.0.conv1 to A8W8 p=1.2533798217773438
perturb layer layer1.0.conv1 to A2W8 and layer layer2.0.conv1 to A2W2 p=5.449451968073845
perturb layer layer1.0.conv1 to A2W8 and layer layer2.0.conv1 to A2W4 p=2.598095491528511
perturb layer layer1.0.conv1 to A2W8 and layer layer2.0.conv1 to A2W8 p=2.1862729638814926
perturb layer layer1.0.conv1 to A2W8 and layer layer2.0.conv1 to A4W2 p=3.983742281794548
perturb layer layer1.0.conv1 to A2W8 and layer layer2.0.conv1 to A4W4 p=1.5510178357362747
perturb layer layer1.0.conv1 to A2W8 and layer layer2.0.conv1 to A4W8 p=1.2678558826446533
perturb layer layer1.0.conv1 to A2W8 and layer layer2.0.conv1 to A8W2 p=3.954250082373619
perturb layer layer1.0.conv1 to A2W8 and layer layer2.0.conv1 to A8W4 p=1.5180986672639847
perturb layer layer1.0.conv1 to A2W8 and layer layer2.0.conv1 to A8W8 p=1.2266564220190048
perturb layer layer1.0.conv1 to A4W2 and layer layer2.0.conv1 to A2W2 p=4.11474572122097
perturb layer layer1.0.conv1 to A4W2 and layer layer2.0.conv1 to A2W4 p=1.6768404394388199
perturb layer layer1.0.conv1 to A4W2 and layer layer2.0.conv1 to A2W8 p=1.4384177476167679
perturb layer layer1.0.conv1 to A4W2 and layer layer2.0.conv1 to A4W2 p=3.146296188235283
perturb layer layer1.0.conv1 to A4W2 and layer layer2.0.conv1 to A4W4 p=1.1359193176031113
perturb layer layer1.0.conv1 to A4W2 and layer layer2.0.conv1 to A4W8 p=0.9548404216766357
perturb layer layer1.0.conv1 to A4W2 and layer layer2.0.conv1 to A8W2 p=3.1546754390001297
perturb layer layer1.0.conv1 to A4W2 and layer layer2.0.conv1 to A8W4 p=1.1419981122016907
perturb layer layer1.0.conv1 to A4W2 and layer layer2.0.conv1 to A8W8 p=0.9566149115562439
perturb layer layer1.0.conv1 to A4W4 and layer layer2.0.conv1 to A2W2 p=3.6500854939222336
perturb layer layer1.0.conv1 to A4W4 and layer layer2.0.conv1 to A2W4 p=1.1030296683311462
perturb layer layer1.0.conv1 to A4W4 and layer layer2.0.conv1 to A2W8 p=0.8164470493793488
perturb layer layer1.0.conv1 to A4W4 and layer layer2.0.conv1 to A4W2 p=2.305472806096077
perturb layer layer1.0.conv1 to A4W4 and layer layer2.0.conv1 to A4W4 p=0.4154869392514229
perturb layer layer1.0.conv1 to A4W4 and layer layer2.0.conv1 to A4W8 p=0.21486765146255493
perturb layer layer1.0.conv1 to A4W4 and layer layer2.0.conv1 to A8W2 p=2.2486880272626877
perturb layer layer1.0.conv1 to A4W4 and layer layer2.0.conv1 to A8W4 p=0.3983771577477455
perturb layer layer1.0.conv1 to A4W4 and layer layer2.0.conv1 to A8W8 p=0.19777794927358627
perturb layer layer1.0.conv1 to A4W8 and layer layer2.0.conv1 to A2W2 p=3.775854155421257
perturb layer layer1.0.conv1 to A4W8 and layer layer2.0.conv1 to A2W4 p=1.1836988627910614
perturb layer layer1.0.conv1 to A4W8 and layer layer2.0.conv1 to A2W8 p=0.8549184054136276
perturb layer layer1.0.conv1 to A4W8 and layer layer2.0.conv1 to A4W2 p=2.2831124514341354
perturb layer layer1.0.conv1 to A4W8 and layer layer2.0.conv1 to A4W4 p=0.43782199919223785
perturb layer layer1.0.conv1 to A4W8 and layer layer2.0.conv1 to A4W8 p=0.21836277097463608
perturb layer layer1.0.conv1 to A4W8 and layer layer2.0.conv1 to A8W2 p=2.226622775197029
perturb layer layer1.0.conv1 to A4W8 and layer layer2.0.conv1 to A8W4 p=0.4313240423798561
perturb layer layer1.0.conv1 to A4W8 and layer layer2.0.conv1 to A8W8 p=0.21192177385091782
perturb layer layer1.0.conv1 to A8W2 and layer layer2.0.conv1 to A2W2 p=3.6774890273809433
perturb layer layer1.0.conv1 to A8W2 and layer layer2.0.conv1 to A2W4 p=1.4048901349306107
perturb layer layer1.0.conv1 to A8W2 and layer layer2.0.conv1 to A2W8 p=1.2066502422094345
perturb layer layer1.0.conv1 to A8W2 and layer layer2.0.conv1 to A4W2 p=2.7584777921438217
perturb layer layer1.0.conv1 to A8W2 and layer layer2.0.conv1 to A4W4 p=0.903164267539978
perturb layer layer1.0.conv1 to A8W2 and layer layer2.0.conv1 to A4W8 p=0.7342968732118607
perturb layer layer1.0.conv1 to A8W2 and layer layer2.0.conv1 to A8W2 p=2.7571977227926254
perturb layer layer1.0.conv1 to A8W2 and layer layer2.0.conv1 to A8W4 p=0.9021733850240707
perturb layer layer1.0.conv1 to A8W2 and layer layer2.0.conv1 to A8W8 p=0.7367192357778549
perturb layer layer1.0.conv1 to A8W4 and layer layer2.0.conv1 to A2W2 p=2.9843605309724808
perturb layer layer1.0.conv1 to A8W4 and layer layer2.0.conv1 to A2W4 p=0.7680178880691528
perturb layer layer1.0.conv1 to A8W4 and layer layer2.0.conv1 to A2W8 p=0.48702748119831085
perturb layer layer1.0.conv1 to A8W4 and layer layer2.0.conv1 to A4W2 p=1.7332530170679092
perturb layer layer1.0.conv1 to A8W4 and layer layer2.0.conv1 to A4W4 p=0.1775655597448349
perturb layer layer1.0.conv1 to A8W4 and layer layer2.0.conv1 to A4W8 p=-0.00438159704208374
perturb layer layer1.0.conv1 to A8W4 and layer layer2.0.conv1 to A8W2 p=1.697085902094841
perturb layer layer1.0.conv1 to A8W4 and layer layer2.0.conv1 to A8W4 p=0.16399968415498734
perturb layer layer1.0.conv1 to A8W4 and layer layer2.0.conv1 to A8W8 p=-0.012540355324745178
perturb layer layer1.0.conv1 to A8W8 and layer layer2.0.conv1 to A2W2 p=3.067924991250038
perturb layer layer1.0.conv1 to A8W8 and layer layer2.0.conv1 to A2W4 p=0.8221996575593948
perturb layer layer1.0.conv1 to A8W8 and layer layer2.0.conv1 to A2W8 p=0.5026089549064636
perturb layer layer1.0.conv1 to A8W8 and layer layer2.0.conv1 to A4W2 p=1.704967513680458
perturb layer layer1.0.conv1 to A8W8 and layer layer2.0.conv1 to A4W4 p=0.20883896946907043
perturb layer layer1.0.conv1 to A8W8 and layer layer2.0.conv1 to A4W8 p=0.010596409440040588
perturb layer layer1.0.conv1 to A8W8 and layer layer2.0.conv1 to A8W2 p=1.664720818400383
perturb layer layer1.0.conv1 to A8W8 and layer layer2.0.conv1 to A8W4 p=0.1974300816655159
perturb layer layer1.0.conv1 to A8W8 and layer layer2.0.conv1 to A8W8 p=0.0028059035539627075
perturb layer layer1.0.conv1 to A2W2 and layer layer2.0.conv2 to A2W2 p=5.158717676997185
perturb layer layer1.0.conv1 to A2W2 and layer layer2.0.conv2 to A2W4 p=2.447066381573677
perturb layer layer1.0.conv1 to A2W2 and layer layer2.0.conv2 to A2W8 p=2.070338472723961
perturb layer layer1.0.conv1 to A2W2 and layer layer2.0.conv2 to A4W2 p=5.178464874625206
perturb layer layer1.0.conv1 to A2W2 and layer layer2.0.conv2 to A4W4 p=2.4543617218732834
perturb layer layer1.0.conv1 to A2W2 and layer layer2.0.conv2 to A4W8 p=2.0679154843091965
perturb layer layer1.0.conv1 to A2W2 and layer layer2.0.conv2 to A8W2 p=5.182473465800285
perturb layer layer1.0.conv1 to A2W2 and layer layer2.0.conv2 to A8W4 p=2.45624740421772
perturb layer layer1.0.conv1 to A2W2 and layer layer2.0.conv2 to A8W8 p=2.068340614438057
perturb layer layer1.0.conv1 to A2W4 and layer layer2.0.conv2 to A2W2 p=4.1475464552640915
perturb layer layer1.0.conv1 to A2W4 and layer layer2.0.conv2 to A2W4 p=1.6206710189580917
perturb layer layer1.0.conv1 to A2W4 and layer layer2.0.conv2 to A2W8 p=1.347193256020546
perturb layer layer1.0.conv1 to A2W4 and layer layer2.0.conv2 to A4W2 p=4.130551263689995
perturb layer layer1.0.conv1 to A2W4 and layer layer2.0.conv2 to A4W4 p=1.5479591935873032
perturb layer layer1.0.conv1 to A2W4 and layer layer2.0.conv2 to A4W8 p=1.2590420097112656
perturb layer layer1.0.conv1 to A2W4 and layer layer2.0.conv2 to A8W2 p=4.130117878317833
perturb layer layer1.0.conv1 to A2W4 and layer layer2.0.conv2 to A8W4 p=1.5425855964422226
perturb layer layer1.0.conv1 to A2W4 and layer layer2.0.conv2 to A8W8 p=1.2531566619873047
perturb layer layer1.0.conv1 to A2W8 and layer layer2.0.conv2 to A2W2 p=3.9700960367918015
perturb layer layer1.0.conv1 to A2W8 and layer layer2.0.conv2 to A2W4 p=1.5527647882699966
perturb layer layer1.0.conv1 to A2W8 and layer layer2.0.conv2 to A2W8 p=1.321849763393402
perturb layer layer1.0.conv1 to A2W8 and layer layer2.0.conv2 to A4W2 p=3.9612547010183334
perturb layer layer1.0.conv1 to A2W8 and layer layer2.0.conv2 to A4W4 p=1.488577976822853
perturb layer layer1.0.conv1 to A2W8 and layer layer2.0.conv2 to A4W8 p=1.2347144931554794
perturb layer layer1.0.conv1 to A2W8 and layer layer2.0.conv2 to A8W2 p=3.955770954489708
perturb layer layer1.0.conv1 to A2W8 and layer layer2.0.conv2 to A8W4 p=1.4793200343847275
perturb layer layer1.0.conv1 to A2W8 and layer layer2.0.conv2 to A8W8 p=1.2262279689311981
perturb layer layer1.0.conv1 to A4W2 and layer layer2.0.conv2 to A2W2 p=3.7998479455709457
perturb layer layer1.0.conv1 to A4W2 and layer layer2.0.conv2 to A2W4 p=1.2740953415632248
perturb layer layer1.0.conv1 to A4W2 and layer layer2.0.conv2 to A2W8 p=0.9962925165891647
perturb layer layer1.0.conv1 to A4W2 and layer layer2.0.conv2 to A4W2 p=3.8066595643758774
perturb layer layer1.0.conv1 to A4W2 and layer layer2.0.conv2 to A4W4 p=1.2285999357700348
perturb layer layer1.0.conv1 to A4W2 and layer layer2.0.conv2 to A4W8 p=0.9531095772981644
perturb layer layer1.0.conv1 to A4W2 and layer layer2.0.conv2 to A8W2 p=3.8051763623952866
perturb layer layer1.0.conv1 to A4W2 and layer layer2.0.conv2 to A8W4 p=1.2331772297620773
perturb layer layer1.0.conv1 to A4W2 and layer layer2.0.conv2 to A8W8 p=0.9551460295915604
perturb layer layer1.0.conv1 to A4W4 and layer layer2.0.conv2 to A2W2 p=2.556226924061775
perturb layer layer1.0.conv1 to A4W4 and layer layer2.0.conv2 to A2W4 p=0.42289094626903534
perturb layer layer1.0.conv1 to A4W4 and layer layer2.0.conv2 to A2W8 p=0.242257259786129
perturb layer layer1.0.conv1 to A4W4 and layer layer2.0.conv2 to A4W2 p=2.559021905064583
perturb layer layer1.0.conv1 to A4W4 and layer layer2.0.conv2 to A4W4 p=0.38321702927351
perturb layer layer1.0.conv1 to A4W4 and layer layer2.0.conv2 to A4W8 p=0.20843986421823502
perturb layer layer1.0.conv1 to A4W4 and layer layer2.0.conv2 to A8W2 p=2.549584373831749
perturb layer layer1.0.conv1 to A4W4 and layer layer2.0.conv2 to A8W4 p=0.37448330968618393
perturb layer layer1.0.conv1 to A4W4 and layer layer2.0.conv2 to A8W8 p=0.19817045331001282
perturb layer layer1.0.conv1 to A4W8 and layer layer2.0.conv2 to A2W2 p=2.439137175679207
perturb layer layer1.0.conv1 to A4W8 and layer layer2.0.conv2 to A2W4 p=0.4278329312801361
perturb layer layer1.0.conv1 to A4W8 and layer layer2.0.conv2 to A2W8 p=0.265105776488781
perturb layer layer1.0.conv1 to A4W8 and layer layer2.0.conv2 to A4W2 p=2.412275478243828
perturb layer layer1.0.conv1 to A4W8 and layer layer2.0.conv2 to A4W4 p=0.37519344687461853
perturb layer layer1.0.conv1 to A4W8 and layer layer2.0.conv2 to A4W8 p=0.22068504989147186
perturb layer layer1.0.conv1 to A4W8 and layer layer2.0.conv2 to A8W2 p=2.4124584048986435
perturb layer layer1.0.conv1 to A4W8 and layer layer2.0.conv2 to A8W4 p=0.36742445826530457
perturb layer layer1.0.conv1 to A4W8 and layer layer2.0.conv2 to A8W8 p=0.21248406916856766
perturb layer layer1.0.conv1 to A8W2 and layer layer2.0.conv2 to A2W2 p=3.513668492436409
perturb layer layer1.0.conv1 to A8W2 and layer layer2.0.conv2 to A2W4 p=1.0072277784347534
perturb layer layer1.0.conv1 to A8W2 and layer layer2.0.conv2 to A2W8 p=0.7732800245285034
perturb layer layer1.0.conv1 to A8W2 and layer layer2.0.conv2 to A4W2 p=3.5349312275648117
perturb layer layer1.0.conv1 to A8W2 and layer layer2.0.conv2 to A4W4 p=0.9711092710494995
perturb layer layer1.0.conv1 to A8W2 and layer layer2.0.conv2 to A4W8 p=0.7372962087392807
perturb layer layer1.0.conv1 to A8W2 and layer layer2.0.conv2 to A8W2 p=3.5381908863782883
perturb layer layer1.0.conv1 to A8W2 and layer layer2.0.conv2 to A8W4 p=0.9723871499300003
perturb layer layer1.0.conv1 to A8W2 and layer layer2.0.conv2 to A8W8 p=0.7375265061855316
perturb layer layer1.0.conv1 to A8W4 and layer layer2.0.conv2 to A2W2 p=2.1461329013109207
perturb layer layer1.0.conv1 to A8W4 and layer layer2.0.conv2 to A2W4 p=0.15398051589727402
perturb layer layer1.0.conv1 to A8W4 and layer layer2.0.conv2 to A2W8 p=0.02638256549835205
perturb layer layer1.0.conv1 to A8W4 and layer layer2.0.conv2 to A4W2 p=2.1358440071344376
perturb layer layer1.0.conv1 to A8W4 and layer layer2.0.conv2 to A4W4 p=0.1201719343662262
perturb layer layer1.0.conv1 to A8W4 and layer layer2.0.conv2 to A4W8 p=-0.0071633607149124146
perturb layer layer1.0.conv1 to A8W4 and layer layer2.0.conv2 to A8W2 p=2.129551336169243
perturb layer layer1.0.conv1 to A8W4 and layer layer2.0.conv2 to A8W4 p=0.11378606408834457
perturb layer layer1.0.conv1 to A8W4 and layer layer2.0.conv2 to A8W8 p=-0.012203007936477661
perturb layer layer1.0.conv1 to A8W8 and layer layer2.0.conv2 to A2W2 p=2.022056892514229
perturb layer layer1.0.conv1 to A8W8 and layer layer2.0.conv2 to A2W4 p=0.16402631253004074
perturb layer layer1.0.conv1 to A8W8 and layer layer2.0.conv2 to A2W8 p=0.05083268880844116
perturb layer layer1.0.conv1 to A8W8 and layer layer2.0.conv2 to A4W2 p=2.0051518827676773
perturb layer layer1.0.conv1 to A8W8 and layer layer2.0.conv2 to A4W4 p=0.12267755717039108
perturb layer layer1.0.conv1 to A8W8 and layer layer2.0.conv2 to A4W8 p=0.007429398596286774
perturb layer layer1.0.conv1 to A8W8 and layer layer2.0.conv2 to A8W2 p=2.0046133249998093
perturb layer layer1.0.conv1 to A8W8 and layer layer2.0.conv2 to A8W4 p=0.11688850075006485
perturb layer layer1.0.conv1 to A8W8 and layer layer2.0.conv2 to A8W8 p=0.0037606507539749146
perturb layer layer1.0.conv1 to A2W2 and layer layer2.0.downsample.0 to A2W2 p=6.449709877371788
perturb layer layer1.0.conv1 to A2W2 and layer layer2.0.downsample.0 to A2W4 p=3.458675429224968
perturb layer layer1.0.conv1 to A2W2 and layer layer2.0.downsample.0 to A2W8 p=2.661538854241371
perturb layer layer1.0.conv1 to A2W2 and layer layer2.0.downsample.0 to A4W2 p=5.858686313033104
perturb layer layer1.0.conv1 to A2W2 and layer layer2.0.downsample.0 to A4W4 p=2.80021770298481
perturb layer layer1.0.conv1 to A2W2 and layer layer2.0.downsample.0 to A4W8 p=2.0575352758169174
perturb layer layer1.0.conv1 to A2W2 and layer layer2.0.downsample.0 to A8W2 p=5.864734694361687
perturb layer layer1.0.conv1 to A2W2 and layer layer2.0.downsample.0 to A8W4 p=2.810922756791115
perturb layer layer1.0.conv1 to A2W2 and layer layer2.0.downsample.0 to A8W8 p=2.068107947707176
perturb layer layer1.0.conv1 to A2W4 and layer layer2.0.downsample.0 to A2W2 p=6.108287140727043
perturb layer layer1.0.conv1 to A2W4 and layer layer2.0.downsample.0 to A2W4 p=2.7186982482671738
perturb layer layer1.0.conv1 to A2W4 and layer layer2.0.downsample.0 to A2W8 p=2.234055832028389
perturb layer layer1.0.conv1 to A2W4 and layer layer2.0.downsample.0 to A4W2 p=5.029559060931206
perturb layer layer1.0.conv1 to A2W4 and layer layer2.0.downsample.0 to A4W4 p=1.7197906225919724
perturb layer layer1.0.conv1 to A2W4 and layer layer2.0.downsample.0 to A4W8 p=1.2770900875329971
perturb layer layer1.0.conv1 to A2W4 and layer layer2.0.downsample.0 to A8W2 p=5.006943687796593
perturb layer layer1.0.conv1 to A2W4 and layer layer2.0.downsample.0 to A8W4 p=1.6943199783563614
perturb layer layer1.0.conv1 to A2W4 and layer layer2.0.downsample.0 to A8W8 p=1.2533798217773438
perturb layer layer1.0.conv1 to A2W8 and layer layer2.0.downsample.0 to A2W2 p=6.05172298848629
perturb layer layer1.0.conv1 to A2W8 and layer layer2.0.downsample.0 to A2W4 p=2.696737602353096
perturb layer layer1.0.conv1 to A2W8 and layer layer2.0.downsample.0 to A2W8 p=2.1862729638814926
perturb layer layer1.0.conv1 to A2W8 and layer layer2.0.downsample.0 to A4W2 p=4.918725177645683
perturb layer layer1.0.conv1 to A2W8 and layer layer2.0.downsample.0 to A4W4 p=1.7058503776788712
perturb layer layer1.0.conv1 to A2W8 and layer layer2.0.downsample.0 to A4W8 p=1.2678558826446533
perturb layer layer1.0.conv1 to A2W8 and layer layer2.0.downsample.0 to A8W2 p=4.893613502383232
perturb layer layer1.0.conv1 to A2W8 and layer layer2.0.downsample.0 to A8W4 p=1.6768471449613571
perturb layer layer1.0.conv1 to A2W8 and layer layer2.0.downsample.0 to A8W8 p=1.2266564220190048
perturb layer layer1.0.conv1 to A4W2 and layer layer2.0.downsample.0 to A2W2 p=5.063855394721031
perturb layer layer1.0.conv1 to A4W2 and layer layer2.0.downsample.0 to A2W4 p=2.1694327741861343
perturb layer layer1.0.conv1 to A4W2 and layer layer2.0.downsample.0 to A2W8 p=1.4384177476167679
perturb layer layer1.0.conv1 to A4W2 and layer layer2.0.downsample.0 to A4W2 p=4.466513440012932
perturb layer layer1.0.conv1 to A4W2 and layer layer2.0.downsample.0 to A4W4 p=1.5571434050798416
perturb layer layer1.0.conv1 to A4W2 and layer layer2.0.downsample.0 to A4W8 p=0.9548404216766357
perturb layer layer1.0.conv1 to A4W2 and layer layer2.0.downsample.0 to A8W2 p=4.454903110861778
perturb layer layer1.0.conv1 to A4W2 and layer layer2.0.downsample.0 to A8W4 p=1.5621222406625748
perturb layer layer1.0.conv1 to A4W2 and layer layer2.0.downsample.0 to A8W8 p=0.9566149115562439
perturb layer layer1.0.conv1 to A4W4 and layer layer2.0.downsample.0 to A2W2 p=4.641122385859489
perturb layer layer1.0.conv1 to A4W4 and layer layer2.0.downsample.0 to A2W4 p=1.2575891017913818
perturb layer layer1.0.conv1 to A4W4 and layer layer2.0.downsample.0 to A2W8 p=0.8164470493793488
perturb layer layer1.0.conv1 to A4W4 and layer layer2.0.downsample.0 to A4W2 p=3.605053350329399
perturb layer layer1.0.conv1 to A4W4 and layer layer2.0.downsample.0 to A4W4 p=0.5128333494067192
perturb layer layer1.0.conv1 to A4W4 and layer layer2.0.downsample.0 to A4W8 p=0.21486765146255493
perturb layer layer1.0.conv1 to A4W4 and layer layer2.0.downsample.0 to A8W2 p=3.573390707373619
perturb layer layer1.0.conv1 to A4W4 and layer layer2.0.downsample.0 to A8W4 p=0.49208609759807587
perturb layer layer1.0.conv1 to A4W4 and layer layer2.0.downsample.0 to A8W8 p=0.19777794927358627
perturb layer layer1.0.conv1 to A4W8 and layer layer2.0.downsample.0 to A2W2 p=4.6381606310606
perturb layer layer1.0.conv1 to A4W8 and layer layer2.0.downsample.0 to A2W4 p=1.2977296561002731
perturb layer layer1.0.conv1 to A4W8 and layer layer2.0.downsample.0 to A2W8 p=0.8549184054136276
perturb layer layer1.0.conv1 to A4W8 and layer layer2.0.downsample.0 to A4W2 p=3.531372681260109
perturb layer layer1.0.conv1 to A4W8 and layer layer2.0.downsample.0 to A4W4 p=0.5163267329335213
perturb layer layer1.0.conv1 to A4W8 and layer layer2.0.downsample.0 to A4W8 p=0.21836277097463608
perturb layer layer1.0.conv1 to A4W8 and layer layer2.0.downsample.0 to A8W2 p=3.495231583714485
perturb layer layer1.0.conv1 to A4W8 and layer layer2.0.downsample.0 to A8W4 p=0.5078157782554626
perturb layer layer1.0.conv1 to A4W8 and layer layer2.0.downsample.0 to A8W8 p=0.21192177385091782
perturb layer layer1.0.conv1 to A8W2 and layer layer2.0.downsample.0 to A2W2 p=4.579209432005882
perturb layer layer1.0.conv1 to A8W2 and layer layer2.0.downsample.0 to A2W4 p=1.9117648154497147
perturb layer layer1.0.conv1 to A8W2 and layer layer2.0.downsample.0 to A2W8 p=1.2066502422094345
perturb layer layer1.0.conv1 to A8W2 and layer layer2.0.downsample.0 to A4W2 p=4.079992517828941
perturb layer layer1.0.conv1 to A8W2 and layer layer2.0.downsample.0 to A4W4 p=1.3494435846805573
perturb layer layer1.0.conv1 to A8W2 and layer layer2.0.downsample.0 to A4W8 p=0.7342968732118607
perturb layer layer1.0.conv1 to A8W2 and layer layer2.0.downsample.0 to A8W2 p=4.079487666487694
perturb layer layer1.0.conv1 to A8W2 and layer layer2.0.downsample.0 to A8W4 p=1.3463286757469177
perturb layer layer1.0.conv1 to A8W2 and layer layer2.0.downsample.0 to A8W8 p=0.7367192357778549
perturb layer layer1.0.conv1 to A8W4 and layer layer2.0.downsample.0 to A2W2 p=4.0304717272520065
perturb layer layer1.0.conv1 to A8W4 and layer layer2.0.downsample.0 to A2W4 p=0.9107630848884583
perturb layer layer1.0.conv1 to A8W4 and layer layer2.0.downsample.0 to A2W8 p=0.48702748119831085
perturb layer layer1.0.conv1 to A8W4 and layer layer2.0.downsample.0 to A4W2 p=3.0785495191812515
perturb layer layer1.0.conv1 to A8W4 and layer layer2.0.downsample.0 to A4W4 p=0.2911800891160965
perturb layer layer1.0.conv1 to A8W4 and layer layer2.0.downsample.0 to A4W8 p=-0.00438159704208374
perturb layer layer1.0.conv1 to A8W4 and layer layer2.0.downsample.0 to A8W2 p=3.0476511269807816
perturb layer layer1.0.conv1 to A8W4 and layer layer2.0.downsample.0 to A8W4 p=0.27283161878585815
perturb layer layer1.0.conv1 to A8W4 and layer layer2.0.downsample.0 to A8W8 p=-0.012540355324745178
perturb layer layer1.0.conv1 to A8W8 and layer layer2.0.downsample.0 to A2W2 p=3.996023043990135
perturb layer layer1.0.conv1 to A8W8 and layer layer2.0.downsample.0 to A2W4 p=0.9633577018976212
perturb layer layer1.0.conv1 to A8W8 and layer layer2.0.downsample.0 to A2W8 p=0.5026089549064636
perturb layer layer1.0.conv1 to A8W8 and layer layer2.0.downsample.0 to A4W2 p=3.0204731971025467
perturb layer layer1.0.conv1 to A8W8 and layer layer2.0.downsample.0 to A4W4 p=0.3064388632774353
perturb layer layer1.0.conv1 to A8W8 and layer layer2.0.downsample.0 to A4W8 p=0.010596409440040588
perturb layer layer1.0.conv1 to A8W8 and layer layer2.0.downsample.0 to A8W2 p=2.983459010720253
perturb layer layer1.0.conv1 to A8W8 and layer layer2.0.downsample.0 to A8W4 p=0.28936924040317535
perturb layer layer1.0.conv1 to A8W8 and layer layer2.0.downsample.0 to A8W8 p=0.0028059035539627075
perturb layer layer1.0.conv1 to A2W2 and layer layer2.1.conv1 to A2W2 p=3.629636660218239
perturb layer layer1.0.conv1 to A2W2 and layer layer2.1.conv1 to A2W4 p=2.8627326041460037
perturb layer layer1.0.conv1 to A2W2 and layer layer2.1.conv1 to A2W8 p=2.8367845863103867
perturb layer layer1.0.conv1 to A2W2 and layer layer2.1.conv1 to A4W2 p=2.658000811934471
perturb layer layer1.0.conv1 to A2W2 and layer layer2.1.conv1 to A4W4 p=2.086395964026451
perturb layer layer1.0.conv1 to A2W2 and layer layer2.1.conv1 to A4W8 p=2.07541261613369
perturb layer layer1.0.conv1 to A2W2 and layer layer2.1.conv1 to A8W2 p=2.657468095421791
perturb layer layer1.0.conv1 to A2W2 and layer layer2.1.conv1 to A8W4 p=2.0791628807783127
perturb layer layer1.0.conv1 to A2W2 and layer layer2.1.conv1 to A8W8 p=2.067211076617241
perturb layer layer1.0.conv1 to A2W4 and layer layer2.1.conv1 to A2W2 p=3.314799025654793
perturb layer layer1.0.conv1 to A2W4 and layer layer2.1.conv1 to A2W4 p=2.420034781098366
perturb layer layer1.0.conv1 to A2W4 and layer layer2.1.conv1 to A2W8 p=2.383837029337883
perturb layer layer1.0.conv1 to A2W4 and layer layer2.1.conv1 to A4W2 p=1.907056525349617
perturb layer layer1.0.conv1 to A2W4 and layer layer2.1.conv1 to A4W4 p=1.2927827686071396
perturb layer layer1.0.conv1 to A2W4 and layer layer2.1.conv1 to A4W8 p=1.2826824337244034
perturb layer layer1.0.conv1 to A2W4 and layer layer2.1.conv1 to A8W2 p=1.8779165297746658
perturb layer layer1.0.conv1 to A2W4 and layer layer2.1.conv1 to A8W4 p=1.2645916640758514
perturb layer layer1.0.conv1 to A2W4 and layer layer2.1.conv1 to A8W8 p=1.2535527050495148
perturb layer layer1.0.conv1 to A2W8 and layer layer2.1.conv1 to A2W2 p=3.2774598747491837
perturb layer layer1.0.conv1 to A2W8 and layer layer2.1.conv1 to A2W4 p=2.3836698085069656
perturb layer layer1.0.conv1 to A2W8 and layer layer2.1.conv1 to A2W8 p=2.351570889353752
perturb layer layer1.0.conv1 to A2W8 and layer layer2.1.conv1 to A4W2 p=1.8529798835515976
perturb layer layer1.0.conv1 to A2W8 and layer layer2.1.conv1 to A4W4 p=1.273600921034813
perturb layer layer1.0.conv1 to A2W8 and layer layer2.1.conv1 to A4W8 p=1.2691373825073242
perturb layer layer1.0.conv1 to A2W8 and layer layer2.1.conv1 to A8W2 p=1.816806122660637
perturb layer layer1.0.conv1 to A2W8 and layer layer2.1.conv1 to A8W4 p=1.2324276864528656
perturb layer layer1.0.conv1 to A2W8 and layer layer2.1.conv1 to A8W8 p=1.2267566621303558
perturb layer layer1.0.conv1 to A4W2 and layer layer2.1.conv1 to A2W2 p=2.136272743344307
perturb layer layer1.0.conv1 to A4W2 and layer layer2.1.conv1 to A2W4 p=1.5652894228696823
perturb layer layer1.0.conv1 to A4W2 and layer layer2.1.conv1 to A2W8 p=1.5470599681138992
perturb layer layer1.0.conv1 to A4W2 and layer layer2.1.conv1 to A4W2 p=1.3235175758600235
perturb layer layer1.0.conv1 to A4W2 and layer layer2.1.conv1 to A4W4 p=0.9663119465112686
perturb layer layer1.0.conv1 to A4W2 and layer layer2.1.conv1 to A4W8 p=0.9681873321533203
perturb layer layer1.0.conv1 to A4W2 and layer layer2.1.conv1 to A8W2 p=1.3134868144989014
perturb layer layer1.0.conv1 to A4W2 and layer layer2.1.conv1 to A8W4 p=0.9514418840408325
perturb layer layer1.0.conv1 to A4W2 and layer layer2.1.conv1 to A8W8 p=0.9567903280258179
perturb layer layer1.0.conv1 to A4W4 and layer layer2.1.conv1 to A2W2 p=1.509286805987358
perturb layer layer1.0.conv1 to A4W4 and layer layer2.1.conv1 to A2W4 p=0.9092137813568115
perturb layer layer1.0.conv1 to A4W4 and layer layer2.1.conv1 to A2W8 p=0.8890203982591629
perturb layer layer1.0.conv1 to A4W4 and layer layer2.1.conv1 to A4W2 p=0.5399942100048065
perturb layer layer1.0.conv1 to A4W4 and layer layer2.1.conv1 to A4W4 p=0.2160285785794258
perturb layer layer1.0.conv1 to A4W4 and layer layer2.1.conv1 to A4W8 p=0.21474333852529526
perturb layer layer1.0.conv1 to A4W4 and layer layer2.1.conv1 to A8W2 p=0.5156983882188797
perturb layer layer1.0.conv1 to A4W4 and layer layer2.1.conv1 to A8W4 p=0.1961994394659996
perturb layer layer1.0.conv1 to A4W4 and layer layer2.1.conv1 to A8W8 p=0.19812249392271042
perturb layer layer1.0.conv1 to A4W8 and layer layer2.1.conv1 to A2W2 p=1.5780789703130722
perturb layer layer1.0.conv1 to A4W8 and layer layer2.1.conv1 to A2W4 p=0.9664175510406494
perturb layer layer1.0.conv1 to A4W8 and layer layer2.1.conv1 to A2W8 p=0.943741649389267
perturb layer layer1.0.conv1 to A4W8 and layer layer2.1.conv1 to A4W2 p=0.5456868410110474
perturb layer layer1.0.conv1 to A4W8 and layer layer2.1.conv1 to A4W4 p=0.22397445887327194
perturb layer layer1.0.conv1 to A4W8 and layer layer2.1.conv1 to A4W8 p=0.22808365523815155
perturb layer layer1.0.conv1 to A4W8 and layer layer2.1.conv1 to A8W2 p=0.5192936211824417
perturb layer layer1.0.conv1 to A4W8 and layer layer2.1.conv1 to A8W4 p=0.2055903971195221
perturb layer layer1.0.conv1 to A4W8 and layer layer2.1.conv1 to A8W8 p=0.21236129850149155
perturb layer layer1.0.conv1 to A8W2 and layer layer2.1.conv1 to A2W2 p=1.8522571474313736
perturb layer layer1.0.conv1 to A8W2 and layer layer2.1.conv1 to A2W4 p=1.3042614609003067
perturb layer layer1.0.conv1 to A8W2 and layer layer2.1.conv1 to A2W8 p=1.289199635386467
perturb layer layer1.0.conv1 to A8W2 and layer layer2.1.conv1 to A4W2 p=1.1050136238336563
perturb layer layer1.0.conv1 to A8W2 and layer layer2.1.conv1 to A4W4 p=0.75507552921772
perturb layer layer1.0.conv1 to A8W2 and layer layer2.1.conv1 to A4W8 p=0.7532681077718735
perturb layer layer1.0.conv1 to A8W2 and layer layer2.1.conv1 to A8W2 p=1.0837915688753128
perturb layer layer1.0.conv1 to A8W2 and layer layer2.1.conv1 to A8W4 p=0.7388150990009308
perturb layer layer1.0.conv1 to A8W2 and layer layer2.1.conv1 to A8W8 p=0.7373119741678238
perturb layer layer1.0.conv1 to A8W4 and layer layer2.1.conv1 to A2W2 p=1.047795832157135
perturb layer layer1.0.conv1 to A8W4 and layer layer2.1.conv1 to A2W4 p=0.5617272704839706
perturb layer layer1.0.conv1 to A8W4 and layer layer2.1.conv1 to A2W8 p=0.5410669445991516
perturb layer layer1.0.conv1 to A8W4 and layer layer2.1.conv1 to A4W2 p=0.25190164148807526
perturb layer layer1.0.conv1 to A8W4 and layer layer2.1.conv1 to A4W4 p=-0.0010315477848052979
perturb layer layer1.0.conv1 to A8W4 and layer layer2.1.conv1 to A4W8 p=0.0011215433478355408
perturb layer layer1.0.conv1 to A8W4 and layer layer2.1.conv1 to A8W2 p=0.24209999293088913
perturb layer layer1.0.conv1 to A8W4 and layer layer2.1.conv1 to A8W4 p=-0.015623234212398529
perturb layer layer1.0.conv1 to A8W4 and layer layer2.1.conv1 to A8W8 p=-0.012077078223228455
perturb layer layer1.0.conv1 to A8W8 and layer layer2.1.conv1 to A2W2 p=1.1234515607357025
perturb layer layer1.0.conv1 to A8W8 and layer layer2.1.conv1 to A2W4 p=0.6151894330978394
perturb layer layer1.0.conv1 to A8W8 and layer layer2.1.conv1 to A2W8 p=0.5975873321294785
perturb layer layer1.0.conv1 to A8W8 and layer layer2.1.conv1 to A4W2 p=0.2788667604327202
perturb layer layer1.0.conv1 to A8W8 and layer layer2.1.conv1 to A4W4 p=0.021946750581264496
perturb layer layer1.0.conv1 to A8W8 and layer layer2.1.conv1 to A4W8 p=0.025446735322475433
perturb layer layer1.0.conv1 to A8W8 and layer layer2.1.conv1 to A8W2 p=0.25233398377895355
perturb layer layer1.0.conv1 to A8W8 and layer layer2.1.conv1 to A8W4 p=-0.0016885697841644287
perturb layer layer1.0.conv1 to A8W8 and layer layer2.1.conv1 to A8W8 p=0.0033120065927505493
perturb layer layer1.0.conv1 to A2W2 and layer layer2.1.conv2 to A2W2 p=2.312050387263298
perturb layer layer1.0.conv1 to A2W2 and layer layer2.1.conv2 to A2W4 p=2.0951546281576157
perturb layer layer1.0.conv1 to A2W2 and layer layer2.1.conv2 to A2W8 p=2.0953799337148666
perturb layer layer1.0.conv1 to A2W2 and layer layer2.1.conv2 to A4W2 p=2.2961072772741318
perturb layer layer1.0.conv1 to A2W2 and layer layer2.1.conv2 to A4W4 p=2.0695980936288834
perturb layer layer1.0.conv1 to A2W2 and layer layer2.1.conv2 to A4W8 p=2.0687984973192215
perturb layer layer1.0.conv1 to A2W2 and layer layer2.1.conv2 to A8W2 p=2.2961204797029495
perturb layer layer1.0.conv1 to A2W2 and layer layer2.1.conv2 to A8W4 p=2.0678993314504623
perturb layer layer1.0.conv1 to A2W2 and layer layer2.1.conv2 to A8W8 p=2.0678070932626724
perturb layer layer1.0.conv1 to A2W4 and layer layer2.1.conv2 to A2W2 p=1.4390225559473038
perturb layer layer1.0.conv1 to A2W4 and layer layer2.1.conv2 to A2W4 p=1.2957221120595932
perturb layer layer1.0.conv1 to A2W4 and layer layer2.1.conv2 to A2W8 p=1.29450623691082
perturb layer layer1.0.conv1 to A2W4 and layer layer2.1.conv2 to A4W2 p=1.4137154072523117
perturb layer layer1.0.conv1 to A2W4 and layer layer2.1.conv2 to A4W4 p=1.2572913020849228
perturb layer layer1.0.conv1 to A2W4 and layer layer2.1.conv2 to A4W8 p=1.2548533976078033
perturb layer layer1.0.conv1 to A2W4 and layer layer2.1.conv2 to A8W2 p=1.4116818755865097
perturb layer layer1.0.conv1 to A2W4 and layer layer2.1.conv2 to A8W4 p=1.2558317631483078
perturb layer layer1.0.conv1 to A2W4 and layer layer2.1.conv2 to A8W8 p=1.253348246216774
perturb layer layer1.0.conv1 to A2W8 and layer layer2.1.conv2 to A2W2 p=1.3964413851499557
perturb layer layer1.0.conv1 to A2W8 and layer layer2.1.conv2 to A2W4 p=1.2666511982679367
perturb layer layer1.0.conv1 to A2W8 and layer layer2.1.conv2 to A2W8 p=1.264029249548912
perturb layer layer1.0.conv1 to A2W8 and layer layer2.1.conv2 to A4W2 p=1.3741485625505447
perturb layer layer1.0.conv1 to A2W8 and layer layer2.1.conv2 to A4W4 p=1.228966012597084
perturb layer layer1.0.conv1 to A2W8 and layer layer2.1.conv2 to A4W8 p=1.2253633737564087
perturb layer layer1.0.conv1 to A2W8 and layer layer2.1.conv2 to A8W2 p=1.3760974258184433
perturb layer layer1.0.conv1 to A2W8 and layer layer2.1.conv2 to A8W4 p=1.2295840680599213
perturb layer layer1.0.conv1 to A2W8 and layer layer2.1.conv2 to A8W8 p=1.226109966635704
perturb layer layer1.0.conv1 to A4W2 and layer layer2.1.conv2 to A2W2 p=1.0775707811117172
perturb layer layer1.0.conv1 to A4W2 and layer layer2.1.conv2 to A2W4 p=0.9662531316280365
perturb layer layer1.0.conv1 to A4W2 and layer layer2.1.conv2 to A2W8 p=0.9683685451745987
perturb layer layer1.0.conv1 to A4W2 and layer layer2.1.conv2 to A4W2 p=1.0794352144002914
perturb layer layer1.0.conv1 to A4W2 and layer layer2.1.conv2 to A4W4 p=0.9542335271835327
perturb layer layer1.0.conv1 to A4W2 and layer layer2.1.conv2 to A4W8 p=0.9576670974493027
perturb layer layer1.0.conv1 to A4W2 and layer layer2.1.conv2 to A8W2 p=1.0775004774332047
perturb layer layer1.0.conv1 to A4W2 and layer layer2.1.conv2 to A8W4 p=0.9522167593240738
perturb layer layer1.0.conv1 to A4W2 and layer layer2.1.conv2 to A8W8 p=0.9554652571678162
perturb layer layer1.0.conv1 to A4W4 and layer layer2.1.conv2 to A2W2 p=0.28036151081323624
perturb layer layer1.0.conv1 to A4W4 and layer layer2.1.conv2 to A2W4 p=0.21966338157653809
perturb layer layer1.0.conv1 to A4W4 and layer layer2.1.conv2 to A2W8 p=0.220224991440773
perturb layer layer1.0.conv1 to A4W4 and layer layer2.1.conv2 to A4W2 p=0.26486635208129883
perturb layer layer1.0.conv1 to A4W4 and layer layer2.1.conv2 to A4W4 p=0.19958964735269547
perturb layer layer1.0.conv1 to A4W4 and layer layer2.1.conv2 to A4W8 p=0.2007140815258026
perturb layer layer1.0.conv1 to A4W4 and layer layer2.1.conv2 to A8W2 p=0.26303479075431824
perturb layer layer1.0.conv1 to A4W4 and layer layer2.1.conv2 to A8W4 p=0.19767337292432785
perturb layer layer1.0.conv1 to A4W4 and layer layer2.1.conv2 to A8W8 p=0.19828540831804276
perturb layer layer1.0.conv1 to A4W8 and layer layer2.1.conv2 to A2W2 p=0.29856085777282715
perturb layer layer1.0.conv1 to A4W8 and layer layer2.1.conv2 to A2W4 p=0.23889072239398956
perturb layer layer1.0.conv1 to A4W8 and layer layer2.1.conv2 to A2W8 p=0.23502106219530106
perturb layer layer1.0.conv1 to A4W8 and layer layer2.1.conv2 to A4W2 p=0.2822612598538399
perturb layer layer1.0.conv1 to A4W8 and layer layer2.1.conv2 to A4W4 p=0.21899475902318954
perturb layer layer1.0.conv1 to A4W8 and layer layer2.1.conv2 to A4W8 p=0.2151857241988182
perturb layer layer1.0.conv1 to A4W8 and layer layer2.1.conv2 to A8W2 p=0.2801768556237221
perturb layer layer1.0.conv1 to A4W8 and layer layer2.1.conv2 to A8W4 p=0.21624833345413208
perturb layer layer1.0.conv1 to A4W8 and layer layer2.1.conv2 to A8W8 p=0.21224144101142883
perturb layer layer1.0.conv1 to A8W2 and layer layer2.1.conv2 to A2W2 p=0.8581031411886215
perturb layer layer1.0.conv1 to A8W2 and layer layer2.1.conv2 to A2W4 p=0.7514595538377762
perturb layer layer1.0.conv1 to A8W2 and layer layer2.1.conv2 to A2W8 p=0.7530895173549652
perturb layer layer1.0.conv1 to A8W2 and layer layer2.1.conv2 to A4W2 p=0.8530275672674179
perturb layer layer1.0.conv1 to A8W2 and layer layer2.1.conv2 to A4W4 p=0.7374618500471115
perturb layer layer1.0.conv1 to A8W2 and layer layer2.1.conv2 to A4W8 p=0.7393511682748795
perturb layer layer1.0.conv1 to A8W2 and layer layer2.1.conv2 to A8W2 p=0.8515737801790237
perturb layer layer1.0.conv1 to A8W2 and layer layer2.1.conv2 to A8W4 p=0.7350525707006454
perturb layer layer1.0.conv1 to A8W2 and layer layer2.1.conv2 to A8W8 p=0.7372857481241226
perturb layer layer1.0.conv1 to A8W4 and layer layer2.1.conv2 to A2W2 p=0.0523797869682312
perturb layer layer1.0.conv1 to A8W4 and layer layer2.1.conv2 to A2W4 p=0.0031367838382720947
perturb layer layer1.0.conv1 to A8W4 and layer layer2.1.conv2 to A2W8 p=0.003049299120903015
perturb layer layer1.0.conv1 to A8W4 and layer layer2.1.conv2 to A4W2 p=0.043939895927906036
perturb layer layer1.0.conv1 to A8W4 and layer layer2.1.conv2 to A4W4 p=-0.010440453886985779
perturb layer layer1.0.conv1 to A8W4 and layer layer2.1.conv2 to A4W8 p=-0.010571032762527466
perturb layer layer1.0.conv1 to A8W4 and layer layer2.1.conv2 to A8W2 p=0.04194287955760956
perturb layer layer1.0.conv1 to A8W4 and layer layer2.1.conv2 to A8W4 p=-0.012357361614704132
perturb layer layer1.0.conv1 to A8W4 and layer layer2.1.conv2 to A8W8 p=-0.01236095279455185
perturb layer layer1.0.conv1 to A8W8 and layer layer2.1.conv2 to A2W2 p=0.07103968411684036
perturb layer layer1.0.conv1 to A8W8 and layer layer2.1.conv2 to A2W4 p=0.024995654821395874
perturb layer layer1.0.conv1 to A8W8 and layer layer2.1.conv2 to A2W8 p=0.020557954907417297
perturb layer layer1.0.conv1 to A8W8 and layer layer2.1.conv2 to A4W2 p=0.06096804887056351
perturb layer layer1.0.conv1 to A8W8 and layer layer2.1.conv2 to A4W4 p=0.011134058237075806
perturb layer layer1.0.conv1 to A8W8 and layer layer2.1.conv2 to A4W8 p=0.00654950737953186
perturb layer layer1.0.conv1 to A8W8 and layer layer2.1.conv2 to A8W2 p=0.05816996097564697
perturb layer layer1.0.conv1 to A8W8 and layer layer2.1.conv2 to A8W4 p=0.008043602108955383
perturb layer layer1.0.conv1 to A8W8 and layer layer2.1.conv2 to A8W8 p=0.00347958505153656
perturb layer layer1.0.conv1 to A2W2 and layer layer2.2.conv1 to A2W2 p=3.8252870589494705
perturb layer layer1.0.conv1 to A2W2 and layer layer2.2.conv1 to A2W4 p=2.7849062234163284
perturb layer layer1.0.conv1 to A2W2 and layer layer2.2.conv1 to A2W8 p=2.7334877997636795
perturb layer layer1.0.conv1 to A2W2 and layer layer2.2.conv1 to A4W2 p=3.13378943502903
perturb layer layer1.0.conv1 to A2W2 and layer layer2.2.conv1 to A4W4 p=2.147951766848564
perturb layer layer1.0.conv1 to A2W2 and layer layer2.2.conv1 to A4W8 p=2.079590931534767
perturb layer layer1.0.conv1 to A2W2 and layer layer2.2.conv1 to A8W2 p=3.136200502514839
perturb layer layer1.0.conv1 to A2W2 and layer layer2.2.conv1 to A8W4 p=2.137694224715233
perturb layer layer1.0.conv1 to A2W2 and layer layer2.2.conv1 to A8W8 p=2.0688605457544327
perturb layer layer1.0.conv1 to A2W4 and layer layer2.2.conv1 to A2W2 p=3.215404823422432
perturb layer layer1.0.conv1 to A2W4 and layer layer2.2.conv1 to A2W4 p=2.172253891825676
perturb layer layer1.0.conv1 to A2W4 and layer layer2.2.conv1 to A2W8 p=2.1475495249032974
perturb layer layer1.0.conv1 to A2W4 and layer layer2.2.conv1 to A4W2 p=2.187973454594612
perturb layer layer1.0.conv1 to A2W4 and layer layer2.2.conv1 to A4W4 p=1.307605192065239
perturb layer layer1.0.conv1 to A2W4 and layer layer2.2.conv1 to A4W8 p=1.292033463716507
perturb layer layer1.0.conv1 to A2W4 and layer layer2.2.conv1 to A8W2 p=2.1496753841638565
perturb layer layer1.0.conv1 to A2W4 and layer layer2.2.conv1 to A8W4 p=1.2717324048280716
perturb layer layer1.0.conv1 to A2W4 and layer layer2.2.conv1 to A8W8 p=1.2548539340496063
perturb layer layer1.0.conv1 to A2W8 and layer layer2.2.conv1 to A2W2 p=3.170519843697548
perturb layer layer1.0.conv1 to A2W8 and layer layer2.2.conv1 to A2W4 p=2.216025933623314
perturb layer layer1.0.conv1 to A2W8 and layer layer2.2.conv1 to A2W8 p=2.193159595131874
perturb layer layer1.0.conv1 to A2W8 and layer layer2.2.conv1 to A4W2 p=2.098749056458473
perturb layer layer1.0.conv1 to A2W8 and layer layer2.2.conv1 to A4W4 p=1.295108288526535
perturb layer layer1.0.conv1 to A2W8 and layer layer2.2.conv1 to A4W8 p=1.2660131007432938
perturb layer layer1.0.conv1 to A2W8 and layer layer2.2.conv1 to A8W2 p=2.0621587187051773
perturb layer layer1.0.conv1 to A2W8 and layer layer2.2.conv1 to A8W4 p=1.2518964856863022
perturb layer layer1.0.conv1 to A2W8 and layer layer2.2.conv1 to A8W8 p=1.2268039733171463
perturb layer layer1.0.conv1 to A4W2 and layer layer2.2.conv1 to A2W2 p=2.3549443036317825
perturb layer layer1.0.conv1 to A4W2 and layer layer2.2.conv1 to A2W4 p=1.4856338948011398
perturb layer layer1.0.conv1 to A4W2 and layer layer2.2.conv1 to A2W8 p=1.447077363729477
perturb layer layer1.0.conv1 to A4W2 and layer layer2.2.conv1 to A4W2 p=1.756543979048729
perturb layer layer1.0.conv1 to A4W2 and layer layer2.2.conv1 to A4W4 p=1.0034690201282501
perturb layer layer1.0.conv1 to A4W2 and layer layer2.2.conv1 to A4W8 p=0.9573779851198196
perturb layer layer1.0.conv1 to A4W2 and layer layer2.2.conv1 to A8W2 p=1.746846929192543
perturb layer layer1.0.conv1 to A4W2 and layer layer2.2.conv1 to A8W4 p=1.0009643137454987
perturb layer layer1.0.conv1 to A4W2 and layer layer2.2.conv1 to A8W8 p=0.9558524340391159
perturb layer layer1.0.conv1 to A4W4 and layer layer2.2.conv1 to A2W2 p=1.521994188427925
perturb layer layer1.0.conv1 to A4W4 and layer layer2.2.conv1 to A2W4 p=0.8020303696393967
perturb layer layer1.0.conv1 to A4W4 and layer layer2.2.conv1 to A2W8 p=0.7853077799081802
perturb layer layer1.0.conv1 to A4W4 and layer layer2.2.conv1 to A4W2 p=0.7575734853744507
perturb layer layer1.0.conv1 to A4W4 and layer layer2.2.conv1 to A4W4 p=0.2463037297129631
perturb layer layer1.0.conv1 to A4W4 and layer layer2.2.conv1 to A4W8 p=0.22121817618608475
perturb layer layer1.0.conv1 to A4W4 and layer layer2.2.conv1 to A8W2 p=0.7280427664518356
perturb layer layer1.0.conv1 to A4W4 and layer layer2.2.conv1 to A8W4 p=0.22111359983682632
perturb layer layer1.0.conv1 to A4W4 and layer layer2.2.conv1 to A8W8 p=0.19805146008729935
perturb layer layer1.0.conv1 to A4W8 and layer layer2.2.conv1 to A2W2 p=1.5455372333526611
perturb layer layer1.0.conv1 to A4W8 and layer layer2.2.conv1 to A2W4 p=0.8407255709171295
perturb layer layer1.0.conv1 to A4W8 and layer layer2.2.conv1 to A2W8 p=0.8064226508140564
perturb layer layer1.0.conv1 to A4W8 and layer layer2.2.conv1 to A4W2 p=0.7516883760690689
perturb layer layer1.0.conv1 to A4W8 and layer layer2.2.conv1 to A4W4 p=0.2630193307995796
perturb layer layer1.0.conv1 to A4W8 and layer layer2.2.conv1 to A4W8 p=0.23723076283931732
perturb layer layer1.0.conv1 to A4W8 and layer layer2.2.conv1 to A8W2 p=0.7193689197301865
perturb layer layer1.0.conv1 to A4W8 and layer layer2.2.conv1 to A8W4 p=0.2380453199148178
perturb layer layer1.0.conv1 to A4W8 and layer layer2.2.conv1 to A8W8 p=0.2133471965789795
perturb layer layer1.0.conv1 to A8W2 and layer layer2.2.conv1 to A2W2 p=2.07854525744915
perturb layer layer1.0.conv1 to A8W2 and layer layer2.2.conv1 to A2W4 p=1.265162244439125
perturb layer layer1.0.conv1 to A8W2 and layer layer2.2.conv1 to A2W8 p=1.2180112153291702
perturb layer layer1.0.conv1 to A8W2 and layer layer2.2.conv1 to A4W2 p=1.440098688006401
perturb layer layer1.0.conv1 to A8W2 and layer layer2.2.conv1 to A4W4 p=0.7934249043464661
perturb layer layer1.0.conv1 to A8W2 and layer layer2.2.conv1 to A4W8 p=0.7512035369873047
perturb layer layer1.0.conv1 to A8W2 and layer layer2.2.conv1 to A8W2 p=1.4260215014219284
perturb layer layer1.0.conv1 to A8W2 and layer layer2.2.conv1 to A8W4 p=0.7789623886346817
perturb layer layer1.0.conv1 to A8W2 and layer layer2.2.conv1 to A8W8 p=0.7359365075826645
perturb layer layer1.0.conv1 to A8W4 and layer layer2.2.conv1 to A2W2 p=1.099361002445221
perturb layer layer1.0.conv1 to A8W4 and layer layer2.2.conv1 to A2W4 p=0.5067430436611176
perturb layer layer1.0.conv1 to A8W4 and layer layer2.2.conv1 to A2W8 p=0.4835561737418175
perturb layer layer1.0.conv1 to A8W4 and layer layer2.2.conv1 to A4W2 p=0.4247005805373192
perturb layer layer1.0.conv1 to A8W4 and layer layer2.2.conv1 to A4W4 p=0.01468975841999054
perturb layer layer1.0.conv1 to A8W4 and layer layer2.2.conv1 to A4W8 p=-0.0016745328903198242
perturb layer layer1.0.conv1 to A8W4 and layer layer2.2.conv1 to A8W2 p=0.4041561707854271
perturb layer layer1.0.conv1 to A8W4 and layer layer2.2.conv1 to A8W4 p=0.0038848891854286194
perturb layer layer1.0.conv1 to A8W4 and layer layer2.2.conv1 to A8W8 p=-0.012319773435592651
perturb layer layer1.0.conv1 to A8W8 and layer layer2.2.conv1 to A2W2 p=1.1299621909856796
perturb layer layer1.0.conv1 to A8W8 and layer layer2.2.conv1 to A2W4 p=0.5345268845558167
perturb layer layer1.0.conv1 to A8W8 and layer layer2.2.conv1 to A2W8 p=0.5066408589482307
perturb layer layer1.0.conv1 to A8W8 and layer layer2.2.conv1 to A4W2 p=0.42480340600013733
perturb layer layer1.0.conv1 to A8W8 and layer layer2.2.conv1 to A4W4 p=0.03584328293800354
perturb layer layer1.0.conv1 to A8W8 and layer layer2.2.conv1 to A4W8 p=0.015140458941459656
perturb layer layer1.0.conv1 to A8W8 and layer layer2.2.conv1 to A8W2 p=0.40197645127773285
perturb layer layer1.0.conv1 to A8W8 and layer layer2.2.conv1 to A8W4 p=0.02159246802330017
perturb layer layer1.0.conv1 to A8W8 and layer layer2.2.conv1 to A8W8 p=0.0038974061608314514
perturb layer layer1.0.conv1 to A2W2 and layer layer2.2.conv2 to A2W2 p=2.374059036374092
perturb layer layer1.0.conv1 to A2W2 and layer layer2.2.conv2 to A2W4 p=2.135549321770668
perturb layer layer1.0.conv1 to A2W2 and layer layer2.2.conv2 to A2W8 p=2.1182622760534286
perturb layer layer1.0.conv1 to A2W2 and layer layer2.2.conv2 to A4W2 p=2.339852914214134
perturb layer layer1.0.conv1 to A2W2 and layer layer2.2.conv2 to A4W4 p=2.0904033333063126
perturb layer layer1.0.conv1 to A2W2 and layer layer2.2.conv2 to A4W8 p=2.0724711269140244
perturb layer layer1.0.conv1 to A2W2 and layer layer2.2.conv2 to A8W2 p=2.3358934968709946
perturb layer layer1.0.conv1 to A2W2 and layer layer2.2.conv2 to A8W4 p=2.08440063893795
perturb layer layer1.0.conv1 to A2W2 and layer layer2.2.conv2 to A8W8 p=2.067198798060417
perturb layer layer1.0.conv1 to A2W4 and layer layer2.2.conv2 to A2W2 p=1.3920159190893173
perturb layer layer1.0.conv1 to A2W4 and layer layer2.2.conv2 to A2W4 p=1.2838406711816788
perturb layer layer1.0.conv1 to A2W4 and layer layer2.2.conv2 to A2W8 p=1.2943497151136398
perturb layer layer1.0.conv1 to A2W4 and layer layer2.2.conv2 to A4W2 p=1.3571463972330093
perturb layer layer1.0.conv1 to A2W4 and layer layer2.2.conv2 to A4W4 p=1.2461237609386444
perturb layer layer1.0.conv1 to A2W4 and layer layer2.2.conv2 to A4W8 p=1.257261961698532
perturb layer layer1.0.conv1 to A2W4 and layer layer2.2.conv2 to A8W2 p=1.3539319187402725
perturb layer layer1.0.conv1 to A2W4 and layer layer2.2.conv2 to A8W4 p=1.242772027850151
perturb layer layer1.0.conv1 to A2W4 and layer layer2.2.conv2 to A8W8 p=1.253401279449463
perturb layer layer1.0.conv1 to A2W8 and layer layer2.2.conv2 to A2W2 p=1.3445408195257187
perturb layer layer1.0.conv1 to A2W8 and layer layer2.2.conv2 to A2W4 p=1.2532919347286224
perturb layer layer1.0.conv1 to A2W8 and layer layer2.2.conv2 to A2W8 p=1.2644853293895721
perturb layer layer1.0.conv1 to A2W8 and layer layer2.2.conv2 to A4W2 p=1.314245119690895
perturb layer layer1.0.conv1 to A2W8 and layer layer2.2.conv2 to A4W4 p=1.2191673964262009
perturb layer layer1.0.conv1 to A2W8 and layer layer2.2.conv2 to A4W8 p=1.2294793277978897
perturb layer layer1.0.conv1 to A2W8 and layer layer2.2.conv2 to A8W2 p=1.3105978816747665
perturb layer layer1.0.conv1 to A2W8 and layer layer2.2.conv2 to A8W4 p=1.2159090638160706
perturb layer layer1.0.conv1 to A2W8 and layer layer2.2.conv2 to A8W8 p=1.2260073870420456
perturb layer layer1.0.conv1 to A4W2 and layer layer2.2.conv2 to A2W2 p=1.1444449126720428
perturb layer layer1.0.conv1 to A4W2 and layer layer2.2.conv2 to A2W4 p=0.9933923035860062
perturb layer layer1.0.conv1 to A4W2 and layer layer2.2.conv2 to A2W8 p=0.9801631420850754
perturb layer layer1.0.conv1 to A4W2 and layer layer2.2.conv2 to A4W2 p=1.129396766424179
perturb layer layer1.0.conv1 to A4W2 and layer layer2.2.conv2 to A4W4 p=0.9740490764379501
perturb layer layer1.0.conv1 to A4W2 and layer layer2.2.conv2 to A4W8 p=0.9601572155952454
perturb layer layer1.0.conv1 to A4W2 and layer layer2.2.conv2 to A8W2 p=1.1251428872346878
perturb layer layer1.0.conv1 to A4W2 and layer layer2.2.conv2 to A8W4 p=0.9697527289390564
perturb layer layer1.0.conv1 to A4W2 and layer layer2.2.conv2 to A8W8 p=0.955941379070282
perturb layer layer1.0.conv1 to A4W4 and layer layer2.2.conv2 to A2W2 p=0.28009597957134247
perturb layer layer1.0.conv1 to A4W4 and layer layer2.2.conv2 to A2W4 p=0.21688777208328247
perturb layer layer1.0.conv1 to A4W4 and layer layer2.2.conv2 to A2W8 p=0.21623842418193817
perturb layer layer1.0.conv1 to A4W4 and layer layer2.2.conv2 to A4W2 p=0.26508117467164993
perturb layer layer1.0.conv1 to A4W4 and layer layer2.2.conv2 to A4W4 p=0.20074132829904556
perturb layer layer1.0.conv1 to A4W4 and layer layer2.2.conv2 to A4W8 p=0.19997461885213852
perturb layer layer1.0.conv1 to A4W4 and layer layer2.2.conv2 to A8W2 p=0.26216842979192734
perturb layer layer1.0.conv1 to A4W4 and layer layer2.2.conv2 to A8W4 p=0.19878247380256653
perturb layer layer1.0.conv1 to A4W4 and layer layer2.2.conv2 to A8W8 p=0.19815031439065933
perturb layer layer1.0.conv1 to A4W8 and layer layer2.2.conv2 to A2W2 p=0.2826017588376999
perturb layer layer1.0.conv1 to A4W8 and layer layer2.2.conv2 to A2W4 p=0.22506755590438843
perturb layer layer1.0.conv1 to A4W8 and layer layer2.2.conv2 to A2W8 p=0.22500339150428772
perturb layer layer1.0.conv1 to A4W8 and layer layer2.2.conv2 to A4W2 p=0.2722621038556099
perturb layer layer1.0.conv1 to A4W8 and layer layer2.2.conv2 to A4W4 p=0.21423622965812683
perturb layer layer1.0.conv1 to A4W8 and layer layer2.2.conv2 to A4W8 p=0.21376017481088638
perturb layer layer1.0.conv1 to A4W8 and layer layer2.2.conv2 to A8W2 p=0.2709346041083336
perturb layer layer1.0.conv1 to A4W8 and layer layer2.2.conv2 to A8W4 p=0.21280869096517563
perturb layer layer1.0.conv1 to A4W8 and layer layer2.2.conv2 to A8W8 p=0.21239280700683594
perturb layer layer1.0.conv1 to A8W2 and layer layer2.2.conv2 to A2W2 p=0.9096575826406479
perturb layer layer1.0.conv1 to A8W2 and layer layer2.2.conv2 to A2W4 p=0.7757308930158615
perturb layer layer1.0.conv1 to A8W2 and layer layer2.2.conv2 to A2W8 p=0.7649000585079193
perturb layer layer1.0.conv1 to A8W2 and layer layer2.2.conv2 to A4W2 p=0.8916637748479843
perturb layer layer1.0.conv1 to A8W2 and layer layer2.2.conv2 to A4W4 p=0.7510917782783508
perturb layer layer1.0.conv1 to A8W2 and layer layer2.2.conv2 to A4W8 p=0.7410337328910828
perturb layer layer1.0.conv1 to A8W2 and layer layer2.2.conv2 to A8W2 p=0.8888139724731445
perturb layer layer1.0.conv1 to A8W2 and layer layer2.2.conv2 to A8W4 p=0.747364416718483
perturb layer layer1.0.conv1 to A8W2 and layer layer2.2.conv2 to A8W8 p=0.7375343143939972
perturb layer layer1.0.conv1 to A8W4 and layer layer2.2.conv2 to A2W2 p=0.055202700197696686
perturb layer layer1.0.conv1 to A8W4 and layer layer2.2.conv2 to A2W4 p=0.0027919337153434753
perturb layer layer1.0.conv1 to A8W4 and layer layer2.2.conv2 to A2W8 p=0.0037470608949661255
perturb layer layer1.0.conv1 to A8W4 and layer layer2.2.conv2 to A4W2 p=0.0399051234126091
perturb layer layer1.0.conv1 to A8W4 and layer layer2.2.conv2 to A4W4 p=-0.013155348598957062
perturb layer layer1.0.conv1 to A8W4 and layer layer2.2.conv2 to A4W8 p=-0.012301549315452576
perturb layer layer1.0.conv1 to A8W4 and layer layer2.2.conv2 to A8W2 p=0.03977643698453903
perturb layer layer1.0.conv1 to A8W4 and layer layer2.2.conv2 to A8W4 p=-0.013102777302265167
perturb layer layer1.0.conv1 to A8W4 and layer layer2.2.conv2 to A8W8 p=-0.012412391602993011
perturb layer layer1.0.conv1 to A8W8 and layer layer2.2.conv2 to A2W2 p=0.06215175241231918
perturb layer layer1.0.conv1 to A8W8 and layer layer2.2.conv2 to A2W4 p=0.01519746333360672
perturb layer layer1.0.conv1 to A8W8 and layer layer2.2.conv2 to A2W8 p=0.015517495572566986
perturb layer layer1.0.conv1 to A8W8 and layer layer2.2.conv2 to A4W2 p=0.05128795653581619
perturb layer layer1.0.conv1 to A8W8 and layer layer2.2.conv2 to A4W4 p=0.0036030933260917664
perturb layer layer1.0.conv1 to A8W8 and layer layer2.2.conv2 to A4W8 p=0.0037915706634521484
perturb layer layer1.0.conv1 to A8W8 and layer layer2.2.conv2 to A8W2 p=0.05078175663948059
perturb layer layer1.0.conv1 to A8W8 and layer layer2.2.conv2 to A8W4 p=0.002902284264564514
perturb layer layer1.0.conv1 to A8W8 and layer layer2.2.conv2 to A8W8 p=0.0031102076172828674
perturb layer layer1.0.conv1 to A2W2 and layer layer2.3.conv1 to A2W2 p=3.674941822886467
perturb layer layer1.0.conv1 to A2W2 and layer layer2.3.conv1 to A2W4 p=2.9008499830961227
perturb layer layer1.0.conv1 to A2W2 and layer layer2.3.conv1 to A2W8 p=2.782653048634529
perturb layer layer1.0.conv1 to A2W2 and layer layer2.3.conv1 to A4W2 p=2.883449986577034
perturb layer layer1.0.conv1 to A2W2 and layer layer2.3.conv1 to A4W4 p=2.1854055672883987
perturb layer layer1.0.conv1 to A2W2 and layer layer2.3.conv1 to A4W8 p=2.0869211107492447
perturb layer layer1.0.conv1 to A2W2 and layer layer2.3.conv1 to A8W2 p=2.8678310364484787
perturb layer layer1.0.conv1 to A2W2 and layer layer2.3.conv1 to A8W4 p=2.165043070912361
perturb layer layer1.0.conv1 to A2W2 and layer layer2.3.conv1 to A8W8 p=2.0678407698869705
perturb layer layer1.0.conv1 to A2W4 and layer layer2.3.conv1 to A2W2 p=2.9595460444688797
perturb layer layer1.0.conv1 to A2W4 and layer layer2.3.conv1 to A2W4 p=2.279523566365242
perturb layer layer1.0.conv1 to A2W4 and layer layer2.3.conv1 to A2W8 p=2.1802893728017807
perturb layer layer1.0.conv1 to A2W4 and layer layer2.3.conv1 to A4W2 p=1.9702323526144028
perturb layer layer1.0.conv1 to A2W4 and layer layer2.3.conv1 to A4W4 p=1.3743483871221542
perturb layer layer1.0.conv1 to A2W4 and layer layer2.3.conv1 to A4W8 p=1.303022637963295
perturb layer layer1.0.conv1 to A2W4 and layer layer2.3.conv1 to A8W2 p=1.9138641506433487
perturb layer layer1.0.conv1 to A2W4 and layer layer2.3.conv1 to A8W4 p=1.3224074989557266
perturb layer layer1.0.conv1 to A2W4 and layer layer2.3.conv1 to A8W8 p=1.253613457083702
perturb layer layer1.0.conv1 to A2W8 and layer layer2.3.conv1 to A2W2 p=2.917640671133995
perturb layer layer1.0.conv1 to A2W8 and layer layer2.3.conv1 to A2W4 p=2.241666153073311
perturb layer layer1.0.conv1 to A2W8 and layer layer2.3.conv1 to A2W8 p=2.153725728392601
perturb layer layer1.0.conv1 to A2W8 and layer layer2.3.conv1 to A4W2 p=1.8980859369039536
perturb layer layer1.0.conv1 to A2W8 and layer layer2.3.conv1 to A4W4 p=1.3318390995264053
perturb layer layer1.0.conv1 to A2W8 and layer layer2.3.conv1 to A4W8 p=1.2708302289247513
perturb layer layer1.0.conv1 to A2W8 and layer layer2.3.conv1 to A8W2 p=1.8442182689905167
perturb layer layer1.0.conv1 to A2W8 and layer layer2.3.conv1 to A8W4 p=1.2845009565353394
perturb layer layer1.0.conv1 to A2W8 and layer layer2.3.conv1 to A8W8 p=1.2259383350610733
perturb layer layer1.0.conv1 to A4W2 and layer layer2.3.conv1 to A2W2 p=2.141038343310356
perturb layer layer1.0.conv1 to A4W2 and layer layer2.3.conv1 to A2W4 p=1.5402824729681015
perturb layer layer1.0.conv1 to A4W2 and layer layer2.3.conv1 to A2W8 p=1.451250597834587
perturb layer layer1.0.conv1 to A4W2 and layer layer2.3.conv1 to A4W2 p=1.488385260105133
perturb layer layer1.0.conv1 to A4W2 and layer layer2.3.conv1 to A4W4 p=1.0308385342359543
perturb layer layer1.0.conv1 to A4W2 and layer layer2.3.conv1 to A4W8 p=0.9694388508796692
perturb layer layer1.0.conv1 to A4W2 and layer layer2.3.conv1 to A8W2 p=1.4716651439666748
perturb layer layer1.0.conv1 to A4W2 and layer layer2.3.conv1 to A8W4 p=1.0147845149040222
perturb layer layer1.0.conv1 to A4W2 and layer layer2.3.conv1 to A8W8 p=0.9547946900129318
perturb layer layer1.0.conv1 to A4W4 and layer layer2.3.conv1 to A2W2 p=1.3525360822677612
perturb layer layer1.0.conv1 to A4W4 and layer layer2.3.conv1 to A2W4 p=0.837185874581337
perturb layer layer1.0.conv1 to A4W4 and layer layer2.3.conv1 to A2W8 p=0.7768601775169373
perturb layer layer1.0.conv1 to A4W4 and layer layer2.3.conv1 to A4W2 p=0.642696276307106
perturb layer layer1.0.conv1 to A4W4 and layer layer2.3.conv1 to A4W4 p=0.2563861533999443
perturb layer layer1.0.conv1 to A4W4 and layer layer2.3.conv1 to A4W8 p=0.2173953354358673
perturb layer layer1.0.conv1 to A4W4 and layer layer2.3.conv1 to A8W2 p=0.607624888420105
perturb layer layer1.0.conv1 to A4W4 and layer layer2.3.conv1 to A8W4 p=0.23345699161291122
perturb layer layer1.0.conv1 to A4W4 and layer layer2.3.conv1 to A8W8 p=0.19869543612003326
perturb layer layer1.0.conv1 to A4W8 and layer layer2.3.conv1 to A2W2 p=1.385601818561554
perturb layer layer1.0.conv1 to A4W8 and layer layer2.3.conv1 to A2W4 p=0.8416820019483566
perturb layer layer1.0.conv1 to A4W8 and layer layer2.3.conv1 to A2W8 p=0.7887649685144424
perturb layer layer1.0.conv1 to A4W8 and layer layer2.3.conv1 to A4W2 p=0.6703600734472275
perturb layer layer1.0.conv1 to A4W8 and layer layer2.3.conv1 to A4W4 p=0.2762128561735153
perturb layer layer1.0.conv1 to A4W8 and layer layer2.3.conv1 to A4W8 p=0.24178821593523026
perturb layer layer1.0.conv1 to A4W8 and layer layer2.3.conv1 to A8W2 p=0.6235468238592148
perturb layer layer1.0.conv1 to A4W8 and layer layer2.3.conv1 to A8W4 p=0.24424012750387192
perturb layer layer1.0.conv1 to A4W8 and layer layer2.3.conv1 to A8W8 p=0.21255888044834137
perturb layer layer1.0.conv1 to A8W2 and layer layer2.3.conv1 to A2W2 p=1.9033614844083786
perturb layer layer1.0.conv1 to A8W2 and layer layer2.3.conv1 to A2W4 p=1.3260677754878998
perturb layer layer1.0.conv1 to A8W2 and layer layer2.3.conv1 to A2W8 p=1.2473098933696747
perturb layer layer1.0.conv1 to A8W2 and layer layer2.3.conv1 to A4W2 p=1.240440234541893
perturb layer layer1.0.conv1 to A8W2 and layer layer2.3.conv1 to A4W4 p=0.801315575838089
perturb layer layer1.0.conv1 to A8W2 and layer layer2.3.conv1 to A4W8 p=0.7459865808486938
perturb layer layer1.0.conv1 to A8W2 and layer layer2.3.conv1 to A8W2 p=1.2241415828466415
perturb layer layer1.0.conv1 to A8W2 and layer layer2.3.conv1 to A8W4 p=0.7924144417047501
perturb layer layer1.0.conv1 to A8W2 and layer layer2.3.conv1 to A8W8 p=0.7387436777353287
perturb layer layer1.0.conv1 to A8W4 and layer layer2.3.conv1 to A2W2 p=0.9999176859855652
perturb layer layer1.0.conv1 to A8W4 and layer layer2.3.conv1 to A2W4 p=0.5549149289727211
perturb layer layer1.0.conv1 to A8W4 and layer layer2.3.conv1 to A2W8 p=0.5030574798583984
perturb layer layer1.0.conv1 to A8W4 and layer layer2.3.conv1 to A4W2 p=0.3511941060423851
perturb layer layer1.0.conv1 to A8W4 and layer layer2.3.conv1 to A4W4 p=0.02777688205242157
perturb layer layer1.0.conv1 to A8W4 and layer layer2.3.conv1 to A4W8 p=-0.0010433867573738098
perturb layer layer1.0.conv1 to A8W4 and layer layer2.3.conv1 to A8W2 p=0.32611776888370514
perturb layer layer1.0.conv1 to A8W4 and layer layer2.3.conv1 to A8W4 p=0.01411670446395874
perturb layer layer1.0.conv1 to A8W4 and layer layer2.3.conv1 to A8W8 p=-0.01201213151216507
perturb layer layer1.0.conv1 to A8W8 and layer layer2.3.conv1 to A2W2 p=1.020831659436226
perturb layer layer1.0.conv1 to A8W8 and layer layer2.3.conv1 to A2W4 p=0.5613742917776108
perturb layer layer1.0.conv1 to A8W8 and layer layer2.3.conv1 to A2W8 p=0.5130991414189339
perturb layer layer1.0.conv1 to A8W8 and layer layer2.3.conv1 to A4W2 p=0.3743616044521332
perturb layer layer1.0.conv1 to A8W8 and layer layer2.3.conv1 to A4W4 p=0.04272142052650452
perturb layer layer1.0.conv1 to A8W8 and layer layer2.3.conv1 to A4W8 p=0.017672643065452576
perturb layer layer1.0.conv1 to A8W8 and layer layer2.3.conv1 to A8W2 p=0.3443395420908928
perturb layer layer1.0.conv1 to A8W8 and layer layer2.3.conv1 to A8W4 p=0.026322737336158752
perturb layer layer1.0.conv1 to A8W8 and layer layer2.3.conv1 to A8W8 p=0.003271736204624176
perturb layer layer1.0.conv1 to A2W2 and layer layer2.3.conv2 to A2W2 p=2.106916978955269
perturb layer layer1.0.conv1 to A2W2 and layer layer2.3.conv2 to A2W4 p=2.0473850816488266
perturb layer layer1.0.conv1 to A2W2 and layer layer2.3.conv2 to A2W8 p=2.0914221853017807
perturb layer layer1.0.conv1 to A2W2 and layer layer2.3.conv2 to A4W2 p=2.086964502930641
perturb layer layer1.0.conv1 to A2W2 and layer layer2.3.conv2 to A4W4 p=2.0254793614149094
perturb layer layer1.0.conv1 to A2W2 and layer layer2.3.conv2 to A4W8 p=2.071225866675377
perturb layer layer1.0.conv1 to A2W2 and layer layer2.3.conv2 to A8W2 p=2.084566906094551
perturb layer layer1.0.conv1 to A2W2 and layer layer2.3.conv2 to A8W4 p=2.0219738632440567
perturb layer layer1.0.conv1 to A2W2 and layer layer2.3.conv2 to A8W8 p=2.0677047222852707
perturb layer layer1.0.conv1 to A2W4 and layer layer2.3.conv2 to A2W2 p=1.264264076948166
perturb layer layer1.0.conv1 to A2W4 and layer layer2.3.conv2 to A2W4 p=1.2583176642656326
perturb layer layer1.0.conv1 to A2W4 and layer layer2.3.conv2 to A2W8 p=1.282652348279953
perturb layer layer1.0.conv1 to A2W4 and layer layer2.3.conv2 to A4W2 p=1.239286184310913
perturb layer layer1.0.conv1 to A2W4 and layer layer2.3.conv2 to A4W4 p=1.2304613292217255
perturb layer layer1.0.conv1 to A2W4 and layer layer2.3.conv2 to A4W8 p=1.2557857930660248
perturb layer layer1.0.conv1 to A2W4 and layer layer2.3.conv2 to A8W2 p=1.236352801322937
perturb layer layer1.0.conv1 to A2W4 and layer layer2.3.conv2 to A8W4 p=1.2276671379804611
perturb layer layer1.0.conv1 to A2W4 and layer layer2.3.conv2 to A8W8 p=1.2531269788742065
perturb layer layer1.0.conv1 to A2W8 and layer layer2.3.conv2 to A2W2 p=1.2315439134836197
perturb layer layer1.0.conv1 to A2W8 and layer layer2.3.conv2 to A2W4 p=1.2286788076162338
perturb layer layer1.0.conv1 to A2W8 and layer layer2.3.conv2 to A2W8 p=1.2532185763120651
perturb layer layer1.0.conv1 to A2W8 and layer layer2.3.conv2 to A4W2 p=1.2028263807296753
perturb layer layer1.0.conv1 to A2W8 and layer layer2.3.conv2 to A4W4 p=1.201332464814186
perturb layer layer1.0.conv1 to A2W8 and layer layer2.3.conv2 to A4W8 p=1.2268240600824356
perturb layer layer1.0.conv1 to A2W8 and layer layer2.3.conv2 to A8W2 p=1.2024869471788406
perturb layer layer1.0.conv1 to A2W8 and layer layer2.3.conv2 to A8W4 p=1.200293317437172
perturb layer layer1.0.conv1 to A2W8 and layer layer2.3.conv2 to A8W8 p=1.2259840369224548
perturb layer layer1.0.conv1 to A4W2 and layer layer2.3.conv2 to A2W2 p=0.9839683771133423
perturb layer layer1.0.conv1 to A4W2 and layer layer2.3.conv2 to A2W4 p=0.9455315321683884
perturb layer layer1.0.conv1 to A4W2 and layer layer2.3.conv2 to A2W8 p=0.9733656346797943
perturb layer layer1.0.conv1 to A4W2 and layer layer2.3.conv2 to A4W2 p=0.970909371972084
perturb layer layer1.0.conv1 to A4W2 and layer layer2.3.conv2 to A4W4 p=0.9288727045059204
perturb layer layer1.0.conv1 to A4W2 and layer layer2.3.conv2 to A4W8 p=0.9580496847629547
perturb layer layer1.0.conv1 to A4W2 and layer layer2.3.conv2 to A8W2 p=0.9693974256515503
perturb layer layer1.0.conv1 to A4W2 and layer layer2.3.conv2 to A8W4 p=0.9267988353967667
perturb layer layer1.0.conv1 to A4W2 and layer layer2.3.conv2 to A8W8 p=0.9559396058320999
perturb layer layer1.0.conv1 to A4W4 and layer layer2.3.conv2 to A2W2 p=0.21803149580955505
perturb layer layer1.0.conv1 to A4W4 and layer layer2.3.conv2 to A2W4 p=0.2083859071135521
perturb layer layer1.0.conv1 to A4W4 and layer layer2.3.conv2 to A2W8 p=0.22357647120952606
perturb layer layer1.0.conv1 to A4W4 and layer layer2.3.conv2 to A4W2 p=0.19935950636863708
perturb layer layer1.0.conv1 to A4W4 and layer layer2.3.conv2 to A4W4 p=0.18709981441497803
perturb layer layer1.0.conv1 to A4W4 and layer layer2.3.conv2 to A4W8 p=0.2016344591975212
perturb layer layer1.0.conv1 to A4W4 and layer layer2.3.conv2 to A8W2 p=0.1969614326953888
perturb layer layer1.0.conv1 to A4W4 and layer layer2.3.conv2 to A8W4 p=0.18388333171606064
perturb layer layer1.0.conv1 to A4W4 and layer layer2.3.conv2 to A8W8 p=0.19824574142694473
perturb layer layer1.0.conv1 to A4W8 and layer layer2.3.conv2 to A2W2 p=0.23444534093141556
perturb layer layer1.0.conv1 to A4W8 and layer layer2.3.conv2 to A2W4 p=0.22449184209108353
perturb layer layer1.0.conv1 to A4W8 and layer layer2.3.conv2 to A2W8 p=0.2391030341386795
perturb layer layer1.0.conv1 to A4W8 and layer layer2.3.conv2 to A4W2 p=0.2131260186433792
perturb layer layer1.0.conv1 to A4W8 and layer layer2.3.conv2 to A4W4 p=0.20100446790456772
perturb layer layer1.0.conv1 to A4W8 and layer layer2.3.conv2 to A4W8 p=0.21522128582000732
perturb layer layer1.0.conv1 to A4W8 and layer layer2.3.conv2 to A8W2 p=0.2122696116566658
perturb layer layer1.0.conv1 to A4W8 and layer layer2.3.conv2 to A8W4 p=0.19835082441568375
perturb layer layer1.0.conv1 to A4W8 and layer layer2.3.conv2 to A8W8 p=0.212312214076519
perturb layer layer1.0.conv1 to A8W2 and layer layer2.3.conv2 to A2W2 p=0.7833810150623322
perturb layer layer1.0.conv1 to A8W2 and layer layer2.3.conv2 to A2W4 p=0.7353244125843048
perturb layer layer1.0.conv1 to A8W2 and layer layer2.3.conv2 to A2W8 p=0.7555085271596909
perturb layer layer1.0.conv1 to A8W2 and layer layer2.3.conv2 to A4W2 p=0.7717364281415939
perturb layer layer1.0.conv1 to A8W2 and layer layer2.3.conv2 to A4W4 p=0.7191829085350037
