{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f1149d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,torchvision,os,time\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from utils.util import get_loader,evaluate\n",
    "from utils.layer import qConv2d,qLinear\n",
    "from utils.train import QAVAT_train\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models             \n",
    "\n",
    "from mqbench.prepare_by_platform import prepare_by_platform   # add quant nodes for specific Backend\n",
    "from mqbench.prepare_by_platform import BackendType           # contain various Backend, like TensorRT, NNIE, etc.\n",
    "from mqbench.utils.state import enable_calibration            # turn on calibration algorithm, determine scale, zero_point, etc.\n",
    "from mqbench.utils.state import enable_quantization           # turn on actually quantization, like FP32 -> INT8\n",
    "from mqbench.utils.state import disable_all           \n",
    "from copy import deepcopy\n",
    "from mqbench.advanced_ptq import ptq_reconstruction\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a4e0889",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "def evaluate(val_loader, model,\n",
    "             criterion = torch.nn.CrossEntropyLoss().cuda(),device='cuda'):\n",
    "    s_time = time.time()\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    count,top1,top5,losses = 0,0,0,0\n",
    "    with torch.no_grad():\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            images, target = images.to(device), target.to(device)\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            losses = losses * count/(count+images.size(0)) + loss * images.size(0)/(count+images.size(0))\n",
    "            top1 = top1 * count/(count+images.size(0)) + acc1 * images.size(0)/(count+images.size(0))\n",
    "            top5 = top5 * count/(count+images.size(0)) + acc5 * images.size(0)/(count+images.size(0))\n",
    "            count += images.size(0)\n",
    "    test_time = time.time() - s_time\n",
    "    \n",
    "    return {'top1':top1,'top5':top5,'loss':losses,'time':test_time}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2109f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /homes/zdeng/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "adv_ptq = False\n",
    "dataset = 'cifar100'\n",
    "modelname = 'resnet56'\n",
    "mn = dataset.lower()+ '_' + modelname\n",
    "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", mn, pretrained=True).cuda()\n",
    "# model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar100_mobilenetv2_x0_5\", pretrained=True).cuda()\n",
    "model.eval()\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "train,test = get_loader(dataset.upper(),batch_size=128,test_batch_size=128)\n",
    "train.num_workers = 2\n",
    "test.num_workers = 2\n",
    "train.pin_in_memory = True\n",
    "test.pin_in_memory = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84d104e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'top1': tensor([72.6300], device='cuda:0'),\n",
       " 'top5': tensor([91.9400], device='cuda:0'),\n",
       " 'loss': tensor(1.2946, device='cuda:0'),\n",
       " 'time': 2.0885140895843506}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(test,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f67af0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibration data used to calibrate PTQ and MPQ\n",
    "calib_data = []\n",
    "stacked_tensor = []\n",
    "calib_fp_output = []\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    for img,label in train:\n",
    "        i += 1\n",
    "        stacked_tensor.append(img)\n",
    "        calib_data.append((img,label))\n",
    "        calib_fp_output.append(model(img.cuda()))\n",
    "        if i == 32: # remember to modify this to get 1024 images\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3d31b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CifarResNet(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (6): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (7): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (8): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (6): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (7): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (8): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (6): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (7): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (8): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=64, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MPQ_scheme = (2,4,8)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b007f436",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare 2bits model using MQBench\n",
      "[MQBENCH] INFO: Quantize model Scheme: BackendType.Academic Mode: Eval\n",
      "[MQBENCH] INFO: Weight Qconfig:\n",
      "    FakeQuantize: FixedFakeQuantize Params: {}\n",
      "    Oberver:      MSEObserver Params: Symmetric: True / Bitwidth: 2 / Per channel: False / Pot scale: False / Extra kwargs: {}\n",
      "[MQBENCH] INFO: Activation Qconfig:\n",
      "    FakeQuantize: FixedFakeQuantize Params: {}\n",
      "    Oberver:      EMAMSEObserver Params: Symmetric: False / Bitwidth: 8 / Per channel: False / Pot scale: False / Extra kwargs: {}\n",
      "[MQBENCH] INFO: Replace module to qat module.\n",
      "[MQBENCH] INFO: Set layer conv1 to 8 bit.\n",
      "[MQBENCH] INFO: Set layer fc to 8 bit.\n",
      "dbg node_to_quantize_output\n",
      " odict_keys([x, relu, layer1_0_relu, layer1_0_relu_1, layer1_1_relu, layer1_1_relu_1, layer1_2_relu, layer1_2_relu_1, layer1_3_relu, layer1_3_relu_1, layer1_4_relu, layer1_4_relu_1, layer1_5_relu, layer1_5_relu_1, layer1_6_relu, layer1_6_relu_1, layer1_7_relu, layer1_7_relu_1, layer1_8_relu, layer1_8_relu_1, layer2_0_relu, layer2_0_relu_1, layer2_1_relu, layer2_1_relu_1, layer2_2_relu, layer2_2_relu_1, layer2_3_relu, layer2_3_relu_1, layer2_4_relu, layer2_4_relu_1, layer2_5_relu, layer2_5_relu_1, layer2_6_relu, layer2_6_relu_1, layer2_7_relu, layer2_7_relu_1, layer2_8_relu, layer2_8_relu_1, layer3_0_relu, layer3_0_relu_1, layer3_1_relu, layer3_1_relu_1, layer3_2_relu, layer3_2_relu_1, layer3_3_relu, layer3_3_relu_1, layer3_4_relu, layer3_4_relu_1, layer3_5_relu, layer3_5_relu_1, layer3_6_relu, layer3_6_relu_1, layer3_7_relu, layer3_7_relu_1, layer3_8_relu, view])\n",
      "[MQBENCH] INFO: Set x post act quantize to 8 bit.\n",
      "[MQBENCH] INFO: Insert act quant x_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_0_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_0_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_1_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_1_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_2_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_2_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_3_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_3_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_4_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_4_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_5_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_5_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_6_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_6_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_7_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_7_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_8_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_8_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_0_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_0_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_1_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_1_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_2_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_2_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_3_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_3_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_4_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_4_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_5_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_5_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_6_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_6_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_7_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_7_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_8_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_8_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_0_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_0_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_1_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_1_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_2_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_2_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_3_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_3_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_4_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_4_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_5_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_5_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_6_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_6_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_7_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_7_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_8_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Set view post act quantize to 8 bit.\n",
      "[MQBENCH] INFO: Insert act quant view_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Enable observer and Disable quantize.\n",
      "Prepare 4bits model using MQBench\n",
      "[MQBENCH] INFO: Quantize model Scheme: BackendType.Academic Mode: Eval\n",
      "[MQBENCH] INFO: Weight Qconfig:\n",
      "    FakeQuantize: FixedFakeQuantize Params: {}\n",
      "    Oberver:      MSEObserver Params: Symmetric: True / Bitwidth: 4 / Per channel: False / Pot scale: False / Extra kwargs: {}\n",
      "[MQBENCH] INFO: Activation Qconfig:\n",
      "    FakeQuantize: FixedFakeQuantize Params: {}\n",
      "    Oberver:      EMAMSEObserver Params: Symmetric: False / Bitwidth: 8 / Per channel: False / Pot scale: False / Extra kwargs: {}\n",
      "[MQBENCH] INFO: Replace module to qat module.\n",
      "[MQBENCH] INFO: Set layer conv1 to 8 bit.\n",
      "[MQBENCH] INFO: Set layer fc to 8 bit.\n",
      "dbg node_to_quantize_output\n",
      " odict_keys([x, relu, layer1_0_relu, layer1_0_relu_1, layer1_1_relu, layer1_1_relu_1, layer1_2_relu, layer1_2_relu_1, layer1_3_relu, layer1_3_relu_1, layer1_4_relu, layer1_4_relu_1, layer1_5_relu, layer1_5_relu_1, layer1_6_relu, layer1_6_relu_1, layer1_7_relu, layer1_7_relu_1, layer1_8_relu, layer1_8_relu_1, layer2_0_relu, layer2_0_relu_1, layer2_1_relu, layer2_1_relu_1, layer2_2_relu, layer2_2_relu_1, layer2_3_relu, layer2_3_relu_1, layer2_4_relu, layer2_4_relu_1, layer2_5_relu, layer2_5_relu_1, layer2_6_relu, layer2_6_relu_1, layer2_7_relu, layer2_7_relu_1, layer2_8_relu, layer2_8_relu_1, layer3_0_relu, layer3_0_relu_1, layer3_1_relu, layer3_1_relu_1, layer3_2_relu, layer3_2_relu_1, layer3_3_relu, layer3_3_relu_1, layer3_4_relu, layer3_4_relu_1, layer3_5_relu, layer3_5_relu_1, layer3_6_relu, layer3_6_relu_1, layer3_7_relu, layer3_7_relu_1, layer3_8_relu, view])\n",
      "[MQBENCH] INFO: Set x post act quantize to 8 bit.\n",
      "[MQBENCH] INFO: Insert act quant x_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_0_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_0_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_1_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_1_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_2_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_2_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_3_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_3_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_4_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_4_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_5_relu_post_act_fake_quantizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MQBENCH] INFO: Insert act quant layer1_5_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_6_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_6_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_7_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_7_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_8_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_8_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_0_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_0_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_1_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_1_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_2_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_2_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_3_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_3_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_4_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_4_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_5_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_5_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_6_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_6_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_7_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_7_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_8_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_8_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_0_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_0_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_1_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_1_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_2_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_2_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_3_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_3_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_4_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_4_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_5_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_5_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_6_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_6_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_7_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_7_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_8_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Set view post act quantize to 8 bit.\n",
      "[MQBENCH] INFO: Insert act quant view_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Enable observer and Disable quantize.\n",
      "Prepare 8bits model using MQBench\n",
      "[MQBENCH] INFO: Quantize model Scheme: BackendType.Academic Mode: Eval\n",
      "[MQBENCH] INFO: Weight Qconfig:\n",
      "    FakeQuantize: FixedFakeQuantize Params: {}\n",
      "    Oberver:      MSEObserver Params: Symmetric: True / Bitwidth: 8 / Per channel: False / Pot scale: False / Extra kwargs: {}\n",
      "[MQBENCH] INFO: Activation Qconfig:\n",
      "    FakeQuantize: FixedFakeQuantize Params: {}\n",
      "    Oberver:      EMAMSEObserver Params: Symmetric: False / Bitwidth: 8 / Per channel: False / Pot scale: False / Extra kwargs: {}\n",
      "[MQBENCH] INFO: Replace module to qat module.\n",
      "[MQBENCH] INFO: Set layer conv1 to 8 bit.\n",
      "[MQBENCH] INFO: Set layer fc to 8 bit.\n",
      "dbg node_to_quantize_output\n",
      " odict_keys([x, relu, layer1_0_relu, layer1_0_relu_1, layer1_1_relu, layer1_1_relu_1, layer1_2_relu, layer1_2_relu_1, layer1_3_relu, layer1_3_relu_1, layer1_4_relu, layer1_4_relu_1, layer1_5_relu, layer1_5_relu_1, layer1_6_relu, layer1_6_relu_1, layer1_7_relu, layer1_7_relu_1, layer1_8_relu, layer1_8_relu_1, layer2_0_relu, layer2_0_relu_1, layer2_1_relu, layer2_1_relu_1, layer2_2_relu, layer2_2_relu_1, layer2_3_relu, layer2_3_relu_1, layer2_4_relu, layer2_4_relu_1, layer2_5_relu, layer2_5_relu_1, layer2_6_relu, layer2_6_relu_1, layer2_7_relu, layer2_7_relu_1, layer2_8_relu, layer2_8_relu_1, layer3_0_relu, layer3_0_relu_1, layer3_1_relu, layer3_1_relu_1, layer3_2_relu, layer3_2_relu_1, layer3_3_relu, layer3_3_relu_1, layer3_4_relu, layer3_4_relu_1, layer3_5_relu, layer3_5_relu_1, layer3_6_relu, layer3_6_relu_1, layer3_7_relu, layer3_7_relu_1, layer3_8_relu, view])\n",
      "[MQBENCH] INFO: Set x post act quantize to 8 bit.\n",
      "[MQBENCH] INFO: Insert act quant x_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_0_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_0_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_1_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_1_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_2_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_2_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_3_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_3_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_4_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_4_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_5_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_5_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_6_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_6_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_7_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_7_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_8_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer1_8_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_0_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_0_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_1_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_1_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_2_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_2_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_3_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_3_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_4_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_4_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_5_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_5_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_6_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_6_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_7_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_7_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_8_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer2_8_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_0_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_0_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_1_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_1_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_2_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_2_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_3_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_3_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_4_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_4_relu_1_post_act_fake_quantizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MQBENCH] INFO: Insert act quant layer3_5_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_5_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_6_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_6_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_7_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_7_relu_1_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Insert act quant layer3_8_relu_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Set view post act quantize to 8 bit.\n",
      "[MQBENCH] INFO: Insert act quant view_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Enable observer and Disable quantize.\n"
     ]
    }
   ],
   "source": [
    "# configuration\n",
    "ptq_reconstruction_config_init = {\n",
    "    'pattern': 'block',                   #? 'layer' for Adaround or 'block' for BRECQ and QDROP\n",
    "    'scale_lr': 4.0e-5,                   #? learning rate for learning step size of activation\n",
    "    'warm_up': 0.2,                       #? 0.2 * max_count iters without regularization to floor or ceil\n",
    "    'weight': 0.01,                       #? loss weight for regularization item\n",
    "    'max_count': 1,                   #? optimization iteration\n",
    "    'b_range': [20,2],                    #? beta decaying range\n",
    "    'keep_gpu': True,                     #? calibration data restore in gpu or cpu\n",
    "    'round_mode': 'learned_hard_sigmoid', #? ways to reconstruct the weight, currently only support learned_hard_sigmoid\n",
    "    'prob': 0.5,                          #? dropping probability of QDROP, 1.0 for Adaround and BRECQ\n",
    "}\n",
    "\n",
    "\n",
    "ptq_reconstruction_config = {\n",
    "    'pattern': 'block',                   #? 'layer' for Adaround or 'block' for BRECQ and QDROP\n",
    "    'scale_lr': 4.0e-5,                   #? learning rate for learning step size of activation\n",
    "    'warm_up': 0.2,                       #? 0.2 * max_count iters without regularization to floor or ceil\n",
    "    'weight': 0.01,                       #? loss weight for regularization item\n",
    "    'max_count': 20000,                   #? optimization iteration\n",
    "    'b_range': [20,2],                    #? beta decaying range\n",
    "    'keep_gpu': True,                     #? calibration data restore in gpu or cpu\n",
    "    'round_mode': 'learned_hard_sigmoid', #? ways to reconstruct the weight, currently only support learned_hard_sigmoid\n",
    "    'prob': 0.5,                          #? dropping probability of QDROP, 1.0 for Adaround and BRECQ\n",
    "}\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "ptq_reconstruction_config = dotdict(ptq_reconstruction_config)\n",
    "ptq_reconstruction_config_init = dotdict(ptq_reconstruction_config_init)\n",
    "\n",
    "def getModuleByName(model,moduleName):\n",
    "    '''\n",
    "        replace module with name modelName.moduleName with newModule\n",
    "    '''\n",
    "    tokens = moduleName.split('.')\n",
    "    m = model\n",
    "    for tok in tokens:\n",
    "        m = getattr(m,tok)\n",
    "    return m\n",
    "\n",
    "for b in MPQ_scheme:\n",
    "    mqb_fp_model = deepcopy(model)\n",
    "    \n",
    "    # MSE calibration on model parameters\n",
    "    backend = BackendType.Academic\n",
    "    extra_config = {\n",
    "        'extra_qconfig_dict': {\n",
    "            'w_observer': 'MSEObserver',                              # custom weight observer\n",
    "            'a_observer': 'EMAMSEObserver',                              # custom activation observer\n",
    "            'w_fakequantize': 'AdaRoundFakeQuantize' if adv_ptq else 'FixedFakeQuantize',\n",
    "            'a_fakequantize': 'QDropFakeQuantize' if adv_ptq else 'FixedFakeQuantize',\n",
    "            'w_qscheme': {\n",
    "                'bit': b,                                             # custom bitwidth for weight,\n",
    "                'symmetry': True,                                    # custom whether quant is symmetric for weight,\n",
    "                'per_channel': False,                                  # custom whether quant is per-channel or per-tensor for weight,\n",
    "                'pot_scale': False,                                   # custom whether scale is power of two for weight.\n",
    "            },\n",
    "            'a_qscheme': {\n",
    "                'bit': 8,                                             # custom bitwidth for activation,\n",
    "                'symmetry': False,                                    # custom whether quant is symmetric for activation,\n",
    "                'per_channel': False,                                  # custom whether quant is per-channel or per-tensor for activation,\n",
    "                'pot_scale': False,                                   # custom whether scale is power of two for activation.\n",
    "            }\n",
    "        }                                                         # custom tracer behavior, checkout https://github.com/pytorch/pytorch/blob/efcbbb177eacdacda80b94ad4ce34b9ed6cf687a/torch/fx/_symbolic_trace.py#L836\n",
    "    }\n",
    "    print(f'Prepare {b}bits model using MQBench')\n",
    "\n",
    "    exec(f'mqb_{b}bits_model=prepare_by_platform(mqb_fp_model, backend,extra_config).cuda()')\n",
    "    \n",
    "    # calibration loop\n",
    "    enable_calibration(eval(f'mqb_{b}bits_model'))\n",
    "    for img,label in calib_data:\n",
    "        eval(f'mqb_{b}bits_model')(img.cuda())\n",
    "    \n",
    "    if adv_ptq:\n",
    "        if os.path.exists(f'QDROP_{b}bits_{mn}.pt'):\n",
    "            exec(f'mqb_{b}bits_model=ptq_reconstruction(mqb_{b}bits_model, stacked_tensor, ptq_reconstruction_config_init).cuda()')\n",
    "            print(f'QDROP model already saved, now loading QDROP_{b}bits_{mn}.pt')\n",
    "            load_from = f'QDROP_{b}bits_{mn}.pt'\n",
    "            exec(f'mqb_{b}bits_model.load_state_dict(torch.load(load_from))')\n",
    "        else:\n",
    "            \n",
    "            exec(f'mqb_{b}bits_model=ptq_reconstruction(mqb_{b}bits_model, stacked_tensor, ptq_reconstruction_config).cuda()')\n",
    "            print(f'saving QDROP tuned model: QDROP_{b}bits_{mn}.pt...')\n",
    "            torch.save(eval(f'mqb_{b}bits_model').state_dict(),f'QDROP_{b}bits_{mn}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c08c97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MQBENCH] INFO: Disable observer and Disable quantize.\n",
      "[MQBENCH] INFO: Disable observer and Enable quantize.\n",
      "evaluate mqb quantized model\n",
      "{'top1': tensor([1.0000], device='cuda:0'), 'top5': tensor([6.0800], device='cuda:0'), 'loss': tensor(6.0304, device='cuda:0'), 'time': 3.1067452430725098}\n",
      "[MQBENCH] INFO: Disable observer and Disable quantize.\n",
      "[MQBENCH] INFO: Disable observer and Enable quantize.\n",
      "evaluate mqb quantized model\n",
      "{'top1': tensor([46.9600], device='cuda:0'), 'top5': tensor([74.7300], device='cuda:0'), 'loss': tensor(3.2726, device='cuda:0'), 'time': 3.08516788482666}\n",
      "[MQBENCH] INFO: Disable observer and Disable quantize.\n",
      "[MQBENCH] INFO: Disable observer and Enable quantize.\n",
      "evaluate mqb quantized model\n",
      "{'top1': tensor([72.4900], device='cuda:0'), 'top5': tensor([91.8900], device='cuda:0'), 'loss': tensor(1.3006, device='cuda:0'), 'time': 3.0914647579193115}\n"
     ]
    }
   ],
   "source": [
    "for b in MPQ_scheme: \n",
    "    disable_all(eval(f'mqb_{b}bits_model'))\n",
    "    # evaluation loop\n",
    "    enable_quantization(eval(f'mqb_{b}bits_model'))\n",
    "    eval(f'mqb_{b}bits_model').eval()\n",
    "    print('evaluate mqb quantized model')\n",
    "    print(evaluate(test,eval(f'mqb_{b}bits_model')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f1d8226",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MQBENCH] INFO: Disable observer and Disable quantize.\n",
      "input of  conv1  is  x_post_act_fake_quantizer\n",
      "input of  layer1.0.conv1  is  relu_post_act_fake_quantizer\n",
      "input of  layer1.0.conv2  is  layer1_0_relu_post_act_fake_quantizer\n",
      "input of  layer1.1.conv1  is  layer1_0_relu_1_post_act_fake_quantizer\n",
      "input of  layer1.1.conv2  is  layer1_1_relu_post_act_fake_quantizer\n",
      "input of  layer1.2.conv1  is  layer1_1_relu_1_post_act_fake_quantizer\n",
      "input of  layer1.2.conv2  is  layer1_2_relu_post_act_fake_quantizer\n",
      "input of  layer1.3.conv1  is  layer1_2_relu_1_post_act_fake_quantizer\n",
      "input of  layer1.3.conv2  is  layer1_3_relu_post_act_fake_quantizer\n",
      "input of  layer1.4.conv1  is  layer1_3_relu_1_post_act_fake_quantizer\n",
      "input of  layer1.4.conv2  is  layer1_4_relu_post_act_fake_quantizer\n",
      "input of  layer1.5.conv1  is  layer1_4_relu_1_post_act_fake_quantizer\n",
      "input of  layer1.5.conv2  is  layer1_5_relu_post_act_fake_quantizer\n",
      "input of  layer1.6.conv1  is  layer1_5_relu_1_post_act_fake_quantizer\n",
      "input of  layer1.6.conv2  is  layer1_6_relu_post_act_fake_quantizer\n",
      "input of  layer1.7.conv1  is  layer1_6_relu_1_post_act_fake_quantizer\n",
      "input of  layer1.7.conv2  is  layer1_7_relu_post_act_fake_quantizer\n",
      "input of  layer1.8.conv1  is  layer1_7_relu_1_post_act_fake_quantizer\n",
      "input of  layer1.8.conv2  is  layer1_8_relu_post_act_fake_quantizer\n",
      "input of  layer2.0.conv1  is  layer1_8_relu_1_post_act_fake_quantizer\n",
      "input of  layer2.0.conv2  is  layer2_0_relu_post_act_fake_quantizer\n",
      "input of  layer2.0.downsample.0  is  layer1_8_relu_1_post_act_fake_quantizer\n",
      "input of  layer2.1.conv1  is  layer2_0_relu_1_post_act_fake_quantizer\n",
      "input of  layer2.1.conv2  is  layer2_1_relu_post_act_fake_quantizer\n",
      "input of  layer2.2.conv1  is  layer2_1_relu_1_post_act_fake_quantizer\n",
      "input of  layer2.2.conv2  is  layer2_2_relu_post_act_fake_quantizer\n",
      "input of  layer2.3.conv1  is  layer2_2_relu_1_post_act_fake_quantizer\n",
      "input of  layer2.3.conv2  is  layer2_3_relu_post_act_fake_quantizer\n",
      "input of  layer2.4.conv1  is  layer2_3_relu_1_post_act_fake_quantizer\n",
      "input of  layer2.4.conv2  is  layer2_4_relu_post_act_fake_quantizer\n",
      "input of  layer2.5.conv1  is  layer2_4_relu_1_post_act_fake_quantizer\n",
      "input of  layer2.5.conv2  is  layer2_5_relu_post_act_fake_quantizer\n",
      "input of  layer2.6.conv1  is  layer2_5_relu_1_post_act_fake_quantizer\n",
      "input of  layer2.6.conv2  is  layer2_6_relu_post_act_fake_quantizer\n",
      "input of  layer2.7.conv1  is  layer2_6_relu_1_post_act_fake_quantizer\n",
      "input of  layer2.7.conv2  is  layer2_7_relu_post_act_fake_quantizer\n",
      "input of  layer2.8.conv1  is  layer2_7_relu_1_post_act_fake_quantizer\n",
      "input of  layer2.8.conv2  is  layer2_8_relu_post_act_fake_quantizer\n",
      "input of  layer3.0.conv1  is  layer2_8_relu_1_post_act_fake_quantizer\n",
      "input of  layer3.0.conv2  is  layer3_0_relu_post_act_fake_quantizer\n",
      "input of  layer3.0.downsample.0  is  layer2_8_relu_1_post_act_fake_quantizer\n",
      "input of  layer3.1.conv1  is  layer3_0_relu_1_post_act_fake_quantizer\n",
      "input of  layer3.1.conv2  is  layer3_1_relu_post_act_fake_quantizer\n",
      "input of  layer3.2.conv1  is  layer3_1_relu_1_post_act_fake_quantizer\n",
      "input of  layer3.2.conv2  is  layer3_2_relu_post_act_fake_quantizer\n",
      "input of  layer3.3.conv1  is  layer3_2_relu_1_post_act_fake_quantizer\n",
      "input of  layer3.3.conv2  is  layer3_3_relu_post_act_fake_quantizer\n",
      "input of  layer3.4.conv1  is  layer3_3_relu_1_post_act_fake_quantizer\n",
      "input of  layer3.4.conv2  is  layer3_4_relu_post_act_fake_quantizer\n",
      "input of  layer3.5.conv1  is  layer3_4_relu_1_post_act_fake_quantizer\n",
      "input of  layer3.5.conv2  is  layer3_5_relu_post_act_fake_quantizer\n",
      "input of  layer3.6.conv1  is  layer3_5_relu_1_post_act_fake_quantizer\n",
      "input of  layer3.6.conv2  is  layer3_6_relu_post_act_fake_quantizer\n",
      "input of  layer3.7.conv1  is  layer3_6_relu_1_post_act_fake_quantizer\n",
      "input of  layer3.7.conv2  is  layer3_7_relu_post_act_fake_quantizer\n",
      "input of  layer3.8.conv1  is  layer3_7_relu_1_post_act_fake_quantizer\n",
      "input of  layer3.8.conv2  is  layer3_8_relu_post_act_fake_quantizer\n",
      "input of  fc  is  view_post_act_fake_quantizer\n"
     ]
    }
   ],
   "source": [
    "mqb_fp_model = deepcopy(mqb_8bits_model)\n",
    "disable_all(mqb_fp_model)\n",
    "mqb_mix_model = deepcopy(mqb_fp_model)\n",
    "\n",
    "# 1. record all modules we want to consider\n",
    "types_to_quant = (torch.nn.Conv2d,torch.nn.Linear)\n",
    "\n",
    "layer_input_map = {}\n",
    "\n",
    "for node in mqb_8bits_model.graph.nodes:\n",
    "    try:\n",
    "        node_target = getModuleByName(mqb_mix_model,node.target)\n",
    "        if isinstance(node_target,types_to_quant):\n",
    "            node_args = node.args[0]\n",
    "            print('input of ',node.target,' is ',node_args)\n",
    "            layer_input_map[node.target] = str(node_args.target)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecaa6af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_metric = ('loss',evaluate(calib_data,mqb_fp_model)['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdeec63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb(perturb_scheme):\n",
    "    # perturb_scheme: {layer_name:(act_bits,weight_bits)}\n",
    "    for layer_name in perturb_scheme:\n",
    "        a_bits,w_bits = perturb_scheme[layer_name]\n",
    "        \n",
    "        if w_bits is not None:\n",
    "            mix_module = getModuleByName(mqb_mix_model,layer_name)\n",
    "            tar_module = getModuleByName(eval(f'mqb_{w_bits}bits_model'),layer_name)\n",
    "            # replace weight quant to use a_bits quantization\n",
    "            w_cmd = f'mix_module.weight_fake_quant=tar_module.weight_fake_quant'\n",
    "            exec(w_cmd)\n",
    "        \n",
    "        if a_bits is not None:\n",
    "        \n",
    "            # replace act quant to use w_bits quantization\n",
    "            a_cmd = f'mqb_mix_model.{layer_input_map[layer_name]}=mqb_{a_bits}bits_model.{layer_input_map[layer_name]}'\n",
    "            exec(a_cmd)\n",
    "        \n",
    "        #print(layer_name)\n",
    "        #print(a_cmd)\n",
    "        #print(w_cmd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ee550ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perturb functionality test\n",
    "perturb_scheme = {}\n",
    "for layer_name in layer_input_map:\n",
    "    perturb_scheme[layer_name] = (8,8)\n",
    "perturb(perturb_scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42af6e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'top1': tensor([72.4900], device='cuda:0'),\n",
       " 'top5': tensor([91.8900], device='cuda:0'),\n",
       " 'loss': tensor(1.3006, device='cuda:0'),\n",
       " 'time': 3.1675009727478027}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(test,mqb_mix_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6987f2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MQBENCH] INFO: Disable observer and Disable quantize.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'top1': tensor([72.6300], device='cuda:0'),\n",
       " 'top5': tensor([91.9400], device='cuda:0'),\n",
       " 'loss': tensor(1.2946, device='cuda:0'),\n",
       " 'time': 2.6905508041381836}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mqb_mix_model = deepcopy(mqb_8bits_model)\n",
    "disable_all(mqb_mix_model)\n",
    "evaluate(test,mqb_mix_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8846ea48",
   "metadata": {},
   "source": [
    "## CLADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa6716b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "kl_loss = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "def kldiv(quant_logit,fp_logit):\n",
    "    inp = F.log_softmax(quant_logit,dim=-1)\n",
    "    tar = F.softmax(fp_logit,dim=-1)\n",
    "    return kl_loss(inp,tar)\n",
    "\n",
    "def perturb_loss(perturb_scheme,ref_metric=ref_metric,\n",
    "                 eval_data=calib_data,printInfo=False,KL=False):\n",
    "    \n",
    "    global mqb_mix_model\n",
    "    mqb_mix_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # perturb layers\n",
    "        perturb(perturb_scheme)\n",
    "            \n",
    "        # do evaluation\n",
    "        if not KL:\n",
    "            res = evaluate(eval_data,mqb_mix_model)\n",
    "            perturbed_loss = res[ref_metric[0]] - ref_metric[1]\n",
    "        else:\n",
    "            perturbed_loss = []\n",
    "            \n",
    "            for (data,fp_out) in zip(calib_data,calib_fp_output):\n",
    "                img,label = data\n",
    "                quant_out = mqb_mix_model(img.cuda())\n",
    "                perturbed_loss.append(kldiv(quant_out,fp_out))\n",
    "            #print(perturbed_loss)\n",
    "            perturbed_loss = torch.tensor(perturbed_loss).mean()    \n",
    "        \n",
    "        if printInfo:\n",
    "            print(f'use kl {KL} perturbed loss {perturbed_loss}')\n",
    "        \n",
    "        # recover layers\n",
    "        mqb_mix_model = deepcopy(mqb_fp_model)\n",
    "            \n",
    "    return perturbed_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de4162ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perturb loss functionality check\n",
    "# del layer_input_map['conv1']\n",
    "# del layer_input_map['fc']\n",
    "\n",
    "# for layer in layer_input_map:\n",
    "#     for a_bits in MPQ_scheme:\n",
    "#         for w_bits in MPQ_scheme:\n",
    "#             print(f'{layer} (a:{a_bits} bits,w:{w_bits} bits))')\n",
    "#             p = perturb_loss({layer:(a_bits,w_bits)},eval_data=test,printInfo=True,KL=False)\n",
    "#             #print(f'{layer} (a:{a_bits} bits,w:{w_bits} bits), accuracy degradation: {p*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47dd85f",
   "metadata": {},
   "source": [
    "## Build Cached Grad if not done before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7319de56",
   "metadata": {},
   "outputs": [],
   "source": [
    "del layer_input_map['conv1']\n",
    "del layer_input_map['fc']\n",
    "\n",
    "import time\n",
    "s_time = time.time()\n",
    "cached = {}\n",
    "aw_scheme = []\n",
    "for a_bits in MPQ_scheme:\n",
    "    for w_bits in MPQ_scheme:\n",
    "        aw_scheme.append((a_bits,w_bits))\n",
    "\n",
    "aw_scheme = [(8,2),(8,4),(8,8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d37d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aw_scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f370df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "KL=True\n",
    "for n in layer_input_map:\n",
    "    for m in layer_input_map:\n",
    "        for naw in aw_scheme:\n",
    "            for maw in aw_scheme:\n",
    "                if (n,m,naw,maw) not in cached:\n",
    "                    if n == m:\n",
    "                        if naw == maw:\n",
    "                            \n",
    "                            p = perturb_loss({n:naw},ref_metric,calib_data,KL=KL)\n",
    "                            print(f'perturb layer {n} to A{naw[0]}W{naw[1]} p={p}')\n",
    "                        else:\n",
    "                            p = 0\n",
    "\n",
    "                    else:\n",
    "                        \n",
    "                        p = perturb_loss({n:naw,m:maw},ref_metric,calib_data,KL=KL)\n",
    "                        print(f'perturb layer {n} to A{naw[0]}W{naw[1]} and layer {m} to A{maw[0]}W{maw[1]} p={p}')\n",
    "                    \n",
    "                    cached[(n,m,naw,maw)] = cached[(m,n,maw,naw)] = p\n",
    "                    \n",
    "print(f'{time.time()-s_time:.2f} seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf90f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239967a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_index = {}\n",
    "cnt = 0\n",
    "for layer in layer_input_map:\n",
    "    for s in aw_scheme:\n",
    "        layer_index[layer+f'{s}bits'] = cnt\n",
    "        cnt += 1\n",
    "L = cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece828de",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fb4786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "hm = np.zeros(shape=(L,L))\n",
    "for n in layer_input_map:\n",
    "    for m in layer_input_map:\n",
    "        for naw in aw_scheme:\n",
    "            for maw in aw_scheme:\n",
    "                hm[layer_index[n+f'{naw}bits'],layer_index[m+f'{maw}bits']] = cached[(n,m,naw,maw)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a31472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_grad = np.zeros_like(hm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99559c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('generala248w248_i1kresnet50_calib','wb') as f:\n",
    "    pickle.dump({'Ltilde':hm,'layer_index':layer_index},f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9644eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturb_loss(['conv1',],ref_metric,eval_data=calib_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babc001d",
   "metadata": {},
   "source": [
    "## Load Cached Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "498e1ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "# with open('generala248w248_c100resnet56_calib','rb') as f:\n",
    "# with open('CachedGrad_QDROP(2, 4, 8)cifar100_resnet56.pkl','rb') as f:\n",
    "with open('CachedGrad_a248w248c100_resnet56KL.pkl','rb') as f:\n",
    "    hm = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb1de436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Ltilde', 'layer_index'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hm.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdf70d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer1.0.conv1(2, 2)bits': 0,\n",
       " 'layer1.0.conv1(2, 4)bits': 1,\n",
       " 'layer1.0.conv1(2, 8)bits': 2,\n",
       " 'layer1.0.conv1(4, 2)bits': 3,\n",
       " 'layer1.0.conv1(4, 4)bits': 4,\n",
       " 'layer1.0.conv1(4, 8)bits': 5,\n",
       " 'layer1.0.conv1(8, 2)bits': 6,\n",
       " 'layer1.0.conv1(8, 4)bits': 7,\n",
       " 'layer1.0.conv1(8, 8)bits': 8,\n",
       " 'layer1.0.conv2(2, 2)bits': 9,\n",
       " 'layer1.0.conv2(2, 4)bits': 10,\n",
       " 'layer1.0.conv2(2, 8)bits': 11,\n",
       " 'layer1.0.conv2(4, 2)bits': 12,\n",
       " 'layer1.0.conv2(4, 4)bits': 13,\n",
       " 'layer1.0.conv2(4, 8)bits': 14,\n",
       " 'layer1.0.conv2(8, 2)bits': 15,\n",
       " 'layer1.0.conv2(8, 4)bits': 16,\n",
       " 'layer1.0.conv2(8, 8)bits': 17,\n",
       " 'layer1.1.conv1(2, 2)bits': 18,\n",
       " 'layer1.1.conv1(2, 4)bits': 19,\n",
       " 'layer1.1.conv1(2, 8)bits': 20,\n",
       " 'layer1.1.conv1(4, 2)bits': 21,\n",
       " 'layer1.1.conv1(4, 4)bits': 22,\n",
       " 'layer1.1.conv1(4, 8)bits': 23,\n",
       " 'layer1.1.conv1(8, 2)bits': 24,\n",
       " 'layer1.1.conv1(8, 4)bits': 25,\n",
       " 'layer1.1.conv1(8, 8)bits': 26,\n",
       " 'layer1.1.conv2(2, 2)bits': 27,\n",
       " 'layer1.1.conv2(2, 4)bits': 28,\n",
       " 'layer1.1.conv2(2, 8)bits': 29,\n",
       " 'layer1.1.conv2(4, 2)bits': 30,\n",
       " 'layer1.1.conv2(4, 4)bits': 31,\n",
       " 'layer1.1.conv2(4, 8)bits': 32,\n",
       " 'layer1.1.conv2(8, 2)bits': 33,\n",
       " 'layer1.1.conv2(8, 4)bits': 34,\n",
       " 'layer1.1.conv2(8, 8)bits': 35,\n",
       " 'layer1.2.conv1(2, 2)bits': 36,\n",
       " 'layer1.2.conv1(2, 4)bits': 37,\n",
       " 'layer1.2.conv1(2, 8)bits': 38,\n",
       " 'layer1.2.conv1(4, 2)bits': 39,\n",
       " 'layer1.2.conv1(4, 4)bits': 40,\n",
       " 'layer1.2.conv1(4, 8)bits': 41,\n",
       " 'layer1.2.conv1(8, 2)bits': 42,\n",
       " 'layer1.2.conv1(8, 4)bits': 43,\n",
       " 'layer1.2.conv1(8, 8)bits': 44,\n",
       " 'layer1.2.conv2(2, 2)bits': 45,\n",
       " 'layer1.2.conv2(2, 4)bits': 46,\n",
       " 'layer1.2.conv2(2, 8)bits': 47,\n",
       " 'layer1.2.conv2(4, 2)bits': 48,\n",
       " 'layer1.2.conv2(4, 4)bits': 49,\n",
       " 'layer1.2.conv2(4, 8)bits': 50,\n",
       " 'layer1.2.conv2(8, 2)bits': 51,\n",
       " 'layer1.2.conv2(8, 4)bits': 52,\n",
       " 'layer1.2.conv2(8, 8)bits': 53,\n",
       " 'layer1.3.conv1(2, 2)bits': 54,\n",
       " 'layer1.3.conv1(2, 4)bits': 55,\n",
       " 'layer1.3.conv1(2, 8)bits': 56,\n",
       " 'layer1.3.conv1(4, 2)bits': 57,\n",
       " 'layer1.3.conv1(4, 4)bits': 58,\n",
       " 'layer1.3.conv1(4, 8)bits': 59,\n",
       " 'layer1.3.conv1(8, 2)bits': 60,\n",
       " 'layer1.3.conv1(8, 4)bits': 61,\n",
       " 'layer1.3.conv1(8, 8)bits': 62,\n",
       " 'layer1.3.conv2(2, 2)bits': 63,\n",
       " 'layer1.3.conv2(2, 4)bits': 64,\n",
       " 'layer1.3.conv2(2, 8)bits': 65,\n",
       " 'layer1.3.conv2(4, 2)bits': 66,\n",
       " 'layer1.3.conv2(4, 4)bits': 67,\n",
       " 'layer1.3.conv2(4, 8)bits': 68,\n",
       " 'layer1.3.conv2(8, 2)bits': 69,\n",
       " 'layer1.3.conv2(8, 4)bits': 70,\n",
       " 'layer1.3.conv2(8, 8)bits': 71,\n",
       " 'layer1.4.conv1(2, 2)bits': 72,\n",
       " 'layer1.4.conv1(2, 4)bits': 73,\n",
       " 'layer1.4.conv1(2, 8)bits': 74,\n",
       " 'layer1.4.conv1(4, 2)bits': 75,\n",
       " 'layer1.4.conv1(4, 4)bits': 76,\n",
       " 'layer1.4.conv1(4, 8)bits': 77,\n",
       " 'layer1.4.conv1(8, 2)bits': 78,\n",
       " 'layer1.4.conv1(8, 4)bits': 79,\n",
       " 'layer1.4.conv1(8, 8)bits': 80,\n",
       " 'layer1.4.conv2(2, 2)bits': 81,\n",
       " 'layer1.4.conv2(2, 4)bits': 82,\n",
       " 'layer1.4.conv2(2, 8)bits': 83,\n",
       " 'layer1.4.conv2(4, 2)bits': 84,\n",
       " 'layer1.4.conv2(4, 4)bits': 85,\n",
       " 'layer1.4.conv2(4, 8)bits': 86,\n",
       " 'layer1.4.conv2(8, 2)bits': 87,\n",
       " 'layer1.4.conv2(8, 4)bits': 88,\n",
       " 'layer1.4.conv2(8, 8)bits': 89,\n",
       " 'layer1.5.conv1(2, 2)bits': 90,\n",
       " 'layer1.5.conv1(2, 4)bits': 91,\n",
       " 'layer1.5.conv1(2, 8)bits': 92,\n",
       " 'layer1.5.conv1(4, 2)bits': 93,\n",
       " 'layer1.5.conv1(4, 4)bits': 94,\n",
       " 'layer1.5.conv1(4, 8)bits': 95,\n",
       " 'layer1.5.conv1(8, 2)bits': 96,\n",
       " 'layer1.5.conv1(8, 4)bits': 97,\n",
       " 'layer1.5.conv1(8, 8)bits': 98,\n",
       " 'layer1.5.conv2(2, 2)bits': 99,\n",
       " 'layer1.5.conv2(2, 4)bits': 100,\n",
       " 'layer1.5.conv2(2, 8)bits': 101,\n",
       " 'layer1.5.conv2(4, 2)bits': 102,\n",
       " 'layer1.5.conv2(4, 4)bits': 103,\n",
       " 'layer1.5.conv2(4, 8)bits': 104,\n",
       " 'layer1.5.conv2(8, 2)bits': 105,\n",
       " 'layer1.5.conv2(8, 4)bits': 106,\n",
       " 'layer1.5.conv2(8, 8)bits': 107,\n",
       " 'layer1.6.conv1(2, 2)bits': 108,\n",
       " 'layer1.6.conv1(2, 4)bits': 109,\n",
       " 'layer1.6.conv1(2, 8)bits': 110,\n",
       " 'layer1.6.conv1(4, 2)bits': 111,\n",
       " 'layer1.6.conv1(4, 4)bits': 112,\n",
       " 'layer1.6.conv1(4, 8)bits': 113,\n",
       " 'layer1.6.conv1(8, 2)bits': 114,\n",
       " 'layer1.6.conv1(8, 4)bits': 115,\n",
       " 'layer1.6.conv1(8, 8)bits': 116,\n",
       " 'layer1.6.conv2(2, 2)bits': 117,\n",
       " 'layer1.6.conv2(2, 4)bits': 118,\n",
       " 'layer1.6.conv2(2, 8)bits': 119,\n",
       " 'layer1.6.conv2(4, 2)bits': 120,\n",
       " 'layer1.6.conv2(4, 4)bits': 121,\n",
       " 'layer1.6.conv2(4, 8)bits': 122,\n",
       " 'layer1.6.conv2(8, 2)bits': 123,\n",
       " 'layer1.6.conv2(8, 4)bits': 124,\n",
       " 'layer1.6.conv2(8, 8)bits': 125,\n",
       " 'layer1.7.conv1(2, 2)bits': 126,\n",
       " 'layer1.7.conv1(2, 4)bits': 127,\n",
       " 'layer1.7.conv1(2, 8)bits': 128,\n",
       " 'layer1.7.conv1(4, 2)bits': 129,\n",
       " 'layer1.7.conv1(4, 4)bits': 130,\n",
       " 'layer1.7.conv1(4, 8)bits': 131,\n",
       " 'layer1.7.conv1(8, 2)bits': 132,\n",
       " 'layer1.7.conv1(8, 4)bits': 133,\n",
       " 'layer1.7.conv1(8, 8)bits': 134,\n",
       " 'layer1.7.conv2(2, 2)bits': 135,\n",
       " 'layer1.7.conv2(2, 4)bits': 136,\n",
       " 'layer1.7.conv2(2, 8)bits': 137,\n",
       " 'layer1.7.conv2(4, 2)bits': 138,\n",
       " 'layer1.7.conv2(4, 4)bits': 139,\n",
       " 'layer1.7.conv2(4, 8)bits': 140,\n",
       " 'layer1.7.conv2(8, 2)bits': 141,\n",
       " 'layer1.7.conv2(8, 4)bits': 142,\n",
       " 'layer1.7.conv2(8, 8)bits': 143,\n",
       " 'layer1.8.conv1(2, 2)bits': 144,\n",
       " 'layer1.8.conv1(2, 4)bits': 145,\n",
       " 'layer1.8.conv1(2, 8)bits': 146,\n",
       " 'layer1.8.conv1(4, 2)bits': 147,\n",
       " 'layer1.8.conv1(4, 4)bits': 148,\n",
       " 'layer1.8.conv1(4, 8)bits': 149,\n",
       " 'layer1.8.conv1(8, 2)bits': 150,\n",
       " 'layer1.8.conv1(8, 4)bits': 151,\n",
       " 'layer1.8.conv1(8, 8)bits': 152,\n",
       " 'layer1.8.conv2(2, 2)bits': 153,\n",
       " 'layer1.8.conv2(2, 4)bits': 154,\n",
       " 'layer1.8.conv2(2, 8)bits': 155,\n",
       " 'layer1.8.conv2(4, 2)bits': 156,\n",
       " 'layer1.8.conv2(4, 4)bits': 157,\n",
       " 'layer1.8.conv2(4, 8)bits': 158,\n",
       " 'layer1.8.conv2(8, 2)bits': 159,\n",
       " 'layer1.8.conv2(8, 4)bits': 160,\n",
       " 'layer1.8.conv2(8, 8)bits': 161,\n",
       " 'layer2.0.conv1(2, 2)bits': 162,\n",
       " 'layer2.0.conv1(2, 4)bits': 163,\n",
       " 'layer2.0.conv1(2, 8)bits': 164,\n",
       " 'layer2.0.conv1(4, 2)bits': 165,\n",
       " 'layer2.0.conv1(4, 4)bits': 166,\n",
       " 'layer2.0.conv1(4, 8)bits': 167,\n",
       " 'layer2.0.conv1(8, 2)bits': 168,\n",
       " 'layer2.0.conv1(8, 4)bits': 169,\n",
       " 'layer2.0.conv1(8, 8)bits': 170,\n",
       " 'layer2.0.conv2(2, 2)bits': 171,\n",
       " 'layer2.0.conv2(2, 4)bits': 172,\n",
       " 'layer2.0.conv2(2, 8)bits': 173,\n",
       " 'layer2.0.conv2(4, 2)bits': 174,\n",
       " 'layer2.0.conv2(4, 4)bits': 175,\n",
       " 'layer2.0.conv2(4, 8)bits': 176,\n",
       " 'layer2.0.conv2(8, 2)bits': 177,\n",
       " 'layer2.0.conv2(8, 4)bits': 178,\n",
       " 'layer2.0.conv2(8, 8)bits': 179,\n",
       " 'layer2.0.downsample.0(2, 2)bits': 180,\n",
       " 'layer2.0.downsample.0(2, 4)bits': 181,\n",
       " 'layer2.0.downsample.0(2, 8)bits': 182,\n",
       " 'layer2.0.downsample.0(4, 2)bits': 183,\n",
       " 'layer2.0.downsample.0(4, 4)bits': 184,\n",
       " 'layer2.0.downsample.0(4, 8)bits': 185,\n",
       " 'layer2.0.downsample.0(8, 2)bits': 186,\n",
       " 'layer2.0.downsample.0(8, 4)bits': 187,\n",
       " 'layer2.0.downsample.0(8, 8)bits': 188,\n",
       " 'layer2.1.conv1(2, 2)bits': 189,\n",
       " 'layer2.1.conv1(2, 4)bits': 190,\n",
       " 'layer2.1.conv1(2, 8)bits': 191,\n",
       " 'layer2.1.conv1(4, 2)bits': 192,\n",
       " 'layer2.1.conv1(4, 4)bits': 193,\n",
       " 'layer2.1.conv1(4, 8)bits': 194,\n",
       " 'layer2.1.conv1(8, 2)bits': 195,\n",
       " 'layer2.1.conv1(8, 4)bits': 196,\n",
       " 'layer2.1.conv1(8, 8)bits': 197,\n",
       " 'layer2.1.conv2(2, 2)bits': 198,\n",
       " 'layer2.1.conv2(2, 4)bits': 199,\n",
       " 'layer2.1.conv2(2, 8)bits': 200,\n",
       " 'layer2.1.conv2(4, 2)bits': 201,\n",
       " 'layer2.1.conv2(4, 4)bits': 202,\n",
       " 'layer2.1.conv2(4, 8)bits': 203,\n",
       " 'layer2.1.conv2(8, 2)bits': 204,\n",
       " 'layer2.1.conv2(8, 4)bits': 205,\n",
       " 'layer2.1.conv2(8, 8)bits': 206,\n",
       " 'layer2.2.conv1(2, 2)bits': 207,\n",
       " 'layer2.2.conv1(2, 4)bits': 208,\n",
       " 'layer2.2.conv1(2, 8)bits': 209,\n",
       " 'layer2.2.conv1(4, 2)bits': 210,\n",
       " 'layer2.2.conv1(4, 4)bits': 211,\n",
       " 'layer2.2.conv1(4, 8)bits': 212,\n",
       " 'layer2.2.conv1(8, 2)bits': 213,\n",
       " 'layer2.2.conv1(8, 4)bits': 214,\n",
       " 'layer2.2.conv1(8, 8)bits': 215,\n",
       " 'layer2.2.conv2(2, 2)bits': 216,\n",
       " 'layer2.2.conv2(2, 4)bits': 217,\n",
       " 'layer2.2.conv2(2, 8)bits': 218,\n",
       " 'layer2.2.conv2(4, 2)bits': 219,\n",
       " 'layer2.2.conv2(4, 4)bits': 220,\n",
       " 'layer2.2.conv2(4, 8)bits': 221,\n",
       " 'layer2.2.conv2(8, 2)bits': 222,\n",
       " 'layer2.2.conv2(8, 4)bits': 223,\n",
       " 'layer2.2.conv2(8, 8)bits': 224,\n",
       " 'layer2.3.conv1(2, 2)bits': 225,\n",
       " 'layer2.3.conv1(2, 4)bits': 226,\n",
       " 'layer2.3.conv1(2, 8)bits': 227,\n",
       " 'layer2.3.conv1(4, 2)bits': 228,\n",
       " 'layer2.3.conv1(4, 4)bits': 229,\n",
       " 'layer2.3.conv1(4, 8)bits': 230,\n",
       " 'layer2.3.conv1(8, 2)bits': 231,\n",
       " 'layer2.3.conv1(8, 4)bits': 232,\n",
       " 'layer2.3.conv1(8, 8)bits': 233,\n",
       " 'layer2.3.conv2(2, 2)bits': 234,\n",
       " 'layer2.3.conv2(2, 4)bits': 235,\n",
       " 'layer2.3.conv2(2, 8)bits': 236,\n",
       " 'layer2.3.conv2(4, 2)bits': 237,\n",
       " 'layer2.3.conv2(4, 4)bits': 238,\n",
       " 'layer2.3.conv2(4, 8)bits': 239,\n",
       " 'layer2.3.conv2(8, 2)bits': 240,\n",
       " 'layer2.3.conv2(8, 4)bits': 241,\n",
       " 'layer2.3.conv2(8, 8)bits': 242,\n",
       " 'layer2.4.conv1(2, 2)bits': 243,\n",
       " 'layer2.4.conv1(2, 4)bits': 244,\n",
       " 'layer2.4.conv1(2, 8)bits': 245,\n",
       " 'layer2.4.conv1(4, 2)bits': 246,\n",
       " 'layer2.4.conv1(4, 4)bits': 247,\n",
       " 'layer2.4.conv1(4, 8)bits': 248,\n",
       " 'layer2.4.conv1(8, 2)bits': 249,\n",
       " 'layer2.4.conv1(8, 4)bits': 250,\n",
       " 'layer2.4.conv1(8, 8)bits': 251,\n",
       " 'layer2.4.conv2(2, 2)bits': 252,\n",
       " 'layer2.4.conv2(2, 4)bits': 253,\n",
       " 'layer2.4.conv2(2, 8)bits': 254,\n",
       " 'layer2.4.conv2(4, 2)bits': 255,\n",
       " 'layer2.4.conv2(4, 4)bits': 256,\n",
       " 'layer2.4.conv2(4, 8)bits': 257,\n",
       " 'layer2.4.conv2(8, 2)bits': 258,\n",
       " 'layer2.4.conv2(8, 4)bits': 259,\n",
       " 'layer2.4.conv2(8, 8)bits': 260,\n",
       " 'layer2.5.conv1(2, 2)bits': 261,\n",
       " 'layer2.5.conv1(2, 4)bits': 262,\n",
       " 'layer2.5.conv1(2, 8)bits': 263,\n",
       " 'layer2.5.conv1(4, 2)bits': 264,\n",
       " 'layer2.5.conv1(4, 4)bits': 265,\n",
       " 'layer2.5.conv1(4, 8)bits': 266,\n",
       " 'layer2.5.conv1(8, 2)bits': 267,\n",
       " 'layer2.5.conv1(8, 4)bits': 268,\n",
       " 'layer2.5.conv1(8, 8)bits': 269,\n",
       " 'layer2.5.conv2(2, 2)bits': 270,\n",
       " 'layer2.5.conv2(2, 4)bits': 271,\n",
       " 'layer2.5.conv2(2, 8)bits': 272,\n",
       " 'layer2.5.conv2(4, 2)bits': 273,\n",
       " 'layer2.5.conv2(4, 4)bits': 274,\n",
       " 'layer2.5.conv2(4, 8)bits': 275,\n",
       " 'layer2.5.conv2(8, 2)bits': 276,\n",
       " 'layer2.5.conv2(8, 4)bits': 277,\n",
       " 'layer2.5.conv2(8, 8)bits': 278,\n",
       " 'layer2.6.conv1(2, 2)bits': 279,\n",
       " 'layer2.6.conv1(2, 4)bits': 280,\n",
       " 'layer2.6.conv1(2, 8)bits': 281,\n",
       " 'layer2.6.conv1(4, 2)bits': 282,\n",
       " 'layer2.6.conv1(4, 4)bits': 283,\n",
       " 'layer2.6.conv1(4, 8)bits': 284,\n",
       " 'layer2.6.conv1(8, 2)bits': 285,\n",
       " 'layer2.6.conv1(8, 4)bits': 286,\n",
       " 'layer2.6.conv1(8, 8)bits': 287,\n",
       " 'layer2.6.conv2(2, 2)bits': 288,\n",
       " 'layer2.6.conv2(2, 4)bits': 289,\n",
       " 'layer2.6.conv2(2, 8)bits': 290,\n",
       " 'layer2.6.conv2(4, 2)bits': 291,\n",
       " 'layer2.6.conv2(4, 4)bits': 292,\n",
       " 'layer2.6.conv2(4, 8)bits': 293,\n",
       " 'layer2.6.conv2(8, 2)bits': 294,\n",
       " 'layer2.6.conv2(8, 4)bits': 295,\n",
       " 'layer2.6.conv2(8, 8)bits': 296,\n",
       " 'layer2.7.conv1(2, 2)bits': 297,\n",
       " 'layer2.7.conv1(2, 4)bits': 298,\n",
       " 'layer2.7.conv1(2, 8)bits': 299,\n",
       " 'layer2.7.conv1(4, 2)bits': 300,\n",
       " 'layer2.7.conv1(4, 4)bits': 301,\n",
       " 'layer2.7.conv1(4, 8)bits': 302,\n",
       " 'layer2.7.conv1(8, 2)bits': 303,\n",
       " 'layer2.7.conv1(8, 4)bits': 304,\n",
       " 'layer2.7.conv1(8, 8)bits': 305,\n",
       " 'layer2.7.conv2(2, 2)bits': 306,\n",
       " 'layer2.7.conv2(2, 4)bits': 307,\n",
       " 'layer2.7.conv2(2, 8)bits': 308,\n",
       " 'layer2.7.conv2(4, 2)bits': 309,\n",
       " 'layer2.7.conv2(4, 4)bits': 310,\n",
       " 'layer2.7.conv2(4, 8)bits': 311,\n",
       " 'layer2.7.conv2(8, 2)bits': 312,\n",
       " 'layer2.7.conv2(8, 4)bits': 313,\n",
       " 'layer2.7.conv2(8, 8)bits': 314,\n",
       " 'layer2.8.conv1(2, 2)bits': 315,\n",
       " 'layer2.8.conv1(2, 4)bits': 316,\n",
       " 'layer2.8.conv1(2, 8)bits': 317,\n",
       " 'layer2.8.conv1(4, 2)bits': 318,\n",
       " 'layer2.8.conv1(4, 4)bits': 319,\n",
       " 'layer2.8.conv1(4, 8)bits': 320,\n",
       " 'layer2.8.conv1(8, 2)bits': 321,\n",
       " 'layer2.8.conv1(8, 4)bits': 322,\n",
       " 'layer2.8.conv1(8, 8)bits': 323,\n",
       " 'layer2.8.conv2(2, 2)bits': 324,\n",
       " 'layer2.8.conv2(2, 4)bits': 325,\n",
       " 'layer2.8.conv2(2, 8)bits': 326,\n",
       " 'layer2.8.conv2(4, 2)bits': 327,\n",
       " 'layer2.8.conv2(4, 4)bits': 328,\n",
       " 'layer2.8.conv2(4, 8)bits': 329,\n",
       " 'layer2.8.conv2(8, 2)bits': 330,\n",
       " 'layer2.8.conv2(8, 4)bits': 331,\n",
       " 'layer2.8.conv2(8, 8)bits': 332,\n",
       " 'layer3.0.conv1(2, 2)bits': 333,\n",
       " 'layer3.0.conv1(2, 4)bits': 334,\n",
       " 'layer3.0.conv1(2, 8)bits': 335,\n",
       " 'layer3.0.conv1(4, 2)bits': 336,\n",
       " 'layer3.0.conv1(4, 4)bits': 337,\n",
       " 'layer3.0.conv1(4, 8)bits': 338,\n",
       " 'layer3.0.conv1(8, 2)bits': 339,\n",
       " 'layer3.0.conv1(8, 4)bits': 340,\n",
       " 'layer3.0.conv1(8, 8)bits': 341,\n",
       " 'layer3.0.conv2(2, 2)bits': 342,\n",
       " 'layer3.0.conv2(2, 4)bits': 343,\n",
       " 'layer3.0.conv2(2, 8)bits': 344,\n",
       " 'layer3.0.conv2(4, 2)bits': 345,\n",
       " 'layer3.0.conv2(4, 4)bits': 346,\n",
       " 'layer3.0.conv2(4, 8)bits': 347,\n",
       " 'layer3.0.conv2(8, 2)bits': 348,\n",
       " 'layer3.0.conv2(8, 4)bits': 349,\n",
       " 'layer3.0.conv2(8, 8)bits': 350,\n",
       " 'layer3.0.downsample.0(2, 2)bits': 351,\n",
       " 'layer3.0.downsample.0(2, 4)bits': 352,\n",
       " 'layer3.0.downsample.0(2, 8)bits': 353,\n",
       " 'layer3.0.downsample.0(4, 2)bits': 354,\n",
       " 'layer3.0.downsample.0(4, 4)bits': 355,\n",
       " 'layer3.0.downsample.0(4, 8)bits': 356,\n",
       " 'layer3.0.downsample.0(8, 2)bits': 357,\n",
       " 'layer3.0.downsample.0(8, 4)bits': 358,\n",
       " 'layer3.0.downsample.0(8, 8)bits': 359,\n",
       " 'layer3.1.conv1(2, 2)bits': 360,\n",
       " 'layer3.1.conv1(2, 4)bits': 361,\n",
       " 'layer3.1.conv1(2, 8)bits': 362,\n",
       " 'layer3.1.conv1(4, 2)bits': 363,\n",
       " 'layer3.1.conv1(4, 4)bits': 364,\n",
       " 'layer3.1.conv1(4, 8)bits': 365,\n",
       " 'layer3.1.conv1(8, 2)bits': 366,\n",
       " 'layer3.1.conv1(8, 4)bits': 367,\n",
       " 'layer3.1.conv1(8, 8)bits': 368,\n",
       " 'layer3.1.conv2(2, 2)bits': 369,\n",
       " 'layer3.1.conv2(2, 4)bits': 370,\n",
       " 'layer3.1.conv2(2, 8)bits': 371,\n",
       " 'layer3.1.conv2(4, 2)bits': 372,\n",
       " 'layer3.1.conv2(4, 4)bits': 373,\n",
       " 'layer3.1.conv2(4, 8)bits': 374,\n",
       " 'layer3.1.conv2(8, 2)bits': 375,\n",
       " 'layer3.1.conv2(8, 4)bits': 376,\n",
       " 'layer3.1.conv2(8, 8)bits': 377,\n",
       " 'layer3.2.conv1(2, 2)bits': 378,\n",
       " 'layer3.2.conv1(2, 4)bits': 379,\n",
       " 'layer3.2.conv1(2, 8)bits': 380,\n",
       " 'layer3.2.conv1(4, 2)bits': 381,\n",
       " 'layer3.2.conv1(4, 4)bits': 382,\n",
       " 'layer3.2.conv1(4, 8)bits': 383,\n",
       " 'layer3.2.conv1(8, 2)bits': 384,\n",
       " 'layer3.2.conv1(8, 4)bits': 385,\n",
       " 'layer3.2.conv1(8, 8)bits': 386,\n",
       " 'layer3.2.conv2(2, 2)bits': 387,\n",
       " 'layer3.2.conv2(2, 4)bits': 388,\n",
       " 'layer3.2.conv2(2, 8)bits': 389,\n",
       " 'layer3.2.conv2(4, 2)bits': 390,\n",
       " 'layer3.2.conv2(4, 4)bits': 391,\n",
       " 'layer3.2.conv2(4, 8)bits': 392,\n",
       " 'layer3.2.conv2(8, 2)bits': 393,\n",
       " 'layer3.2.conv2(8, 4)bits': 394,\n",
       " 'layer3.2.conv2(8, 8)bits': 395,\n",
       " 'layer3.3.conv1(2, 2)bits': 396,\n",
       " 'layer3.3.conv1(2, 4)bits': 397,\n",
       " 'layer3.3.conv1(2, 8)bits': 398,\n",
       " 'layer3.3.conv1(4, 2)bits': 399,\n",
       " 'layer3.3.conv1(4, 4)bits': 400,\n",
       " 'layer3.3.conv1(4, 8)bits': 401,\n",
       " 'layer3.3.conv1(8, 2)bits': 402,\n",
       " 'layer3.3.conv1(8, 4)bits': 403,\n",
       " 'layer3.3.conv1(8, 8)bits': 404,\n",
       " 'layer3.3.conv2(2, 2)bits': 405,\n",
       " 'layer3.3.conv2(2, 4)bits': 406,\n",
       " 'layer3.3.conv2(2, 8)bits': 407,\n",
       " 'layer3.3.conv2(4, 2)bits': 408,\n",
       " 'layer3.3.conv2(4, 4)bits': 409,\n",
       " 'layer3.3.conv2(4, 8)bits': 410,\n",
       " 'layer3.3.conv2(8, 2)bits': 411,\n",
       " 'layer3.3.conv2(8, 4)bits': 412,\n",
       " 'layer3.3.conv2(8, 8)bits': 413,\n",
       " 'layer3.4.conv1(2, 2)bits': 414,\n",
       " 'layer3.4.conv1(2, 4)bits': 415,\n",
       " 'layer3.4.conv1(2, 8)bits': 416,\n",
       " 'layer3.4.conv1(4, 2)bits': 417,\n",
       " 'layer3.4.conv1(4, 4)bits': 418,\n",
       " 'layer3.4.conv1(4, 8)bits': 419,\n",
       " 'layer3.4.conv1(8, 2)bits': 420,\n",
       " 'layer3.4.conv1(8, 4)bits': 421,\n",
       " 'layer3.4.conv1(8, 8)bits': 422,\n",
       " 'layer3.4.conv2(2, 2)bits': 423,\n",
       " 'layer3.4.conv2(2, 4)bits': 424,\n",
       " 'layer3.4.conv2(2, 8)bits': 425,\n",
       " 'layer3.4.conv2(4, 2)bits': 426,\n",
       " 'layer3.4.conv2(4, 4)bits': 427,\n",
       " 'layer3.4.conv2(4, 8)bits': 428,\n",
       " 'layer3.4.conv2(8, 2)bits': 429,\n",
       " 'layer3.4.conv2(8, 4)bits': 430,\n",
       " 'layer3.4.conv2(8, 8)bits': 431,\n",
       " 'layer3.5.conv1(2, 2)bits': 432,\n",
       " 'layer3.5.conv1(2, 4)bits': 433,\n",
       " 'layer3.5.conv1(2, 8)bits': 434,\n",
       " 'layer3.5.conv1(4, 2)bits': 435,\n",
       " 'layer3.5.conv1(4, 4)bits': 436,\n",
       " 'layer3.5.conv1(4, 8)bits': 437,\n",
       " 'layer3.5.conv1(8, 2)bits': 438,\n",
       " 'layer3.5.conv1(8, 4)bits': 439,\n",
       " 'layer3.5.conv1(8, 8)bits': 440,\n",
       " 'layer3.5.conv2(2, 2)bits': 441,\n",
       " 'layer3.5.conv2(2, 4)bits': 442,\n",
       " 'layer3.5.conv2(2, 8)bits': 443,\n",
       " 'layer3.5.conv2(4, 2)bits': 444,\n",
       " 'layer3.5.conv2(4, 4)bits': 445,\n",
       " 'layer3.5.conv2(4, 8)bits': 446,\n",
       " 'layer3.5.conv2(8, 2)bits': 447,\n",
       " 'layer3.5.conv2(8, 4)bits': 448,\n",
       " 'layer3.5.conv2(8, 8)bits': 449,\n",
       " 'layer3.6.conv1(2, 2)bits': 450,\n",
       " 'layer3.6.conv1(2, 4)bits': 451,\n",
       " 'layer3.6.conv1(2, 8)bits': 452,\n",
       " 'layer3.6.conv1(4, 2)bits': 453,\n",
       " 'layer3.6.conv1(4, 4)bits': 454,\n",
       " 'layer3.6.conv1(4, 8)bits': 455,\n",
       " 'layer3.6.conv1(8, 2)bits': 456,\n",
       " 'layer3.6.conv1(8, 4)bits': 457,\n",
       " 'layer3.6.conv1(8, 8)bits': 458,\n",
       " 'layer3.6.conv2(2, 2)bits': 459,\n",
       " 'layer3.6.conv2(2, 4)bits': 460,\n",
       " 'layer3.6.conv2(2, 8)bits': 461,\n",
       " 'layer3.6.conv2(4, 2)bits': 462,\n",
       " 'layer3.6.conv2(4, 4)bits': 463,\n",
       " 'layer3.6.conv2(4, 8)bits': 464,\n",
       " 'layer3.6.conv2(8, 2)bits': 465,\n",
       " 'layer3.6.conv2(8, 4)bits': 466,\n",
       " 'layer3.6.conv2(8, 8)bits': 467,\n",
       " 'layer3.7.conv1(2, 2)bits': 468,\n",
       " 'layer3.7.conv1(2, 4)bits': 469,\n",
       " 'layer3.7.conv1(2, 8)bits': 470,\n",
       " 'layer3.7.conv1(4, 2)bits': 471,\n",
       " 'layer3.7.conv1(4, 4)bits': 472,\n",
       " 'layer3.7.conv1(4, 8)bits': 473,\n",
       " 'layer3.7.conv1(8, 2)bits': 474,\n",
       " 'layer3.7.conv1(8, 4)bits': 475,\n",
       " 'layer3.7.conv1(8, 8)bits': 476,\n",
       " 'layer3.7.conv2(2, 2)bits': 477,\n",
       " 'layer3.7.conv2(2, 4)bits': 478,\n",
       " 'layer3.7.conv2(2, 8)bits': 479,\n",
       " 'layer3.7.conv2(4, 2)bits': 480,\n",
       " 'layer3.7.conv2(4, 4)bits': 481,\n",
       " 'layer3.7.conv2(4, 8)bits': 482,\n",
       " 'layer3.7.conv2(8, 2)bits': 483,\n",
       " 'layer3.7.conv2(8, 4)bits': 484,\n",
       " 'layer3.7.conv2(8, 8)bits': 485,\n",
       " 'layer3.8.conv1(2, 2)bits': 486,\n",
       " 'layer3.8.conv1(2, 4)bits': 487,\n",
       " 'layer3.8.conv1(2, 8)bits': 488,\n",
       " 'layer3.8.conv1(4, 2)bits': 489,\n",
       " 'layer3.8.conv1(4, 4)bits': 490,\n",
       " 'layer3.8.conv1(4, 8)bits': 491,\n",
       " 'layer3.8.conv1(8, 2)bits': 492,\n",
       " 'layer3.8.conv1(8, 4)bits': 493,\n",
       " 'layer3.8.conv1(8, 8)bits': 494,\n",
       " 'layer3.8.conv2(2, 2)bits': 495,\n",
       " 'layer3.8.conv2(2, 4)bits': 496,\n",
       " 'layer3.8.conv2(2, 8)bits': 497,\n",
       " 'layer3.8.conv2(4, 2)bits': 498,\n",
       " 'layer3.8.conv2(4, 4)bits': 499,\n",
       " 'layer3.8.conv2(4, 8)bits': 500,\n",
       " 'layer3.8.conv2(8, 2)bits': 501,\n",
       " 'layer3.8.conv2(8, 4)bits': 502,\n",
       " 'layer3.8.conv2(8, 8)bits': 503}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hm['layer_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "561a7c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(504, 504)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hm['Ltilde'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf87d3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0 layer layer1.0.conv1 scheme (2, 2)bits Ltilde 2.323860\n",
      "index 1 layer layer1.0.conv1 scheme (2, 4)bits Ltilde 1.421405\n",
      "index 2 layer layer1.0.conv1 scheme (2, 8)bits Ltilde 1.417552\n",
      "index 3 layer layer1.0.conv1 scheme (4, 2)bits Ltilde 1.187883\n",
      "index 4 layer layer1.0.conv1 scheme (4, 4)bits Ltilde 0.374167\n",
      "index 5 layer layer1.0.conv1 scheme (4, 8)bits Ltilde 0.386472\n",
      "index 6 layer layer1.0.conv1 scheme (8, 2)bits Ltilde 1.028558\n",
      "index 7 layer layer1.0.conv1 scheme (8, 4)bits Ltilde 0.182747\n",
      "index 8 layer layer1.0.conv1 scheme (8, 8)bits Ltilde 0.196041\n",
      "index 9 layer layer1.0.conv2 scheme (2, 2)bits Ltilde 0.587505\n",
      "index 10 layer layer1.0.conv2 scheme (2, 4)bits Ltilde 0.346233\n",
      "index 11 layer layer1.0.conv2 scheme (2, 8)bits Ltilde 0.365954\n",
      "index 12 layer layer1.0.conv2 scheme (4, 2)bits Ltilde 0.474257\n",
      "index 13 layer layer1.0.conv2 scheme (4, 4)bits Ltilde 0.198513\n",
      "index 14 layer layer1.0.conv2 scheme (4, 8)bits Ltilde 0.210499\n",
      "index 15 layer layer1.0.conv2 scheme (8, 2)bits Ltilde 0.451126\n",
      "index 16 layer layer1.0.conv2 scheme (8, 4)bits Ltilde 0.179892\n",
      "index 17 layer layer1.0.conv2 scheme (8, 8)bits Ltilde 0.192742\n",
      "index 18 layer layer1.1.conv1 scheme (2, 2)bits Ltilde 1.072661\n",
      "index 19 layer layer1.1.conv1 scheme (2, 4)bits Ltilde 0.894945\n",
      "index 20 layer layer1.1.conv1 scheme (2, 8)bits Ltilde 0.917900\n",
      "index 21 layer layer1.1.conv1 scheme (4, 2)bits Ltilde 0.463913\n",
      "index 22 layer layer1.1.conv1 scheme (4, 4)bits Ltilde 0.258905\n",
      "index 23 layer layer1.1.conv1 scheme (4, 8)bits Ltilde 0.263400\n",
      "index 24 layer layer1.1.conv1 scheme (8, 2)bits Ltilde 0.410358\n",
      "index 25 layer layer1.1.conv1 scheme (8, 4)bits Ltilde 0.191226\n",
      "index 26 layer layer1.1.conv1 scheme (8, 8)bits Ltilde 0.194515\n",
      "index 27 layer layer1.1.conv2 scheme (2, 2)bits Ltilde 0.394754\n",
      "index 28 layer layer1.1.conv2 scheme (2, 4)bits Ltilde 0.248051\n",
      "index 29 layer layer1.1.conv2 scheme (2, 8)bits Ltilde 0.265431\n",
      "index 30 layer layer1.1.conv2 scheme (4, 2)bits Ltilde 0.315573\n",
      "index 31 layer layer1.1.conv2 scheme (4, 4)bits Ltilde 0.175202\n",
      "index 32 layer layer1.1.conv2 scheme (4, 8)bits Ltilde 0.202109\n",
      "index 33 layer layer1.1.conv2 scheme (8, 2)bits Ltilde 0.306446\n",
      "index 34 layer layer1.1.conv2 scheme (8, 4)bits Ltilde 0.166723\n",
      "index 35 layer layer1.1.conv2 scheme (8, 8)bits Ltilde 0.192804\n",
      "index 36 layer layer1.2.conv1 scheme (2, 2)bits Ltilde 0.740007\n",
      "index 37 layer layer1.2.conv1 scheme (2, 4)bits Ltilde 0.715454\n",
      "index 38 layer layer1.2.conv1 scheme (2, 8)bits Ltilde 0.720840\n",
      "index 39 layer layer1.2.conv1 scheme (4, 2)bits Ltilde 0.267959\n",
      "index 40 layer layer1.2.conv1 scheme (4, 4)bits Ltilde 0.231593\n",
      "index 41 layer layer1.2.conv1 scheme (4, 8)bits Ltilde 0.240337\n",
      "index 42 layer layer1.2.conv1 scheme (8, 2)bits Ltilde 0.233766\n",
      "index 43 layer layer1.2.conv1 scheme (8, 4)bits Ltilde 0.187429\n",
      "index 44 layer layer1.2.conv1 scheme (8, 8)bits Ltilde 0.193365\n",
      "index 45 layer layer1.2.conv2 scheme (2, 2)bits Ltilde 0.198106\n",
      "index 46 layer layer1.2.conv2 scheme (2, 4)bits Ltilde 0.195260\n",
      "index 47 layer layer1.2.conv2 scheme (2, 8)bits Ltilde 0.205234\n",
      "index 48 layer layer1.2.conv2 scheme (4, 2)bits Ltilde 0.191118\n",
      "index 49 layer layer1.2.conv2 scheme (4, 4)bits Ltilde 0.185751\n",
      "index 50 layer layer1.2.conv2 scheme (4, 8)bits Ltilde 0.197435\n",
      "index 51 layer layer1.2.conv2 scheme (8, 2)bits Ltilde 0.189965\n",
      "index 52 layer layer1.2.conv2 scheme (8, 4)bits Ltilde 0.183045\n",
      "index 53 layer layer1.2.conv2 scheme (8, 8)bits Ltilde 0.192870\n",
      "index 54 layer layer1.3.conv1 scheme (2, 2)bits Ltilde 0.894536\n",
      "index 55 layer layer1.3.conv1 scheme (2, 4)bits Ltilde 0.710588\n",
      "index 56 layer layer1.3.conv1 scheme (2, 8)bits Ltilde 0.698618\n",
      "index 57 layer layer1.3.conv1 scheme (4, 2)bits Ltilde 0.420490\n",
      "index 58 layer layer1.3.conv1 scheme (4, 4)bits Ltilde 0.246983\n",
      "index 59 layer layer1.3.conv1 scheme (4, 8)bits Ltilde 0.247082\n",
      "index 60 layer layer1.3.conv1 scheme (8, 2)bits Ltilde 0.390958\n",
      "index 61 layer layer1.3.conv1 scheme (8, 4)bits Ltilde 0.197075\n",
      "index 62 layer layer1.3.conv1 scheme (8, 8)bits Ltilde 0.192778\n",
      "index 63 layer layer1.3.conv2 scheme (2, 2)bits Ltilde 0.297074\n",
      "index 64 layer layer1.3.conv2 scheme (2, 4)bits Ltilde 0.210737\n",
      "index 65 layer layer1.3.conv2 scheme (2, 8)bits Ltilde 0.209458\n",
      "index 66 layer layer1.3.conv2 scheme (4, 2)bits Ltilde 0.285975\n",
      "index 67 layer layer1.3.conv2 scheme (4, 4)bits Ltilde 0.195358\n",
      "index 68 layer layer1.3.conv2 scheme (4, 8)bits Ltilde 0.193061\n",
      "index 69 layer layer1.3.conv2 scheme (8, 2)bits Ltilde 0.286393\n",
      "index 70 layer layer1.3.conv2 scheme (8, 4)bits Ltilde 0.194895\n",
      "index 71 layer layer1.3.conv2 scheme (8, 8)bits Ltilde 0.192820\n",
      "index 72 layer layer1.4.conv1 scheme (2, 2)bits Ltilde 0.860152\n",
      "index 73 layer layer1.4.conv1 scheme (2, 4)bits Ltilde 0.780640\n",
      "index 74 layer layer1.4.conv1 scheme (2, 8)bits Ltilde 0.765531\n",
      "index 75 layer layer1.4.conv1 scheme (4, 2)bits Ltilde 0.324008\n",
      "index 76 layer layer1.4.conv1 scheme (4, 4)bits Ltilde 0.242917\n",
      "index 77 layer layer1.4.conv1 scheme (4, 8)bits Ltilde 0.228648\n",
      "index 78 layer layer1.4.conv1 scheme (8, 2)bits Ltilde 0.299009\n",
      "index 79 layer layer1.4.conv1 scheme (8, 4)bits Ltilde 0.209059\n",
      "index 80 layer layer1.4.conv1 scheme (8, 8)bits Ltilde 0.192200\n",
      "index 81 layer layer1.4.conv2 scheme (2, 2)bits Ltilde 0.263468\n",
      "index 82 layer layer1.4.conv2 scheme (2, 4)bits Ltilde 0.203324\n",
      "index 83 layer layer1.4.conv2 scheme (2, 8)bits Ltilde 0.209307\n",
      "index 84 layer layer1.4.conv2 scheme (4, 2)bits Ltilde 0.249679\n",
      "index 85 layer layer1.4.conv2 scheme (4, 4)bits Ltilde 0.190013\n",
      "index 86 layer layer1.4.conv2 scheme (4, 8)bits Ltilde 0.196582\n",
      "index 87 layer layer1.4.conv2 scheme (8, 2)bits Ltilde 0.246231\n",
      "index 88 layer layer1.4.conv2 scheme (8, 4)bits Ltilde 0.186158\n",
      "index 89 layer layer1.4.conv2 scheme (8, 8)bits Ltilde 0.192803\n",
      "index 90 layer layer1.5.conv1 scheme (2, 2)bits Ltilde 0.714654\n",
      "index 91 layer layer1.5.conv1 scheme (2, 4)bits Ltilde 0.717526\n",
      "index 92 layer layer1.5.conv1 scheme (2, 8)bits Ltilde 0.707097\n",
      "index 93 layer layer1.5.conv1 scheme (4, 2)bits Ltilde 0.244844\n",
      "index 94 layer layer1.5.conv1 scheme (4, 4)bits Ltilde 0.244148\n",
      "index 95 layer layer1.5.conv1 scheme (4, 8)bits Ltilde 0.219628\n",
      "index 96 layer layer1.5.conv1 scheme (8, 2)bits Ltilde 0.219714\n",
      "index 97 layer layer1.5.conv1 scheme (8, 4)bits Ltilde 0.216797\n",
      "index 98 layer layer1.5.conv1 scheme (8, 8)bits Ltilde 0.193085\n",
      "index 99 layer layer1.5.conv2 scheme (2, 2)bits Ltilde 0.180800\n",
      "index 100 layer layer1.5.conv2 scheme (2, 4)bits Ltilde 0.184098\n",
      "index 101 layer layer1.5.conv2 scheme (2, 8)bits Ltilde 0.196678\n",
      "index 102 layer layer1.5.conv2 scheme (4, 2)bits Ltilde 0.174449\n",
      "index 103 layer layer1.5.conv2 scheme (4, 4)bits Ltilde 0.177910\n",
      "index 104 layer layer1.5.conv2 scheme (4, 8)bits Ltilde 0.192331\n",
      "index 105 layer layer1.5.conv2 scheme (8, 2)bits Ltilde 0.173944\n",
      "index 106 layer layer1.5.conv2 scheme (8, 4)bits Ltilde 0.177923\n",
      "index 107 layer layer1.5.conv2 scheme (8, 8)bits Ltilde 0.192755\n",
      "index 108 layer layer1.6.conv1 scheme (2, 2)bits Ltilde 0.810832\n",
      "index 109 layer layer1.6.conv1 scheme (2, 4)bits Ltilde 0.667913\n",
      "index 110 layer layer1.6.conv1 scheme (2, 8)bits Ltilde 0.637483\n",
      "index 111 layer layer1.6.conv1 scheme (4, 2)bits Ltilde 0.284537\n",
      "index 112 layer layer1.6.conv1 scheme (4, 4)bits Ltilde 0.217692\n",
      "index 113 layer layer1.6.conv1 scheme (4, 8)bits Ltilde 0.202290\n",
      "index 114 layer layer1.6.conv1 scheme (8, 2)bits Ltilde 0.264043\n",
      "index 115 layer layer1.6.conv1 scheme (8, 4)bits Ltilde 0.207155\n",
      "index 116 layer layer1.6.conv1 scheme (8, 8)bits Ltilde 0.192771\n",
      "index 117 layer layer1.6.conv2 scheme (2, 2)bits Ltilde 0.214440\n",
      "index 118 layer layer1.6.conv2 scheme (2, 4)bits Ltilde 0.192948\n",
      "index 119 layer layer1.6.conv2 scheme (2, 8)bits Ltilde 0.194756\n",
      "index 120 layer layer1.6.conv2 scheme (4, 2)bits Ltilde 0.206470\n",
      "index 121 layer layer1.6.conv2 scheme (4, 4)bits Ltilde 0.189315\n",
      "index 122 layer layer1.6.conv2 scheme (4, 8)bits Ltilde 0.192617\n",
      "index 123 layer layer1.6.conv2 scheme (8, 2)bits Ltilde 0.206418\n",
      "index 124 layer layer1.6.conv2 scheme (8, 4)bits Ltilde 0.189355\n",
      "index 125 layer layer1.6.conv2 scheme (8, 8)bits Ltilde 0.192993\n",
      "index 126 layer layer1.7.conv1 scheme (2, 2)bits Ltilde 0.683153\n",
      "index 127 layer layer1.7.conv1 scheme (2, 4)bits Ltilde 0.612921\n",
      "index 128 layer layer1.7.conv1 scheme (2, 8)bits Ltilde 0.625754\n",
      "index 129 layer layer1.7.conv1 scheme (4, 2)bits Ltilde 0.269921\n",
      "index 130 layer layer1.7.conv1 scheme (4, 4)bits Ltilde 0.200212\n",
      "index 131 layer layer1.7.conv1 scheme (4, 8)bits Ltilde 0.210855\n",
      "index 132 layer layer1.7.conv1 scheme (8, 2)bits Ltilde 0.258851\n",
      "index 133 layer layer1.7.conv1 scheme (8, 4)bits Ltilde 0.183490\n",
      "index 134 layer layer1.7.conv1 scheme (8, 8)bits Ltilde 0.193802\n",
      "index 135 layer layer1.7.conv2 scheme (2, 2)bits Ltilde 0.187572\n",
      "index 136 layer layer1.7.conv2 scheme (2, 4)bits Ltilde 0.193522\n",
      "index 137 layer layer1.7.conv2 scheme (2, 8)bits Ltilde 0.200924\n",
      "index 138 layer layer1.7.conv2 scheme (4, 2)bits Ltilde 0.181527\n",
      "index 139 layer layer1.7.conv2 scheme (4, 4)bits Ltilde 0.185966\n",
      "index 140 layer layer1.7.conv2 scheme (4, 8)bits Ltilde 0.193405\n",
      "index 141 layer layer1.7.conv2 scheme (8, 2)bits Ltilde 0.180948\n",
      "index 142 layer layer1.7.conv2 scheme (8, 4)bits Ltilde 0.185094\n",
      "index 143 layer layer1.7.conv2 scheme (8, 8)bits Ltilde 0.192657\n",
      "index 144 layer layer1.8.conv1 scheme (2, 2)bits Ltilde 0.861925\n",
      "index 145 layer layer1.8.conv1 scheme (2, 4)bits Ltilde 0.611125\n",
      "index 146 layer layer1.8.conv1 scheme (2, 8)bits Ltilde 0.604857\n",
      "index 147 layer layer1.8.conv1 scheme (4, 2)bits Ltilde 0.529493\n",
      "index 148 layer layer1.8.conv1 scheme (4, 4)bits Ltilde 0.218421\n",
      "index 149 layer layer1.8.conv1 scheme (4, 8)bits Ltilde 0.206401\n",
      "index 150 layer layer1.8.conv1 scheme (8, 2)bits Ltilde 0.510742\n",
      "index 151 layer layer1.8.conv1 scheme (8, 4)bits Ltilde 0.208308\n",
      "index 152 layer layer1.8.conv1 scheme (8, 8)bits Ltilde 0.193037\n",
      "index 153 layer layer1.8.conv2 scheme (2, 2)bits Ltilde 0.211574\n",
      "index 154 layer layer1.8.conv2 scheme (2, 4)bits Ltilde 0.176640\n",
      "index 155 layer layer1.8.conv2 scheme (2, 8)bits Ltilde 0.178726\n",
      "index 156 layer layer1.8.conv2 scheme (4, 2)bits Ltilde 0.212429\n",
      "index 157 layer layer1.8.conv2 scheme (4, 4)bits Ltilde 0.183013\n",
      "index 158 layer layer1.8.conv2 scheme (4, 8)bits Ltilde 0.191058\n",
      "index 159 layer layer1.8.conv2 scheme (8, 2)bits Ltilde 0.213255\n",
      "index 160 layer layer1.8.conv2 scheme (8, 4)bits Ltilde 0.184141\n",
      "index 161 layer layer1.8.conv2 scheme (8, 8)bits Ltilde 0.193093\n",
      "index 162 layer layer2.0.conv1 scheme (2, 2)bits Ltilde 2.600348\n",
      "index 163 layer layer2.0.conv1 scheme (2, 4)bits Ltilde 0.767988\n",
      "index 164 layer layer2.0.conv1 scheme (2, 8)bits Ltilde 0.571378\n",
      "index 165 layer layer2.0.conv1 scheme (4, 2)bits Ltilde 1.851831\n",
      "index 166 layer layer2.0.conv1 scheme (4, 4)bits Ltilde 0.384320\n",
      "index 167 layer layer2.0.conv1 scheme (4, 8)bits Ltilde 0.208452\n",
      "index 168 layer layer2.0.conv1 scheme (8, 2)bits Ltilde 1.823254\n",
      "index 169 layer layer2.0.conv1 scheme (8, 4)bits Ltilde 0.367355\n",
      "index 170 layer layer2.0.conv1 scheme (8, 8)bits Ltilde 0.193165\n",
      "index 171 layer layer2.0.conv2 scheme (2, 2)bits Ltilde 2.573986\n",
      "index 172 layer layer2.0.conv2 scheme (2, 4)bits Ltilde 0.499264\n",
      "index 173 layer layer2.0.conv2 scheme (2, 8)bits Ltilde 0.275552\n",
      "index 174 layer layer2.0.conv2 scheme (4, 2)bits Ltilde 2.431427\n",
      "index 175 layer layer2.0.conv2 scheme (4, 4)bits Ltilde 0.416781\n",
      "index 176 layer layer2.0.conv2 scheme (4, 8)bits Ltilde 0.205209\n",
      "index 177 layer layer2.0.conv2 scheme (8, 2)bits Ltilde 2.419000\n",
      "index 178 layer layer2.0.conv2 scheme (8, 4)bits Ltilde 0.409782\n",
      "index 179 layer layer2.0.conv2 scheme (8, 8)bits Ltilde 0.197418\n",
      "index 180 layer layer2.0.downsample.0 scheme (2, 2)bits Ltilde 3.683901\n",
      "index 181 layer layer2.0.downsample.0 scheme (2, 4)bits Ltilde 0.933781\n",
      "index 182 layer layer2.0.downsample.0 scheme (2, 8)bits Ltilde 0.563772\n",
      "index 183 layer layer2.0.downsample.0 scheme (4, 2)bits Ltilde 3.183841\n",
      "index 184 layer layer2.0.downsample.0 scheme (4, 4)bits Ltilde 0.482717\n",
      "index 185 layer layer2.0.downsample.0 scheme (4, 8)bits Ltilde 0.202827\n",
      "index 186 layer layer2.0.downsample.0 scheme (8, 2)bits Ltilde 3.152477\n",
      "index 187 layer layer2.0.downsample.0 scheme (8, 4)bits Ltilde 0.458666\n",
      "index 188 layer layer2.0.downsample.0 scheme (8, 8)bits Ltilde 0.187284\n",
      "index 189 layer layer2.1.conv1 scheme (2, 2)bits Ltilde 0.849238\n",
      "index 190 layer layer2.1.conv1 scheme (2, 4)bits Ltilde 0.546433\n",
      "index 191 layer layer2.1.conv1 scheme (2, 8)bits Ltilde 0.535530\n",
      "index 192 layer layer2.1.conv1 scheme (4, 2)bits Ltilde 0.428815\n",
      "index 193 layer layer2.1.conv1 scheme (4, 4)bits Ltilde 0.208585\n",
      "index 194 layer layer2.1.conv1 scheme (4, 8)bits Ltilde 0.207348\n",
      "index 195 layer layer2.1.conv1 scheme (8, 2)bits Ltilde 0.406843\n",
      "index 196 layer layer2.1.conv1 scheme (8, 4)bits Ltilde 0.193387\n",
      "index 197 layer layer2.1.conv1 scheme (8, 8)bits Ltilde 0.192335\n",
      "index 198 layer layer2.1.conv2 scheme (2, 2)bits Ltilde 0.277469\n",
      "index 199 layer layer2.1.conv2 scheme (2, 4)bits Ltilde 0.208942\n",
      "index 200 layer layer2.1.conv2 scheme (2, 8)bits Ltilde 0.205819\n",
      "index 201 layer layer2.1.conv2 scheme (4, 2)bits Ltilde 0.267657\n",
      "index 202 layer layer2.1.conv2 scheme (4, 4)bits Ltilde 0.196981\n",
      "index 203 layer layer2.1.conv2 scheme (4, 8)bits Ltilde 0.193706\n",
      "index 204 layer layer2.1.conv2 scheme (8, 2)bits Ltilde 0.267166\n",
      "index 205 layer layer2.1.conv2 scheme (8, 4)bits Ltilde 0.195941\n",
      "index 206 layer layer2.1.conv2 scheme (8, 8)bits Ltilde 0.192747\n",
      "index 207 layer layer2.2.conv1 scheme (2, 2)bits Ltilde 0.795405\n",
      "index 208 layer layer2.2.conv1 scheme (2, 4)bits Ltilde 0.490380\n",
      "index 209 layer layer2.2.conv1 scheme (2, 8)bits Ltilde 0.490522\n",
      "index 210 layer layer2.2.conv1 scheme (4, 2)bits Ltilde 0.561448\n",
      "index 211 layer layer2.2.conv1 scheme (4, 4)bits Ltilde 0.227430\n",
      "index 212 layer layer2.2.conv1 scheme (4, 8)bits Ltilde 0.213050\n",
      "index 213 layer layer2.2.conv1 scheme (8, 2)bits Ltilde 0.548254\n",
      "index 214 layer layer2.2.conv1 scheme (8, 4)bits Ltilde 0.206726\n",
      "index 215 layer layer2.2.conv1 scheme (8, 8)bits Ltilde 0.192080\n",
      "index 216 layer layer2.2.conv2 scheme (2, 2)bits Ltilde 0.264165\n",
      "index 217 layer layer2.2.conv2 scheme (2, 4)bits Ltilde 0.208851\n",
      "index 218 layer layer2.2.conv2 scheme (2, 8)bits Ltilde 0.207349\n",
      "index 219 layer layer2.2.conv2 scheme (4, 2)bits Ltilde 0.253399\n",
      "index 220 layer layer2.2.conv2 scheme (4, 4)bits Ltilde 0.194753\n",
      "index 221 layer layer2.2.conv2 scheme (4, 8)bits Ltilde 0.193893\n",
      "index 222 layer layer2.2.conv2 scheme (8, 2)bits Ltilde 0.252256\n",
      "index 223 layer layer2.2.conv2 scheme (8, 4)bits Ltilde 0.193266\n",
      "index 224 layer layer2.2.conv2 scheme (8, 8)bits Ltilde 0.192529\n",
      "index 225 layer layer2.3.conv1 scheme (2, 2)bits Ltilde 0.815735\n",
      "index 226 layer layer2.3.conv1 scheme (2, 4)bits Ltilde 0.521913\n",
      "index 227 layer layer2.3.conv1 scheme (2, 8)bits Ltilde 0.493305\n",
      "index 228 layer layer2.3.conv1 scheme (4, 2)bits Ltilde 0.498154\n",
      "index 229 layer layer2.3.conv1 scheme (4, 4)bits Ltilde 0.222824\n",
      "index 230 layer layer2.3.conv1 scheme (4, 8)bits Ltilde 0.205691\n",
      "index 231 layer layer2.3.conv1 scheme (8, 2)bits Ltilde 0.467129\n",
      "index 232 layer layer2.3.conv1 scheme (8, 4)bits Ltilde 0.206417\n",
      "index 233 layer layer2.3.conv1 scheme (8, 8)bits Ltilde 0.192989\n",
      "index 234 layer layer2.3.conv2 scheme (2, 2)bits Ltilde 0.234177\n",
      "index 235 layer layer2.3.conv2 scheme (2, 4)bits Ltilde 0.201245\n",
      "index 236 layer layer2.3.conv2 scheme (2, 8)bits Ltilde 0.203568\n",
      "index 237 layer layer2.3.conv2 scheme (4, 2)bits Ltilde 0.227693\n",
      "index 238 layer layer2.3.conv2 scheme (4, 4)bits Ltilde 0.190742\n",
      "index 239 layer layer2.3.conv2 scheme (4, 8)bits Ltilde 0.193469\n",
      "index 240 layer layer2.3.conv2 scheme (8, 2)bits Ltilde 0.226882\n",
      "index 241 layer layer2.3.conv2 scheme (8, 4)bits Ltilde 0.189548\n",
      "index 242 layer layer2.3.conv2 scheme (8, 8)bits Ltilde 0.192430\n",
      "index 243 layer layer2.4.conv1 scheme (2, 2)bits Ltilde 0.834253\n",
      "index 244 layer layer2.4.conv1 scheme (2, 4)bits Ltilde 0.519543\n",
      "index 245 layer layer2.4.conv1 scheme (2, 8)bits Ltilde 0.483149\n",
      "index 246 layer layer2.4.conv1 scheme (4, 2)bits Ltilde 0.518350\n",
      "index 247 layer layer2.4.conv1 scheme (4, 4)bits Ltilde 0.230529\n",
      "index 248 layer layer2.4.conv1 scheme (4, 8)bits Ltilde 0.207277\n",
      "index 249 layer layer2.4.conv1 scheme (8, 2)bits Ltilde 0.490294\n",
      "index 250 layer layer2.4.conv1 scheme (8, 4)bits Ltilde 0.214538\n",
      "index 251 layer layer2.4.conv1 scheme (8, 8)bits Ltilde 0.194062\n",
      "index 252 layer layer2.4.conv2 scheme (2, 2)bits Ltilde 0.251387\n",
      "index 253 layer layer2.4.conv2 scheme (2, 4)bits Ltilde 0.207555\n",
      "index 254 layer layer2.4.conv2 scheme (2, 8)bits Ltilde 0.203164\n",
      "index 255 layer layer2.4.conv2 scheme (4, 2)bits Ltilde 0.241525\n",
      "index 256 layer layer2.4.conv2 scheme (4, 4)bits Ltilde 0.198045\n",
      "index 257 layer layer2.4.conv2 scheme (4, 8)bits Ltilde 0.193451\n",
      "index 258 layer layer2.4.conv2 scheme (8, 2)bits Ltilde 0.240764\n",
      "index 259 layer layer2.4.conv2 scheme (8, 4)bits Ltilde 0.197190\n",
      "index 260 layer layer2.4.conv2 scheme (8, 8)bits Ltilde 0.192943\n",
      "index 261 layer layer2.5.conv1 scheme (2, 2)bits Ltilde 0.781092\n",
      "index 262 layer layer2.5.conv1 scheme (2, 4)bits Ltilde 0.485084\n",
      "index 263 layer layer2.5.conv1 scheme (2, 8)bits Ltilde 0.472035\n",
      "index 264 layer layer2.5.conv1 scheme (4, 2)bits Ltilde 0.519330\n",
      "index 265 layer layer2.5.conv1 scheme (4, 4)bits Ltilde 0.227292\n",
      "index 266 layer layer2.5.conv1 scheme (4, 8)bits Ltilde 0.211638\n",
      "index 267 layer layer2.5.conv1 scheme (8, 2)bits Ltilde 0.500619\n",
      "index 268 layer layer2.5.conv1 scheme (8, 4)bits Ltilde 0.209167\n",
      "index 269 layer layer2.5.conv1 scheme (8, 8)bits Ltilde 0.193110\n",
      "index 270 layer layer2.5.conv2 scheme (2, 2)bits Ltilde 0.243411\n",
      "index 271 layer layer2.5.conv2 scheme (2, 4)bits Ltilde 0.202331\n",
      "index 272 layer layer2.5.conv2 scheme (2, 8)bits Ltilde 0.199306\n",
      "index 273 layer layer2.5.conv2 scheme (4, 2)bits Ltilde 0.237919\n",
      "index 274 layer layer2.5.conv2 scheme (4, 4)bits Ltilde 0.196048\n",
      "index 275 layer layer2.5.conv2 scheme (4, 8)bits Ltilde 0.193506\n",
      "index 276 layer layer2.5.conv2 scheme (8, 2)bits Ltilde 0.237382\n",
      "index 277 layer layer2.5.conv2 scheme (8, 4)bits Ltilde 0.195280\n",
      "index 278 layer layer2.5.conv2 scheme (8, 8)bits Ltilde 0.192759\n",
      "index 279 layer layer2.6.conv1 scheme (2, 2)bits Ltilde 0.803075\n",
      "index 280 layer layer2.6.conv1 scheme (2, 4)bits Ltilde 0.489989\n",
      "index 281 layer layer2.6.conv1 scheme (2, 8)bits Ltilde 0.439197\n",
      "index 282 layer layer2.6.conv1 scheme (4, 2)bits Ltilde 0.587083\n",
      "index 283 layer layer2.6.conv1 scheme (4, 4)bits Ltilde 0.263832\n",
      "index 284 layer layer2.6.conv1 scheme (4, 8)bits Ltilde 0.210289\n",
      "index 285 layer layer2.6.conv1 scheme (8, 2)bits Ltilde 0.559084\n",
      "index 286 layer layer2.6.conv1 scheme (8, 4)bits Ltilde 0.241130\n",
      "index 287 layer layer2.6.conv1 scheme (8, 8)bits Ltilde 0.189970\n",
      "index 288 layer layer2.6.conv2 scheme (2, 2)bits Ltilde 0.300183\n",
      "index 289 layer layer2.6.conv2 scheme (2, 4)bits Ltilde 0.209264\n",
      "index 290 layer layer2.6.conv2 scheme (2, 8)bits Ltilde 0.204777\n",
      "index 291 layer layer2.6.conv2 scheme (4, 2)bits Ltilde 0.291269\n",
      "index 292 layer layer2.6.conv2 scheme (4, 4)bits Ltilde 0.196825\n",
      "index 293 layer layer2.6.conv2 scheme (4, 8)bits Ltilde 0.193057\n",
      "index 294 layer layer2.6.conv2 scheme (8, 2)bits Ltilde 0.291752\n",
      "index 295 layer layer2.6.conv2 scheme (8, 4)bits Ltilde 0.196775\n",
      "index 296 layer layer2.6.conv2 scheme (8, 8)bits Ltilde 0.192931\n",
      "index 297 layer layer2.7.conv1 scheme (2, 2)bits Ltilde 0.628822\n",
      "index 298 layer layer2.7.conv1 scheme (2, 4)bits Ltilde 0.464729\n",
      "index 299 layer layer2.7.conv1 scheme (2, 8)bits Ltilde 0.450110\n",
      "index 300 layer layer2.7.conv1 scheme (4, 2)bits Ltilde 0.434256\n",
      "index 301 layer layer2.7.conv1 scheme (4, 4)bits Ltilde 0.223986\n",
      "index 302 layer layer2.7.conv1 scheme (4, 8)bits Ltilde 0.207898\n",
      "index 303 layer layer2.7.conv1 scheme (8, 2)bits Ltilde 0.417681\n",
      "index 304 layer layer2.7.conv1 scheme (8, 4)bits Ltilde 0.209836\n",
      "index 305 layer layer2.7.conv1 scheme (8, 8)bits Ltilde 0.194481\n",
      "index 306 layer layer2.7.conv2 scheme (2, 2)bits Ltilde 0.256942\n",
      "index 307 layer layer2.7.conv2 scheme (2, 4)bits Ltilde 0.207705\n",
      "index 308 layer layer2.7.conv2 scheme (2, 8)bits Ltilde 0.200731\n",
      "index 309 layer layer2.7.conv2 scheme (4, 2)bits Ltilde 0.250775\n",
      "index 310 layer layer2.7.conv2 scheme (4, 4)bits Ltilde 0.199615\n",
      "index 311 layer layer2.7.conv2 scheme (4, 8)bits Ltilde 0.193130\n",
      "index 312 layer layer2.7.conv2 scheme (8, 2)bits Ltilde 0.249859\n",
      "index 313 layer layer2.7.conv2 scheme (8, 4)bits Ltilde 0.199178\n",
      "index 314 layer layer2.7.conv2 scheme (8, 8)bits Ltilde 0.192971\n",
      "index 315 layer layer2.8.conv1 scheme (2, 2)bits Ltilde 0.657521\n",
      "index 316 layer layer2.8.conv1 scheme (2, 4)bits Ltilde 0.438875\n",
      "index 317 layer layer2.8.conv1 scheme (2, 8)bits Ltilde 0.404001\n",
      "index 318 layer layer2.8.conv1 scheme (4, 2)bits Ltilde 0.536431\n",
      "index 319 layer layer2.8.conv1 scheme (4, 4)bits Ltilde 0.248977\n",
      "index 320 layer layer2.8.conv1 scheme (4, 8)bits Ltilde 0.204169\n",
      "index 321 layer layer2.8.conv1 scheme (8, 2)bits Ltilde 0.528204\n",
      "index 322 layer layer2.8.conv1 scheme (8, 4)bits Ltilde 0.236483\n",
      "index 323 layer layer2.8.conv1 scheme (8, 8)bits Ltilde 0.194170\n",
      "index 324 layer layer2.8.conv2 scheme (2, 2)bits Ltilde 0.295220\n",
      "index 325 layer layer2.8.conv2 scheme (2, 4)bits Ltilde 0.214551\n",
      "index 326 layer layer2.8.conv2 scheme (2, 8)bits Ltilde 0.204120\n",
      "index 327 layer layer2.8.conv2 scheme (4, 2)bits Ltilde 0.286289\n",
      "index 328 layer layer2.8.conv2 scheme (4, 4)bits Ltilde 0.204219\n",
      "index 329 layer layer2.8.conv2 scheme (4, 8)bits Ltilde 0.194323\n",
      "index 330 layer layer2.8.conv2 scheme (8, 2)bits Ltilde 0.285243\n",
      "index 331 layer layer2.8.conv2 scheme (8, 4)bits Ltilde 0.203153\n",
      "index 332 layer layer2.8.conv2 scheme (8, 8)bits Ltilde 0.193289\n",
      "index 333 layer layer3.0.conv1 scheme (2, 2)bits Ltilde 4.121852\n",
      "index 334 layer layer3.0.conv1 scheme (2, 4)bits Ltilde 0.547751\n",
      "index 335 layer layer3.0.conv1 scheme (2, 8)bits Ltilde 0.391523\n",
      "index 336 layer layer3.0.conv1 scheme (4, 2)bits Ltilde 3.770680\n",
      "index 337 layer layer3.0.conv1 scheme (4, 4)bits Ltilde 0.324732\n",
      "index 338 layer layer3.0.conv1 scheme (4, 8)bits Ltilde 0.206899\n",
      "index 339 layer layer3.0.conv1 scheme (8, 2)bits Ltilde 3.734431\n",
      "index 340 layer layer3.0.conv1 scheme (8, 4)bits Ltilde 0.304044\n",
      "index 341 layer layer3.0.conv1 scheme (8, 8)bits Ltilde 0.192733\n",
      "index 342 layer layer3.0.conv2 scheme (2, 2)bits Ltilde 2.030928\n",
      "index 343 layer layer3.0.conv2 scheme (2, 4)bits Ltilde 0.320264\n",
      "index 344 layer layer3.0.conv2 scheme (2, 8)bits Ltilde 0.250640\n",
      "index 345 layer layer3.0.conv2 scheme (4, 2)bits Ltilde 1.937619\n",
      "index 346 layer layer3.0.conv2 scheme (4, 4)bits Ltilde 0.247733\n",
      "index 347 layer layer3.0.conv2 scheme (4, 8)bits Ltilde 0.194132\n",
      "index 348 layer layer3.0.conv2 scheme (8, 2)bits Ltilde 1.927257\n",
      "index 349 layer layer3.0.conv2 scheme (8, 4)bits Ltilde 0.244259\n",
      "index 350 layer layer3.0.conv2 scheme (8, 8)bits Ltilde 0.192554\n",
      "index 351 layer layer3.0.downsample.0 scheme (2, 2)bits Ltilde 0.922549\n",
      "index 352 layer layer3.0.downsample.0 scheme (2, 4)bits Ltilde 0.431918\n",
      "index 353 layer layer3.0.downsample.0 scheme (2, 8)bits Ltilde 0.390109\n",
      "index 354 layer layer3.0.downsample.0 scheme (4, 2)bits Ltilde 0.691435\n",
      "index 355 layer layer3.0.downsample.0 scheme (4, 4)bits Ltilde 0.239377\n",
      "index 356 layer layer3.0.downsample.0 scheme (4, 8)bits Ltilde 0.206275\n",
      "index 357 layer layer3.0.downsample.0 scheme (8, 2)bits Ltilde 0.663943\n",
      "index 358 layer layer3.0.downsample.0 scheme (8, 4)bits Ltilde 0.223279\n",
      "index 359 layer layer3.0.downsample.0 scheme (8, 8)bits Ltilde 0.191917\n",
      "index 360 layer layer3.1.conv1 scheme (2, 2)bits Ltilde 1.070826\n",
      "index 361 layer layer3.1.conv1 scheme (2, 4)bits Ltilde 0.430820\n",
      "index 362 layer layer3.1.conv1 scheme (2, 8)bits Ltilde 0.398963\n",
      "index 363 layer layer3.1.conv1 scheme (4, 2)bits Ltilde 0.831979\n",
      "index 364 layer layer3.1.conv1 scheme (4, 4)bits Ltilde 0.227964\n",
      "index 365 layer layer3.1.conv1 scheme (4, 8)bits Ltilde 0.204794\n",
      "index 366 layer layer3.1.conv1 scheme (8, 2)bits Ltilde 0.807244\n",
      "index 367 layer layer3.1.conv1 scheme (8, 4)bits Ltilde 0.214754\n",
      "index 368 layer layer3.1.conv1 scheme (8, 8)bits Ltilde 0.193798\n",
      "index 369 layer layer3.1.conv2 scheme (2, 2)bits Ltilde 0.404819\n",
      "index 370 layer layer3.1.conv2 scheme (2, 4)bits Ltilde 0.226291\n",
      "index 371 layer layer3.1.conv2 scheme (2, 8)bits Ltilde 0.213025\n",
      "index 372 layer layer3.1.conv2 scheme (4, 2)bits Ltilde 0.396071\n",
      "index 373 layer layer3.1.conv2 scheme (4, 4)bits Ltilde 0.207171\n",
      "index 374 layer layer3.1.conv2 scheme (4, 8)bits Ltilde 0.195029\n",
      "index 375 layer layer3.1.conv2 scheme (8, 2)bits Ltilde 0.394273\n",
      "index 376 layer layer3.1.conv2 scheme (8, 4)bits Ltilde 0.205820\n",
      "index 377 layer layer3.1.conv2 scheme (8, 8)bits Ltilde 0.193541\n",
      "index 378 layer layer3.2.conv1 scheme (2, 2)bits Ltilde 1.428363\n",
      "index 379 layer layer3.2.conv1 scheme (2, 4)bits Ltilde 0.417014\n",
      "index 380 layer layer3.2.conv1 scheme (2, 8)bits Ltilde 0.378427\n",
      "index 381 layer layer3.2.conv1 scheme (4, 2)bits Ltilde 1.186954\n",
      "index 382 layer layer3.2.conv1 scheme (4, 4)bits Ltilde 0.235738\n",
      "index 383 layer layer3.2.conv1 scheme (4, 8)bits Ltilde 0.204737\n",
      "index 384 layer layer3.2.conv1 scheme (8, 2)bits Ltilde 1.163368\n",
      "index 385 layer layer3.2.conv1 scheme (8, 4)bits Ltilde 0.221623\n",
      "index 386 layer layer3.2.conv1 scheme (8, 8)bits Ltilde 0.191627\n",
      "index 387 layer layer3.2.conv2 scheme (2, 2)bits Ltilde 0.449142\n",
      "index 388 layer layer3.2.conv2 scheme (2, 4)bits Ltilde 0.238997\n",
      "index 389 layer layer3.2.conv2 scheme (2, 8)bits Ltilde 0.221586\n",
      "index 390 layer layer3.2.conv2 scheme (4, 2)bits Ltilde 0.422667\n",
      "index 391 layer layer3.2.conv2 scheme (4, 4)bits Ltilde 0.209843\n",
      "index 392 layer layer3.2.conv2 scheme (4, 8)bits Ltilde 0.195899\n",
      "index 393 layer layer3.2.conv2 scheme (8, 2)bits Ltilde 0.418655\n",
      "index 394 layer layer3.2.conv2 scheme (8, 4)bits Ltilde 0.206806\n",
      "index 395 layer layer3.2.conv2 scheme (8, 8)bits Ltilde 0.193142\n",
      "index 396 layer layer3.3.conv1 scheme (2, 2)bits Ltilde 1.275387\n",
      "index 397 layer layer3.3.conv1 scheme (2, 4)bits Ltilde 0.402536\n",
      "index 398 layer layer3.3.conv1 scheme (2, 8)bits Ltilde 0.375084\n",
      "index 399 layer layer3.3.conv1 scheme (4, 2)bits Ltilde 1.068663\n",
      "index 400 layer layer3.3.conv1 scheme (4, 4)bits Ltilde 0.228095\n",
      "index 401 layer layer3.3.conv1 scheme (4, 8)bits Ltilde 0.207687\n",
      "index 402 layer layer3.3.conv1 scheme (8, 2)bits Ltilde 1.028325\n",
      "index 403 layer layer3.3.conv1 scheme (8, 4)bits Ltilde 0.212359\n",
      "index 404 layer layer3.3.conv1 scheme (8, 8)bits Ltilde 0.193078\n",
      "index 405 layer layer3.3.conv2 scheme (2, 2)bits Ltilde 0.355276\n",
      "index 406 layer layer3.3.conv2 scheme (2, 4)bits Ltilde 0.226332\n",
      "index 407 layer layer3.3.conv2 scheme (2, 8)bits Ltilde 0.217680\n",
      "index 408 layer layer3.3.conv2 scheme (4, 2)bits Ltilde 0.328886\n",
      "index 409 layer layer3.3.conv2 scheme (4, 4)bits Ltilde 0.203053\n",
      "index 410 layer layer3.3.conv2 scheme (4, 8)bits Ltilde 0.195973\n",
      "index 411 layer layer3.3.conv2 scheme (8, 2)bits Ltilde 0.325604\n",
      "index 412 layer layer3.3.conv2 scheme (8, 4)bits Ltilde 0.199940\n",
      "index 413 layer layer3.3.conv2 scheme (8, 8)bits Ltilde 0.192944\n",
      "index 414 layer layer3.4.conv1 scheme (2, 2)bits Ltilde 1.009643\n",
      "index 415 layer layer3.4.conv1 scheme (2, 4)bits Ltilde 0.428469\n",
      "index 416 layer layer3.4.conv1 scheme (2, 8)bits Ltilde 0.400589\n",
      "index 417 layer layer3.4.conv1 scheme (4, 2)bits Ltilde 0.833483\n",
      "index 418 layer layer3.4.conv1 scheme (4, 4)bits Ltilde 0.234789\n",
      "index 419 layer layer3.4.conv1 scheme (4, 8)bits Ltilde 0.208857\n",
      "index 420 layer layer3.4.conv1 scheme (8, 2)bits Ltilde 0.809451\n",
      "index 421 layer layer3.4.conv1 scheme (8, 4)bits Ltilde 0.216417\n",
      "index 422 layer layer3.4.conv1 scheme (8, 8)bits Ltilde 0.192644\n",
      "index 423 layer layer3.4.conv2 scheme (2, 2)bits Ltilde 0.325562\n",
      "index 424 layer layer3.4.conv2 scheme (2, 4)bits Ltilde 0.228592\n",
      "index 425 layer layer3.4.conv2 scheme (2, 8)bits Ltilde 0.222356\n",
      "index 426 layer layer3.4.conv2 scheme (4, 2)bits Ltilde 0.294651\n",
      "index 427 layer layer3.4.conv2 scheme (4, 4)bits Ltilde 0.200201\n",
      "index 428 layer layer3.4.conv2 scheme (4, 8)bits Ltilde 0.194317\n",
      "index 429 layer layer3.4.conv2 scheme (8, 2)bits Ltilde 0.294133\n",
      "index 430 layer layer3.4.conv2 scheme (8, 4)bits Ltilde 0.198771\n",
      "index 431 layer layer3.4.conv2 scheme (8, 8)bits Ltilde 0.192796\n",
      "index 432 layer layer3.5.conv1 scheme (2, 2)bits Ltilde 1.185410\n",
      "index 433 layer layer3.5.conv1 scheme (2, 4)bits Ltilde 0.381976\n",
      "index 434 layer layer3.5.conv1 scheme (2, 8)bits Ltilde 0.342603\n",
      "index 435 layer layer3.5.conv1 scheme (4, 2)bits Ltilde 1.063300\n",
      "index 436 layer layer3.5.conv1 scheme (4, 4)bits Ltilde 0.240533\n",
      "index 437 layer layer3.5.conv1 scheme (4, 8)bits Ltilde 0.198814\n",
      "index 438 layer layer3.5.conv1 scheme (8, 2)bits Ltilde 1.051326\n",
      "index 439 layer layer3.5.conv1 scheme (8, 4)bits Ltilde 0.234029\n",
      "index 440 layer layer3.5.conv1 scheme (8, 8)bits Ltilde 0.192774\n",
      "index 441 layer layer3.5.conv2 scheme (2, 2)bits Ltilde 0.355890\n",
      "index 442 layer layer3.5.conv2 scheme (2, 4)bits Ltilde 0.228498\n",
      "index 443 layer layer3.5.conv2 scheme (2, 8)bits Ltilde 0.222432\n",
      "index 444 layer layer3.5.conv2 scheme (4, 2)bits Ltilde 0.325814\n",
      "index 445 layer layer3.5.conv2 scheme (4, 4)bits Ltilde 0.198840\n",
      "index 446 layer layer3.5.conv2 scheme (4, 8)bits Ltilde 0.193700\n",
      "index 447 layer layer3.5.conv2 scheme (8, 2)bits Ltilde 0.322946\n",
      "index 448 layer layer3.5.conv2 scheme (8, 4)bits Ltilde 0.197787\n",
      "index 449 layer layer3.5.conv2 scheme (8, 8)bits Ltilde 0.192951\n",
      "index 450 layer layer3.6.conv1 scheme (2, 2)bits Ltilde 1.440646\n",
      "index 451 layer layer3.6.conv1 scheme (2, 4)bits Ltilde 0.278226\n",
      "index 452 layer layer3.6.conv1 scheme (2, 8)bits Ltilde 0.250202\n",
      "index 453 layer layer3.6.conv1 scheme (4, 2)bits Ltilde 1.486518\n",
      "index 454 layer layer3.6.conv1 scheme (4, 4)bits Ltilde 0.231135\n",
      "index 455 layer layer3.6.conv1 scheme (4, 8)bits Ltilde 0.195641\n",
      "index 456 layer layer3.6.conv1 scheme (8, 2)bits Ltilde 1.494197\n",
      "index 457 layer layer3.6.conv1 scheme (8, 4)bits Ltilde 0.230021\n",
      "index 458 layer layer3.6.conv1 scheme (8, 8)bits Ltilde 0.194544\n",
      "index 459 layer layer3.6.conv2 scheme (2, 2)bits Ltilde 0.421566\n",
      "index 460 layer layer3.6.conv2 scheme (2, 4)bits Ltilde 0.226073\n",
      "index 461 layer layer3.6.conv2 scheme (2, 8)bits Ltilde 0.215044\n",
      "index 462 layer layer3.6.conv2 scheme (4, 2)bits Ltilde 0.384252\n",
      "index 463 layer layer3.6.conv2 scheme (4, 4)bits Ltilde 0.202785\n",
      "index 464 layer layer3.6.conv2 scheme (4, 8)bits Ltilde 0.194184\n",
      "index 465 layer layer3.6.conv2 scheme (8, 2)bits Ltilde 0.381604\n",
      "index 466 layer layer3.6.conv2 scheme (8, 4)bits Ltilde 0.201380\n",
      "index 467 layer layer3.6.conv2 scheme (8, 8)bits Ltilde 0.193143\n",
      "index 468 layer layer3.7.conv1 scheme (2, 2)bits Ltilde 0.549354\n",
      "index 469 layer layer3.7.conv1 scheme (2, 4)bits Ltilde 0.252273\n",
      "index 470 layer layer3.7.conv1 scheme (2, 8)bits Ltilde 0.247097\n",
      "index 471 layer layer3.7.conv1 scheme (4, 2)bits Ltilde 0.431614\n",
      "index 472 layer layer3.7.conv1 scheme (4, 4)bits Ltilde 0.195496\n",
      "index 473 layer layer3.7.conv1 scheme (4, 8)bits Ltilde 0.195242\n",
      "index 474 layer layer3.7.conv1 scheme (8, 2)bits Ltilde 0.423635\n",
      "index 475 layer layer3.7.conv1 scheme (8, 4)bits Ltilde 0.191871\n",
      "index 476 layer layer3.7.conv1 scheme (8, 8)bits Ltilde 0.192323\n",
      "index 477 layer layer3.7.conv2 scheme (2, 2)bits Ltilde 0.245982\n",
      "index 478 layer layer3.7.conv2 scheme (2, 4)bits Ltilde 0.194384\n",
      "index 479 layer layer3.7.conv2 scheme (2, 8)bits Ltilde 0.193235\n",
      "index 480 layer layer3.7.conv2 scheme (4, 2)bits Ltilde 0.231146\n",
      "index 481 layer layer3.7.conv2 scheme (4, 4)bits Ltilde 0.191990\n",
      "index 482 layer layer3.7.conv2 scheme (4, 8)bits Ltilde 0.192787\n",
      "index 483 layer layer3.7.conv2 scheme (8, 2)bits Ltilde 0.230500\n",
      "index 484 layer layer3.7.conv2 scheme (8, 4)bits Ltilde 0.191350\n",
      "index 485 layer layer3.7.conv2 scheme (8, 8)bits Ltilde 0.192477\n",
      "index 486 layer layer3.8.conv1 scheme (2, 2)bits Ltilde 0.326916\n",
      "index 487 layer layer3.8.conv1 scheme (2, 4)bits Ltilde 0.198433\n",
      "index 488 layer layer3.8.conv1 scheme (2, 8)bits Ltilde 0.202565\n",
      "index 489 layer layer3.8.conv1 scheme (4, 2)bits Ltilde 0.280373\n",
      "index 490 layer layer3.8.conv1 scheme (4, 4)bits Ltilde 0.187560\n",
      "index 491 layer layer3.8.conv1 scheme (4, 8)bits Ltilde 0.192888\n",
      "index 492 layer layer3.8.conv1 scheme (8, 2)bits Ltilde 0.278727\n",
      "index 493 layer layer3.8.conv1 scheme (8, 4)bits Ltilde 0.187720\n",
      "index 494 layer layer3.8.conv1 scheme (8, 8)bits Ltilde 0.193071\n",
      "index 495 layer layer3.8.conv2 scheme (2, 2)bits Ltilde 0.215918\n",
      "index 496 layer layer3.8.conv2 scheme (2, 4)bits Ltilde 0.190808\n",
      "index 497 layer layer3.8.conv2 scheme (2, 8)bits Ltilde 0.189367\n",
      "index 498 layer layer3.8.conv2 scheme (4, 2)bits Ltilde 0.208983\n",
      "index 499 layer layer3.8.conv2 scheme (4, 4)bits Ltilde 0.192493\n",
      "index 500 layer layer3.8.conv2 scheme (4, 8)bits Ltilde 0.192650\n",
      "index 501 layer layer3.8.conv2 scheme (8, 2)bits Ltilde 0.208124\n",
      "index 502 layer layer3.8.conv2 scheme (8, 4)bits Ltilde 0.192474\n",
      "index 503 layer layer3.8.conv2 scheme (8, 8)bits Ltilde 0.192842\n"
     ]
    }
   ],
   "source": [
    "index2layerscheme = [None for i in range(hm['Ltilde'].shape[0])]\n",
    "\n",
    "for name in hm['layer_index']:\n",
    "    index = hm['layer_index'][name]\n",
    "    layer_name = name[:-10]\n",
    "    scheme = name[-10:]\n",
    "    a = hm['Ltilde']\n",
    "    print(f'index {index} layer {layer_name} scheme {scheme} Ltilde {a[index,index].item():.6f}')\n",
    "    \n",
    "    index2layerscheme[index] = (layer_name,scheme)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17bd25f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f41afbf4a00>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(hm['Ltilde'],cmap='hot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "beb3fb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cached_grad is left hand side of equation 10 in draft\n",
    "# hm['Ltilde'] is loaded from pickle file (gradient caching phase)\n",
    "L = hm['Ltilde'].shape[0]\n",
    "cached_grad = np.zeros_like(hm['Ltilde'])\n",
    "for i in range(L):\n",
    "    for j in range(L):\n",
    "        layer_i,scheme_i = index2layerscheme[i]\n",
    "        layer_j,scheme_j = index2layerscheme[j]\n",
    "        if layer_i == layer_j:\n",
    "            if scheme_i == scheme_j:\n",
    "                cached_grad[i,j] = cached_grad[j,i] = 2 * hm['Ltilde'][i,j]\n",
    "            else:\n",
    "                #cached_grad[i,j] = cached_grad[j,i] = 4 * hm['Ltilde'][i,j] - hm['Ltilde'][i,i] - hm['Ltilde'][j,j]\n",
    "                cached_grad[i,j] = cached_grad[j,i] = 0\n",
    "        else:\n",
    "            cached_grad[i,j] = cached_grad[j,i] = hm['Ltilde'][i,j] - hm['Ltilde'][i,i] - hm['Ltilde'][j,j]\n",
    "        '''\n",
    "        print(index2layerscheme[i])\n",
    "        print(index2layerscheme[j])\n",
    "        '''\n",
    "        '''\n",
    "        if i == j:\n",
    "            cached_grad[i,j] = 0.5 * hm['Ltilde'][i,j]\n",
    "        else:\n",
    "            cached_grad[i,j] = 0.25 * (hm['Ltilde'][i,j]-hm['Ltilde'][i,i]-hm['Ltilde'][j,j])\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e34a3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f41ad915c70>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cached_grad[cached_grad<0]=0\n",
    "plt.imshow(cached_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71d6487d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(504, 504)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a982e3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge pyscipopt=3.5.0\n",
    "#!pip install cvxpy-base\n",
    "#!pip install -U pymoo\n",
    "#!pip install gurobipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864bfca8",
   "metadata": {},
   "source": [
    "### Hook to record input and output shapes of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88a839b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer_hook(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(layer_hook, self).__init__()\n",
    "        self.in_shape = None\n",
    "        self.out_shape = None\n",
    "\n",
    "    def hook(self, module, inp, outp):\n",
    "        self.in_shape = inp[0].size()\n",
    "        self.out_shape = outp.size()\n",
    "    \n",
    "\n",
    "hooks = {}\n",
    "\n",
    "for layer in hm['layer_index']:\n",
    "    m = getModuleByName(model,layer[:-10])\n",
    "    hook = layer_hook()\n",
    "    hooks[layer[:-10]] = (hook,m.register_forward_hook(hook.hook))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1bc42d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in hooks:\n",
    "#     hooks[layer][1].remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f56512fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for img,label in train:\n",
    "        model(img.cuda())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76e26a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_bitops(layer_name,a_bits,w_bits):\n",
    "    \n",
    "    m = getModuleByName(model,layer_name)\n",
    "    \n",
    "    if isinstance(m,torch.nn.Conv2d):\n",
    "        _,cin,_,_ = hooks[layer_name][0].in_shape\n",
    "        _,cout,hout,wout = hooks[layer_name][0].out_shape\n",
    "        \n",
    "#         print('in',hooks[layer_name][0].in_shape)\n",
    "#         print('out',hooks[layer_name][0].out_shape)\n",
    "        \n",
    "        n_muls = cin * m.weight.size()[2] * m.weight.size()[3] * cout * hout * wout\n",
    "        n_accs = (cin * m.weight.size()[2] * m.weight.size()[3] - 1) * cout * hout * wout\n",
    "        \n",
    "        bitops_per_mul = 2 * a_bits * w_bits\n",
    "        bitops_per_acc = (a_bits + w_bits) + np.ceil(np.log2(cin * m.weight.size()[2] * m.weight.size()[3]))\n",
    "        \n",
    "#         print(f'n_muls {n_muls} ops_per_mul {bitops_per_mul} totl {n_muls*bitops_per_mul}')\n",
    "#         print(f'n_accs {n_accs} ops_per_acc {bitops_per_acc} totl {n_accs*bitops_per_acc}')\n",
    "#         print()\n",
    "        \n",
    "        return n_muls * bitops_per_mul + n_accs * bitops_per_acc\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e0cbe17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('layer1.0.conv1', '(2, 2)bits'),\n",
       " ('layer1.0.conv1', '(2, 4)bits'),\n",
       " ('layer1.0.conv1', '(2, 8)bits'),\n",
       " ('layer1.0.conv1', '(4, 2)bits'),\n",
       " ('layer1.0.conv1', '(4, 4)bits'),\n",
       " ('layer1.0.conv1', '(4, 8)bits'),\n",
       " ('layer1.0.conv1', '(8, 2)bits'),\n",
       " ('layer1.0.conv1', '(8, 4)bits'),\n",
       " ('layer1.0.conv1', '(8, 8)bits'),\n",
       " ('layer1.0.conv2', '(2, 2)bits'),\n",
       " ('layer1.0.conv2', '(2, 4)bits'),\n",
       " ('layer1.0.conv2', '(2, 8)bits'),\n",
       " ('layer1.0.conv2', '(4, 2)bits'),\n",
       " ('layer1.0.conv2', '(4, 4)bits'),\n",
       " ('layer1.0.conv2', '(4, 8)bits'),\n",
       " ('layer1.0.conv2', '(8, 2)bits'),\n",
       " ('layer1.0.conv2', '(8, 4)bits'),\n",
       " ('layer1.0.conv2', '(8, 8)bits'),\n",
       " ('layer1.1.conv1', '(2, 2)bits'),\n",
       " ('layer1.1.conv1', '(2, 4)bits'),\n",
       " ('layer1.1.conv1', '(2, 8)bits'),\n",
       " ('layer1.1.conv1', '(4, 2)bits'),\n",
       " ('layer1.1.conv1', '(4, 4)bits'),\n",
       " ('layer1.1.conv1', '(4, 8)bits'),\n",
       " ('layer1.1.conv1', '(8, 2)bits'),\n",
       " ('layer1.1.conv1', '(8, 4)bits'),\n",
       " ('layer1.1.conv1', '(8, 8)bits'),\n",
       " ('layer1.1.conv2', '(2, 2)bits'),\n",
       " ('layer1.1.conv2', '(2, 4)bits'),\n",
       " ('layer1.1.conv2', '(2, 8)bits'),\n",
       " ('layer1.1.conv2', '(4, 2)bits'),\n",
       " ('layer1.1.conv2', '(4, 4)bits'),\n",
       " ('layer1.1.conv2', '(4, 8)bits'),\n",
       " ('layer1.1.conv2', '(8, 2)bits'),\n",
       " ('layer1.1.conv2', '(8, 4)bits'),\n",
       " ('layer1.1.conv2', '(8, 8)bits'),\n",
       " ('layer1.2.conv1', '(2, 2)bits'),\n",
       " ('layer1.2.conv1', '(2, 4)bits'),\n",
       " ('layer1.2.conv1', '(2, 8)bits'),\n",
       " ('layer1.2.conv1', '(4, 2)bits'),\n",
       " ('layer1.2.conv1', '(4, 4)bits'),\n",
       " ('layer1.2.conv1', '(4, 8)bits'),\n",
       " ('layer1.2.conv1', '(8, 2)bits'),\n",
       " ('layer1.2.conv1', '(8, 4)bits'),\n",
       " ('layer1.2.conv1', '(8, 8)bits'),\n",
       " ('layer1.2.conv2', '(2, 2)bits'),\n",
       " ('layer1.2.conv2', '(2, 4)bits'),\n",
       " ('layer1.2.conv2', '(2, 8)bits'),\n",
       " ('layer1.2.conv2', '(4, 2)bits'),\n",
       " ('layer1.2.conv2', '(4, 4)bits'),\n",
       " ('layer1.2.conv2', '(4, 8)bits'),\n",
       " ('layer1.2.conv2', '(8, 2)bits'),\n",
       " ('layer1.2.conv2', '(8, 4)bits'),\n",
       " ('layer1.2.conv2', '(8, 8)bits'),\n",
       " ('layer1.3.conv1', '(2, 2)bits'),\n",
       " ('layer1.3.conv1', '(2, 4)bits'),\n",
       " ('layer1.3.conv1', '(2, 8)bits'),\n",
       " ('layer1.3.conv1', '(4, 2)bits'),\n",
       " ('layer1.3.conv1', '(4, 4)bits'),\n",
       " ('layer1.3.conv1', '(4, 8)bits'),\n",
       " ('layer1.3.conv1', '(8, 2)bits'),\n",
       " ('layer1.3.conv1', '(8, 4)bits'),\n",
       " ('layer1.3.conv1', '(8, 8)bits'),\n",
       " ('layer1.3.conv2', '(2, 2)bits'),\n",
       " ('layer1.3.conv2', '(2, 4)bits'),\n",
       " ('layer1.3.conv2', '(2, 8)bits'),\n",
       " ('layer1.3.conv2', '(4, 2)bits'),\n",
       " ('layer1.3.conv2', '(4, 4)bits'),\n",
       " ('layer1.3.conv2', '(4, 8)bits'),\n",
       " ('layer1.3.conv2', '(8, 2)bits'),\n",
       " ('layer1.3.conv2', '(8, 4)bits'),\n",
       " ('layer1.3.conv2', '(8, 8)bits'),\n",
       " ('layer1.4.conv1', '(2, 2)bits'),\n",
       " ('layer1.4.conv1', '(2, 4)bits'),\n",
       " ('layer1.4.conv1', '(2, 8)bits'),\n",
       " ('layer1.4.conv1', '(4, 2)bits'),\n",
       " ('layer1.4.conv1', '(4, 4)bits'),\n",
       " ('layer1.4.conv1', '(4, 8)bits'),\n",
       " ('layer1.4.conv1', '(8, 2)bits'),\n",
       " ('layer1.4.conv1', '(8, 4)bits'),\n",
       " ('layer1.4.conv1', '(8, 8)bits'),\n",
       " ('layer1.4.conv2', '(2, 2)bits'),\n",
       " ('layer1.4.conv2', '(2, 4)bits'),\n",
       " ('layer1.4.conv2', '(2, 8)bits'),\n",
       " ('layer1.4.conv2', '(4, 2)bits'),\n",
       " ('layer1.4.conv2', '(4, 4)bits'),\n",
       " ('layer1.4.conv2', '(4, 8)bits'),\n",
       " ('layer1.4.conv2', '(8, 2)bits'),\n",
       " ('layer1.4.conv2', '(8, 4)bits'),\n",
       " ('layer1.4.conv2', '(8, 8)bits'),\n",
       " ('layer1.5.conv1', '(2, 2)bits'),\n",
       " ('layer1.5.conv1', '(2, 4)bits'),\n",
       " ('layer1.5.conv1', '(2, 8)bits'),\n",
       " ('layer1.5.conv1', '(4, 2)bits'),\n",
       " ('layer1.5.conv1', '(4, 4)bits'),\n",
       " ('layer1.5.conv1', '(4, 8)bits'),\n",
       " ('layer1.5.conv1', '(8, 2)bits'),\n",
       " ('layer1.5.conv1', '(8, 4)bits'),\n",
       " ('layer1.5.conv1', '(8, 8)bits'),\n",
       " ('layer1.5.conv2', '(2, 2)bits'),\n",
       " ('layer1.5.conv2', '(2, 4)bits'),\n",
       " ('layer1.5.conv2', '(2, 8)bits'),\n",
       " ('layer1.5.conv2', '(4, 2)bits'),\n",
       " ('layer1.5.conv2', '(4, 4)bits'),\n",
       " ('layer1.5.conv2', '(4, 8)bits'),\n",
       " ('layer1.5.conv2', '(8, 2)bits'),\n",
       " ('layer1.5.conv2', '(8, 4)bits'),\n",
       " ('layer1.5.conv2', '(8, 8)bits'),\n",
       " ('layer1.6.conv1', '(2, 2)bits'),\n",
       " ('layer1.6.conv1', '(2, 4)bits'),\n",
       " ('layer1.6.conv1', '(2, 8)bits'),\n",
       " ('layer1.6.conv1', '(4, 2)bits'),\n",
       " ('layer1.6.conv1', '(4, 4)bits'),\n",
       " ('layer1.6.conv1', '(4, 8)bits'),\n",
       " ('layer1.6.conv1', '(8, 2)bits'),\n",
       " ('layer1.6.conv1', '(8, 4)bits'),\n",
       " ('layer1.6.conv1', '(8, 8)bits'),\n",
       " ('layer1.6.conv2', '(2, 2)bits'),\n",
       " ('layer1.6.conv2', '(2, 4)bits'),\n",
       " ('layer1.6.conv2', '(2, 8)bits'),\n",
       " ('layer1.6.conv2', '(4, 2)bits'),\n",
       " ('layer1.6.conv2', '(4, 4)bits'),\n",
       " ('layer1.6.conv2', '(4, 8)bits'),\n",
       " ('layer1.6.conv2', '(8, 2)bits'),\n",
       " ('layer1.6.conv2', '(8, 4)bits'),\n",
       " ('layer1.6.conv2', '(8, 8)bits'),\n",
       " ('layer1.7.conv1', '(2, 2)bits'),\n",
       " ('layer1.7.conv1', '(2, 4)bits'),\n",
       " ('layer1.7.conv1', '(2, 8)bits'),\n",
       " ('layer1.7.conv1', '(4, 2)bits'),\n",
       " ('layer1.7.conv1', '(4, 4)bits'),\n",
       " ('layer1.7.conv1', '(4, 8)bits'),\n",
       " ('layer1.7.conv1', '(8, 2)bits'),\n",
       " ('layer1.7.conv1', '(8, 4)bits'),\n",
       " ('layer1.7.conv1', '(8, 8)bits'),\n",
       " ('layer1.7.conv2', '(2, 2)bits'),\n",
       " ('layer1.7.conv2', '(2, 4)bits'),\n",
       " ('layer1.7.conv2', '(2, 8)bits'),\n",
       " ('layer1.7.conv2', '(4, 2)bits'),\n",
       " ('layer1.7.conv2', '(4, 4)bits'),\n",
       " ('layer1.7.conv2', '(4, 8)bits'),\n",
       " ('layer1.7.conv2', '(8, 2)bits'),\n",
       " ('layer1.7.conv2', '(8, 4)bits'),\n",
       " ('layer1.7.conv2', '(8, 8)bits'),\n",
       " ('layer1.8.conv1', '(2, 2)bits'),\n",
       " ('layer1.8.conv1', '(2, 4)bits'),\n",
       " ('layer1.8.conv1', '(2, 8)bits'),\n",
       " ('layer1.8.conv1', '(4, 2)bits'),\n",
       " ('layer1.8.conv1', '(4, 4)bits'),\n",
       " ('layer1.8.conv1', '(4, 8)bits'),\n",
       " ('layer1.8.conv1', '(8, 2)bits'),\n",
       " ('layer1.8.conv1', '(8, 4)bits'),\n",
       " ('layer1.8.conv1', '(8, 8)bits'),\n",
       " ('layer1.8.conv2', '(2, 2)bits'),\n",
       " ('layer1.8.conv2', '(2, 4)bits'),\n",
       " ('layer1.8.conv2', '(2, 8)bits'),\n",
       " ('layer1.8.conv2', '(4, 2)bits'),\n",
       " ('layer1.8.conv2', '(4, 4)bits'),\n",
       " ('layer1.8.conv2', '(4, 8)bits'),\n",
       " ('layer1.8.conv2', '(8, 2)bits'),\n",
       " ('layer1.8.conv2', '(8, 4)bits'),\n",
       " ('layer1.8.conv2', '(8, 8)bits'),\n",
       " ('layer2.0.conv1', '(2, 2)bits'),\n",
       " ('layer2.0.conv1', '(2, 4)bits'),\n",
       " ('layer2.0.conv1', '(2, 8)bits'),\n",
       " ('layer2.0.conv1', '(4, 2)bits'),\n",
       " ('layer2.0.conv1', '(4, 4)bits'),\n",
       " ('layer2.0.conv1', '(4, 8)bits'),\n",
       " ('layer2.0.conv1', '(8, 2)bits'),\n",
       " ('layer2.0.conv1', '(8, 4)bits'),\n",
       " ('layer2.0.conv1', '(8, 8)bits'),\n",
       " ('layer2.0.conv2', '(2, 2)bits'),\n",
       " ('layer2.0.conv2', '(2, 4)bits'),\n",
       " ('layer2.0.conv2', '(2, 8)bits'),\n",
       " ('layer2.0.conv2', '(4, 2)bits'),\n",
       " ('layer2.0.conv2', '(4, 4)bits'),\n",
       " ('layer2.0.conv2', '(4, 8)bits'),\n",
       " ('layer2.0.conv2', '(8, 2)bits'),\n",
       " ('layer2.0.conv2', '(8, 4)bits'),\n",
       " ('layer2.0.conv2', '(8, 8)bits'),\n",
       " ('layer2.0.downsample.0', '(2, 2)bits'),\n",
       " ('layer2.0.downsample.0', '(2, 4)bits'),\n",
       " ('layer2.0.downsample.0', '(2, 8)bits'),\n",
       " ('layer2.0.downsample.0', '(4, 2)bits'),\n",
       " ('layer2.0.downsample.0', '(4, 4)bits'),\n",
       " ('layer2.0.downsample.0', '(4, 8)bits'),\n",
       " ('layer2.0.downsample.0', '(8, 2)bits'),\n",
       " ('layer2.0.downsample.0', '(8, 4)bits'),\n",
       " ('layer2.0.downsample.0', '(8, 8)bits'),\n",
       " ('layer2.1.conv1', '(2, 2)bits'),\n",
       " ('layer2.1.conv1', '(2, 4)bits'),\n",
       " ('layer2.1.conv1', '(2, 8)bits'),\n",
       " ('layer2.1.conv1', '(4, 2)bits'),\n",
       " ('layer2.1.conv1', '(4, 4)bits'),\n",
       " ('layer2.1.conv1', '(4, 8)bits'),\n",
       " ('layer2.1.conv1', '(8, 2)bits'),\n",
       " ('layer2.1.conv1', '(8, 4)bits'),\n",
       " ('layer2.1.conv1', '(8, 8)bits'),\n",
       " ('layer2.1.conv2', '(2, 2)bits'),\n",
       " ('layer2.1.conv2', '(2, 4)bits'),\n",
       " ('layer2.1.conv2', '(2, 8)bits'),\n",
       " ('layer2.1.conv2', '(4, 2)bits'),\n",
       " ('layer2.1.conv2', '(4, 4)bits'),\n",
       " ('layer2.1.conv2', '(4, 8)bits'),\n",
       " ('layer2.1.conv2', '(8, 2)bits'),\n",
       " ('layer2.1.conv2', '(8, 4)bits'),\n",
       " ('layer2.1.conv2', '(8, 8)bits'),\n",
       " ('layer2.2.conv1', '(2, 2)bits'),\n",
       " ('layer2.2.conv1', '(2, 4)bits'),\n",
       " ('layer2.2.conv1', '(2, 8)bits'),\n",
       " ('layer2.2.conv1', '(4, 2)bits'),\n",
       " ('layer2.2.conv1', '(4, 4)bits'),\n",
       " ('layer2.2.conv1', '(4, 8)bits'),\n",
       " ('layer2.2.conv1', '(8, 2)bits'),\n",
       " ('layer2.2.conv1', '(8, 4)bits'),\n",
       " ('layer2.2.conv1', '(8, 8)bits'),\n",
       " ('layer2.2.conv2', '(2, 2)bits'),\n",
       " ('layer2.2.conv2', '(2, 4)bits'),\n",
       " ('layer2.2.conv2', '(2, 8)bits'),\n",
       " ('layer2.2.conv2', '(4, 2)bits'),\n",
       " ('layer2.2.conv2', '(4, 4)bits'),\n",
       " ('layer2.2.conv2', '(4, 8)bits'),\n",
       " ('layer2.2.conv2', '(8, 2)bits'),\n",
       " ('layer2.2.conv2', '(8, 4)bits'),\n",
       " ('layer2.2.conv2', '(8, 8)bits'),\n",
       " ('layer2.3.conv1', '(2, 2)bits'),\n",
       " ('layer2.3.conv1', '(2, 4)bits'),\n",
       " ('layer2.3.conv1', '(2, 8)bits'),\n",
       " ('layer2.3.conv1', '(4, 2)bits'),\n",
       " ('layer2.3.conv1', '(4, 4)bits'),\n",
       " ('layer2.3.conv1', '(4, 8)bits'),\n",
       " ('layer2.3.conv1', '(8, 2)bits'),\n",
       " ('layer2.3.conv1', '(8, 4)bits'),\n",
       " ('layer2.3.conv1', '(8, 8)bits'),\n",
       " ('layer2.3.conv2', '(2, 2)bits'),\n",
       " ('layer2.3.conv2', '(2, 4)bits'),\n",
       " ('layer2.3.conv2', '(2, 8)bits'),\n",
       " ('layer2.3.conv2', '(4, 2)bits'),\n",
       " ('layer2.3.conv2', '(4, 4)bits'),\n",
       " ('layer2.3.conv2', '(4, 8)bits'),\n",
       " ('layer2.3.conv2', '(8, 2)bits'),\n",
       " ('layer2.3.conv2', '(8, 4)bits'),\n",
       " ('layer2.3.conv2', '(8, 8)bits'),\n",
       " ('layer2.4.conv1', '(2, 2)bits'),\n",
       " ('layer2.4.conv1', '(2, 4)bits'),\n",
       " ('layer2.4.conv1', '(2, 8)bits'),\n",
       " ('layer2.4.conv1', '(4, 2)bits'),\n",
       " ('layer2.4.conv1', '(4, 4)bits'),\n",
       " ('layer2.4.conv1', '(4, 8)bits'),\n",
       " ('layer2.4.conv1', '(8, 2)bits'),\n",
       " ('layer2.4.conv1', '(8, 4)bits'),\n",
       " ('layer2.4.conv1', '(8, 8)bits'),\n",
       " ('layer2.4.conv2', '(2, 2)bits'),\n",
       " ('layer2.4.conv2', '(2, 4)bits'),\n",
       " ('layer2.4.conv2', '(2, 8)bits'),\n",
       " ('layer2.4.conv2', '(4, 2)bits'),\n",
       " ('layer2.4.conv2', '(4, 4)bits'),\n",
       " ('layer2.4.conv2', '(4, 8)bits'),\n",
       " ('layer2.4.conv2', '(8, 2)bits'),\n",
       " ('layer2.4.conv2', '(8, 4)bits'),\n",
       " ('layer2.4.conv2', '(8, 8)bits'),\n",
       " ('layer2.5.conv1', '(2, 2)bits'),\n",
       " ('layer2.5.conv1', '(2, 4)bits'),\n",
       " ('layer2.5.conv1', '(2, 8)bits'),\n",
       " ('layer2.5.conv1', '(4, 2)bits'),\n",
       " ('layer2.5.conv1', '(4, 4)bits'),\n",
       " ('layer2.5.conv1', '(4, 8)bits'),\n",
       " ('layer2.5.conv1', '(8, 2)bits'),\n",
       " ('layer2.5.conv1', '(8, 4)bits'),\n",
       " ('layer2.5.conv1', '(8, 8)bits'),\n",
       " ('layer2.5.conv2', '(2, 2)bits'),\n",
       " ('layer2.5.conv2', '(2, 4)bits'),\n",
       " ('layer2.5.conv2', '(2, 8)bits'),\n",
       " ('layer2.5.conv2', '(4, 2)bits'),\n",
       " ('layer2.5.conv2', '(4, 4)bits'),\n",
       " ('layer2.5.conv2', '(4, 8)bits'),\n",
       " ('layer2.5.conv2', '(8, 2)bits'),\n",
       " ('layer2.5.conv2', '(8, 4)bits'),\n",
       " ('layer2.5.conv2', '(8, 8)bits'),\n",
       " ('layer2.6.conv1', '(2, 2)bits'),\n",
       " ('layer2.6.conv1', '(2, 4)bits'),\n",
       " ('layer2.6.conv1', '(2, 8)bits'),\n",
       " ('layer2.6.conv1', '(4, 2)bits'),\n",
       " ('layer2.6.conv1', '(4, 4)bits'),\n",
       " ('layer2.6.conv1', '(4, 8)bits'),\n",
       " ('layer2.6.conv1', '(8, 2)bits'),\n",
       " ('layer2.6.conv1', '(8, 4)bits'),\n",
       " ('layer2.6.conv1', '(8, 8)bits'),\n",
       " ('layer2.6.conv2', '(2, 2)bits'),\n",
       " ('layer2.6.conv2', '(2, 4)bits'),\n",
       " ('layer2.6.conv2', '(2, 8)bits'),\n",
       " ('layer2.6.conv2', '(4, 2)bits'),\n",
       " ('layer2.6.conv2', '(4, 4)bits'),\n",
       " ('layer2.6.conv2', '(4, 8)bits'),\n",
       " ('layer2.6.conv2', '(8, 2)bits'),\n",
       " ('layer2.6.conv2', '(8, 4)bits'),\n",
       " ('layer2.6.conv2', '(8, 8)bits'),\n",
       " ('layer2.7.conv1', '(2, 2)bits'),\n",
       " ('layer2.7.conv1', '(2, 4)bits'),\n",
       " ('layer2.7.conv1', '(2, 8)bits'),\n",
       " ('layer2.7.conv1', '(4, 2)bits'),\n",
       " ('layer2.7.conv1', '(4, 4)bits'),\n",
       " ('layer2.7.conv1', '(4, 8)bits'),\n",
       " ('layer2.7.conv1', '(8, 2)bits'),\n",
       " ('layer2.7.conv1', '(8, 4)bits'),\n",
       " ('layer2.7.conv1', '(8, 8)bits'),\n",
       " ('layer2.7.conv2', '(2, 2)bits'),\n",
       " ('layer2.7.conv2', '(2, 4)bits'),\n",
       " ('layer2.7.conv2', '(2, 8)bits'),\n",
       " ('layer2.7.conv2', '(4, 2)bits'),\n",
       " ('layer2.7.conv2', '(4, 4)bits'),\n",
       " ('layer2.7.conv2', '(4, 8)bits'),\n",
       " ('layer2.7.conv2', '(8, 2)bits'),\n",
       " ('layer2.7.conv2', '(8, 4)bits'),\n",
       " ('layer2.7.conv2', '(8, 8)bits'),\n",
       " ('layer2.8.conv1', '(2, 2)bits'),\n",
       " ('layer2.8.conv1', '(2, 4)bits'),\n",
       " ('layer2.8.conv1', '(2, 8)bits'),\n",
       " ('layer2.8.conv1', '(4, 2)bits'),\n",
       " ('layer2.8.conv1', '(4, 4)bits'),\n",
       " ('layer2.8.conv1', '(4, 8)bits'),\n",
       " ('layer2.8.conv1', '(8, 2)bits'),\n",
       " ('layer2.8.conv1', '(8, 4)bits'),\n",
       " ('layer2.8.conv1', '(8, 8)bits'),\n",
       " ('layer2.8.conv2', '(2, 2)bits'),\n",
       " ('layer2.8.conv2', '(2, 4)bits'),\n",
       " ('layer2.8.conv2', '(2, 8)bits'),\n",
       " ('layer2.8.conv2', '(4, 2)bits'),\n",
       " ('layer2.8.conv2', '(4, 4)bits'),\n",
       " ('layer2.8.conv2', '(4, 8)bits'),\n",
       " ('layer2.8.conv2', '(8, 2)bits'),\n",
       " ('layer2.8.conv2', '(8, 4)bits'),\n",
       " ('layer2.8.conv2', '(8, 8)bits'),\n",
       " ('layer3.0.conv1', '(2, 2)bits'),\n",
       " ('layer3.0.conv1', '(2, 4)bits'),\n",
       " ('layer3.0.conv1', '(2, 8)bits'),\n",
       " ('layer3.0.conv1', '(4, 2)bits'),\n",
       " ('layer3.0.conv1', '(4, 4)bits'),\n",
       " ('layer3.0.conv1', '(4, 8)bits'),\n",
       " ('layer3.0.conv1', '(8, 2)bits'),\n",
       " ('layer3.0.conv1', '(8, 4)bits'),\n",
       " ('layer3.0.conv1', '(8, 8)bits'),\n",
       " ('layer3.0.conv2', '(2, 2)bits'),\n",
       " ('layer3.0.conv2', '(2, 4)bits'),\n",
       " ('layer3.0.conv2', '(2, 8)bits'),\n",
       " ('layer3.0.conv2', '(4, 2)bits'),\n",
       " ('layer3.0.conv2', '(4, 4)bits'),\n",
       " ('layer3.0.conv2', '(4, 8)bits'),\n",
       " ('layer3.0.conv2', '(8, 2)bits'),\n",
       " ('layer3.0.conv2', '(8, 4)bits'),\n",
       " ('layer3.0.conv2', '(8, 8)bits'),\n",
       " ('layer3.0.downsample.0', '(2, 2)bits'),\n",
       " ('layer3.0.downsample.0', '(2, 4)bits'),\n",
       " ('layer3.0.downsample.0', '(2, 8)bits'),\n",
       " ('layer3.0.downsample.0', '(4, 2)bits'),\n",
       " ('layer3.0.downsample.0', '(4, 4)bits'),\n",
       " ('layer3.0.downsample.0', '(4, 8)bits'),\n",
       " ('layer3.0.downsample.0', '(8, 2)bits'),\n",
       " ('layer3.0.downsample.0', '(8, 4)bits'),\n",
       " ('layer3.0.downsample.0', '(8, 8)bits'),\n",
       " ('layer3.1.conv1', '(2, 2)bits'),\n",
       " ('layer3.1.conv1', '(2, 4)bits'),\n",
       " ('layer3.1.conv1', '(2, 8)bits'),\n",
       " ('layer3.1.conv1', '(4, 2)bits'),\n",
       " ('layer3.1.conv1', '(4, 4)bits'),\n",
       " ('layer3.1.conv1', '(4, 8)bits'),\n",
       " ('layer3.1.conv1', '(8, 2)bits'),\n",
       " ('layer3.1.conv1', '(8, 4)bits'),\n",
       " ('layer3.1.conv1', '(8, 8)bits'),\n",
       " ('layer3.1.conv2', '(2, 2)bits'),\n",
       " ('layer3.1.conv2', '(2, 4)bits'),\n",
       " ('layer3.1.conv2', '(2, 8)bits'),\n",
       " ('layer3.1.conv2', '(4, 2)bits'),\n",
       " ('layer3.1.conv2', '(4, 4)bits'),\n",
       " ('layer3.1.conv2', '(4, 8)bits'),\n",
       " ('layer3.1.conv2', '(8, 2)bits'),\n",
       " ('layer3.1.conv2', '(8, 4)bits'),\n",
       " ('layer3.1.conv2', '(8, 8)bits'),\n",
       " ('layer3.2.conv1', '(2, 2)bits'),\n",
       " ('layer3.2.conv1', '(2, 4)bits'),\n",
       " ('layer3.2.conv1', '(2, 8)bits'),\n",
       " ('layer3.2.conv1', '(4, 2)bits'),\n",
       " ('layer3.2.conv1', '(4, 4)bits'),\n",
       " ('layer3.2.conv1', '(4, 8)bits'),\n",
       " ('layer3.2.conv1', '(8, 2)bits'),\n",
       " ('layer3.2.conv1', '(8, 4)bits'),\n",
       " ('layer3.2.conv1', '(8, 8)bits'),\n",
       " ('layer3.2.conv2', '(2, 2)bits'),\n",
       " ('layer3.2.conv2', '(2, 4)bits'),\n",
       " ('layer3.2.conv2', '(2, 8)bits'),\n",
       " ('layer3.2.conv2', '(4, 2)bits'),\n",
       " ('layer3.2.conv2', '(4, 4)bits'),\n",
       " ('layer3.2.conv2', '(4, 8)bits'),\n",
       " ('layer3.2.conv2', '(8, 2)bits'),\n",
       " ('layer3.2.conv2', '(8, 4)bits'),\n",
       " ('layer3.2.conv2', '(8, 8)bits'),\n",
       " ('layer3.3.conv1', '(2, 2)bits'),\n",
       " ('layer3.3.conv1', '(2, 4)bits'),\n",
       " ('layer3.3.conv1', '(2, 8)bits'),\n",
       " ('layer3.3.conv1', '(4, 2)bits'),\n",
       " ('layer3.3.conv1', '(4, 4)bits'),\n",
       " ('layer3.3.conv1', '(4, 8)bits'),\n",
       " ('layer3.3.conv1', '(8, 2)bits'),\n",
       " ('layer3.3.conv1', '(8, 4)bits'),\n",
       " ('layer3.3.conv1', '(8, 8)bits'),\n",
       " ('layer3.3.conv2', '(2, 2)bits'),\n",
       " ('layer3.3.conv2', '(2, 4)bits'),\n",
       " ('layer3.3.conv2', '(2, 8)bits'),\n",
       " ('layer3.3.conv2', '(4, 2)bits'),\n",
       " ('layer3.3.conv2', '(4, 4)bits'),\n",
       " ('layer3.3.conv2', '(4, 8)bits'),\n",
       " ('layer3.3.conv2', '(8, 2)bits'),\n",
       " ('layer3.3.conv2', '(8, 4)bits'),\n",
       " ('layer3.3.conv2', '(8, 8)bits'),\n",
       " ('layer3.4.conv1', '(2, 2)bits'),\n",
       " ('layer3.4.conv1', '(2, 4)bits'),\n",
       " ('layer3.4.conv1', '(2, 8)bits'),\n",
       " ('layer3.4.conv1', '(4, 2)bits'),\n",
       " ('layer3.4.conv1', '(4, 4)bits'),\n",
       " ('layer3.4.conv1', '(4, 8)bits'),\n",
       " ('layer3.4.conv1', '(8, 2)bits'),\n",
       " ('layer3.4.conv1', '(8, 4)bits'),\n",
       " ('layer3.4.conv1', '(8, 8)bits'),\n",
       " ('layer3.4.conv2', '(2, 2)bits'),\n",
       " ('layer3.4.conv2', '(2, 4)bits'),\n",
       " ('layer3.4.conv2', '(2, 8)bits'),\n",
       " ('layer3.4.conv2', '(4, 2)bits'),\n",
       " ('layer3.4.conv2', '(4, 4)bits'),\n",
       " ('layer3.4.conv2', '(4, 8)bits'),\n",
       " ('layer3.4.conv2', '(8, 2)bits'),\n",
       " ('layer3.4.conv2', '(8, 4)bits'),\n",
       " ('layer3.4.conv2', '(8, 8)bits'),\n",
       " ('layer3.5.conv1', '(2, 2)bits'),\n",
       " ('layer3.5.conv1', '(2, 4)bits'),\n",
       " ('layer3.5.conv1', '(2, 8)bits'),\n",
       " ('layer3.5.conv1', '(4, 2)bits'),\n",
       " ('layer3.5.conv1', '(4, 4)bits'),\n",
       " ('layer3.5.conv1', '(4, 8)bits'),\n",
       " ('layer3.5.conv1', '(8, 2)bits'),\n",
       " ('layer3.5.conv1', '(8, 4)bits'),\n",
       " ('layer3.5.conv1', '(8, 8)bits'),\n",
       " ('layer3.5.conv2', '(2, 2)bits'),\n",
       " ('layer3.5.conv2', '(2, 4)bits'),\n",
       " ('layer3.5.conv2', '(2, 8)bits'),\n",
       " ('layer3.5.conv2', '(4, 2)bits'),\n",
       " ('layer3.5.conv2', '(4, 4)bits'),\n",
       " ('layer3.5.conv2', '(4, 8)bits'),\n",
       " ('layer3.5.conv2', '(8, 2)bits'),\n",
       " ('layer3.5.conv2', '(8, 4)bits'),\n",
       " ('layer3.5.conv2', '(8, 8)bits'),\n",
       " ('layer3.6.conv1', '(2, 2)bits'),\n",
       " ('layer3.6.conv1', '(2, 4)bits'),\n",
       " ('layer3.6.conv1', '(2, 8)bits'),\n",
       " ('layer3.6.conv1', '(4, 2)bits'),\n",
       " ('layer3.6.conv1', '(4, 4)bits'),\n",
       " ('layer3.6.conv1', '(4, 8)bits'),\n",
       " ('layer3.6.conv1', '(8, 2)bits'),\n",
       " ('layer3.6.conv1', '(8, 4)bits'),\n",
       " ('layer3.6.conv1', '(8, 8)bits'),\n",
       " ('layer3.6.conv2', '(2, 2)bits'),\n",
       " ('layer3.6.conv2', '(2, 4)bits'),\n",
       " ('layer3.6.conv2', '(2, 8)bits'),\n",
       " ('layer3.6.conv2', '(4, 2)bits'),\n",
       " ('layer3.6.conv2', '(4, 4)bits'),\n",
       " ('layer3.6.conv2', '(4, 8)bits'),\n",
       " ('layer3.6.conv2', '(8, 2)bits'),\n",
       " ('layer3.6.conv2', '(8, 4)bits'),\n",
       " ('layer3.6.conv2', '(8, 8)bits'),\n",
       " ('layer3.7.conv1', '(2, 2)bits'),\n",
       " ('layer3.7.conv1', '(2, 4)bits'),\n",
       " ('layer3.7.conv1', '(2, 8)bits'),\n",
       " ('layer3.7.conv1', '(4, 2)bits'),\n",
       " ('layer3.7.conv1', '(4, 4)bits'),\n",
       " ('layer3.7.conv1', '(4, 8)bits'),\n",
       " ('layer3.7.conv1', '(8, 2)bits'),\n",
       " ('layer3.7.conv1', '(8, 4)bits'),\n",
       " ('layer3.7.conv1', '(8, 8)bits'),\n",
       " ('layer3.7.conv2', '(2, 2)bits'),\n",
       " ('layer3.7.conv2', '(2, 4)bits'),\n",
       " ('layer3.7.conv2', '(2, 8)bits'),\n",
       " ('layer3.7.conv2', '(4, 2)bits'),\n",
       " ('layer3.7.conv2', '(4, 4)bits'),\n",
       " ('layer3.7.conv2', '(4, 8)bits'),\n",
       " ('layer3.7.conv2', '(8, 2)bits'),\n",
       " ('layer3.7.conv2', '(8, 4)bits'),\n",
       " ('layer3.7.conv2', '(8, 8)bits'),\n",
       " ('layer3.8.conv1', '(2, 2)bits'),\n",
       " ('layer3.8.conv1', '(2, 4)bits'),\n",
       " ('layer3.8.conv1', '(2, 8)bits'),\n",
       " ('layer3.8.conv1', '(4, 2)bits'),\n",
       " ('layer3.8.conv1', '(4, 4)bits'),\n",
       " ('layer3.8.conv1', '(4, 8)bits'),\n",
       " ('layer3.8.conv1', '(8, 2)bits'),\n",
       " ('layer3.8.conv1', '(8, 4)bits'),\n",
       " ('layer3.8.conv1', '(8, 8)bits'),\n",
       " ('layer3.8.conv2', '(2, 2)bits'),\n",
       " ('layer3.8.conv2', '(2, 4)bits'),\n",
       " ('layer3.8.conv2', '(2, 8)bits'),\n",
       " ('layer3.8.conv2', '(4, 2)bits'),\n",
       " ('layer3.8.conv2', '(4, 4)bits'),\n",
       " ('layer3.8.conv2', '(4, 8)bits'),\n",
       " ('layer3.8.conv2', '(8, 2)bits'),\n",
       " ('layer3.8.conv2', '(8, 4)bits'),\n",
       " ('layer3.8.conv2', '(8, 8)bits')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2layerscheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db18b4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_size = np.array([0 for i in range(L)])\n",
    "for l in hm['layer_index']:\n",
    "    index = hm['layer_index'][l]\n",
    "    layer_name, scheme = index2layerscheme[index]\n",
    "    scheme = eval(scheme[:-4])\n",
    "    layer_size[index] = torch.numel(getModuleByName(model,layer_name).weight) * int(scheme[1])\n",
    "\n",
    "layer_bitops = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4555d2",
   "metadata": {},
   "source": [
    "### Calculate sizes and numbers of bitoperations for layers under different quantization options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5886f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_size = np.array([0 for i in range(L)])\n",
    "layer_bitops = np.array([0 for i in range(L)])\n",
    "for l in hm['layer_index']:\n",
    "    index = hm['layer_index'][l]\n",
    "    layer_name, scheme = index2layerscheme[index]\n",
    "    a_bits,w_bits = eval(scheme[:-4])\n",
    "    #print(layer_name,a_bits,w_bits)\n",
    "    layer_size[index] = torch.numel(getModuleByName(model,layer_name).weight) * int(w_bits)\n",
    "    layer_bitops[index] = get_layer_bitops(layer_name,a_bits,w_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79495d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_size # list[layer_index] = size of layer with layer_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30e7a908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_bitops # list[layer_index] = bitops of layer with layer_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97818b5",
   "metadata": {},
   "source": [
    "## Modify the quantization options for layers\n",
    "modify the aw_scheme to list of (a_bits,w_bits) so that CLADO will only consider to choose from these options.\n",
    "the corresponding cached_grad, layer_size, layer_bitops will also exclude quantization options that are not considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c17fa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_grad_full = deepcopy(cached_grad)\n",
    "index2layerscheme_full = deepcopy(index2layerscheme)\n",
    "layer_size_full = deepcopy(layer_size)\n",
    "layer_bitops_full = deepcopy(layer_bitops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3de435fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete 0: (2, 2)bits\n",
      "delete 1: (2, 4)bits\n",
      "delete 2: (2, 8)bits\n",
      "delete 3: (4, 2)bits\n",
      "delete 4: (4, 4)bits\n",
      "delete 5: (4, 8)bits\n",
      "delete 9: (2, 2)bits\n",
      "delete 10: (2, 4)bits\n",
      "delete 11: (2, 8)bits\n",
      "delete 12: (4, 2)bits\n",
      "delete 13: (4, 4)bits\n",
      "delete 14: (4, 8)bits\n",
      "delete 18: (2, 2)bits\n",
      "delete 19: (2, 4)bits\n",
      "delete 20: (2, 8)bits\n",
      "delete 21: (4, 2)bits\n",
      "delete 22: (4, 4)bits\n",
      "delete 23: (4, 8)bits\n",
      "delete 27: (2, 2)bits\n",
      "delete 28: (2, 4)bits\n",
      "delete 29: (2, 8)bits\n",
      "delete 30: (4, 2)bits\n",
      "delete 31: (4, 4)bits\n",
      "delete 32: (4, 8)bits\n",
      "delete 36: (2, 2)bits\n",
      "delete 37: (2, 4)bits\n",
      "delete 38: (2, 8)bits\n",
      "delete 39: (4, 2)bits\n",
      "delete 40: (4, 4)bits\n",
      "delete 41: (4, 8)bits\n",
      "delete 45: (2, 2)bits\n",
      "delete 46: (2, 4)bits\n",
      "delete 47: (2, 8)bits\n",
      "delete 48: (4, 2)bits\n",
      "delete 49: (4, 4)bits\n",
      "delete 50: (4, 8)bits\n",
      "delete 54: (2, 2)bits\n",
      "delete 55: (2, 4)bits\n",
      "delete 56: (2, 8)bits\n",
      "delete 57: (4, 2)bits\n",
      "delete 58: (4, 4)bits\n",
      "delete 59: (4, 8)bits\n",
      "delete 63: (2, 2)bits\n",
      "delete 64: (2, 4)bits\n",
      "delete 65: (2, 8)bits\n",
      "delete 66: (4, 2)bits\n",
      "delete 67: (4, 4)bits\n",
      "delete 68: (4, 8)bits\n",
      "delete 72: (2, 2)bits\n",
      "delete 73: (2, 4)bits\n",
      "delete 74: (2, 8)bits\n",
      "delete 75: (4, 2)bits\n",
      "delete 76: (4, 4)bits\n",
      "delete 77: (4, 8)bits\n",
      "delete 81: (2, 2)bits\n",
      "delete 82: (2, 4)bits\n",
      "delete 83: (2, 8)bits\n",
      "delete 84: (4, 2)bits\n",
      "delete 85: (4, 4)bits\n",
      "delete 86: (4, 8)bits\n",
      "delete 90: (2, 2)bits\n",
      "delete 91: (2, 4)bits\n",
      "delete 92: (2, 8)bits\n",
      "delete 93: (4, 2)bits\n",
      "delete 94: (4, 4)bits\n",
      "delete 95: (4, 8)bits\n",
      "delete 99: (2, 2)bits\n",
      "delete 100: (2, 4)bits\n",
      "delete 101: (2, 8)bits\n",
      "delete 102: (4, 2)bits\n",
      "delete 103: (4, 4)bits\n",
      "delete 104: (4, 8)bits\n",
      "delete 108: (2, 2)bits\n",
      "delete 109: (2, 4)bits\n",
      "delete 110: (2, 8)bits\n",
      "delete 111: (4, 2)bits\n",
      "delete 112: (4, 4)bits\n",
      "delete 113: (4, 8)bits\n",
      "delete 117: (2, 2)bits\n",
      "delete 118: (2, 4)bits\n",
      "delete 119: (2, 8)bits\n",
      "delete 120: (4, 2)bits\n",
      "delete 121: (4, 4)bits\n",
      "delete 122: (4, 8)bits\n",
      "delete 126: (2, 2)bits\n",
      "delete 127: (2, 4)bits\n",
      "delete 128: (2, 8)bits\n",
      "delete 129: (4, 2)bits\n",
      "delete 130: (4, 4)bits\n",
      "delete 131: (4, 8)bits\n",
      "delete 135: (2, 2)bits\n",
      "delete 136: (2, 4)bits\n",
      "delete 137: (2, 8)bits\n",
      "delete 138: (4, 2)bits\n",
      "delete 139: (4, 4)bits\n",
      "delete 140: (4, 8)bits\n",
      "delete 144: (2, 2)bits\n",
      "delete 145: (2, 4)bits\n",
      "delete 146: (2, 8)bits\n",
      "delete 147: (4, 2)bits\n",
      "delete 148: (4, 4)bits\n",
      "delete 149: (4, 8)bits\n",
      "delete 153: (2, 2)bits\n",
      "delete 154: (2, 4)bits\n",
      "delete 155: (2, 8)bits\n",
      "delete 156: (4, 2)bits\n",
      "delete 157: (4, 4)bits\n",
      "delete 158: (4, 8)bits\n",
      "delete 162: (2, 2)bits\n",
      "delete 163: (2, 4)bits\n",
      "delete 164: (2, 8)bits\n",
      "delete 165: (4, 2)bits\n",
      "delete 166: (4, 4)bits\n",
      "delete 167: (4, 8)bits\n",
      "delete 171: (2, 2)bits\n",
      "delete 172: (2, 4)bits\n",
      "delete 173: (2, 8)bits\n",
      "delete 174: (4, 2)bits\n",
      "delete 175: (4, 4)bits\n",
      "delete 176: (4, 8)bits\n",
      "delete 180: (2, 2)bits\n",
      "delete 181: (2, 4)bits\n",
      "delete 182: (2, 8)bits\n",
      "delete 183: (4, 2)bits\n",
      "delete 184: (4, 4)bits\n",
      "delete 185: (4, 8)bits\n",
      "delete 189: (2, 2)bits\n",
      "delete 190: (2, 4)bits\n",
      "delete 191: (2, 8)bits\n",
      "delete 192: (4, 2)bits\n",
      "delete 193: (4, 4)bits\n",
      "delete 194: (4, 8)bits\n",
      "delete 198: (2, 2)bits\n",
      "delete 199: (2, 4)bits\n",
      "delete 200: (2, 8)bits\n",
      "delete 201: (4, 2)bits\n",
      "delete 202: (4, 4)bits\n",
      "delete 203: (4, 8)bits\n",
      "delete 207: (2, 2)bits\n",
      "delete 208: (2, 4)bits\n",
      "delete 209: (2, 8)bits\n",
      "delete 210: (4, 2)bits\n",
      "delete 211: (4, 4)bits\n",
      "delete 212: (4, 8)bits\n",
      "delete 216: (2, 2)bits\n",
      "delete 217: (2, 4)bits\n",
      "delete 218: (2, 8)bits\n",
      "delete 219: (4, 2)bits\n",
      "delete 220: (4, 4)bits\n",
      "delete 221: (4, 8)bits\n",
      "delete 225: (2, 2)bits\n",
      "delete 226: (2, 4)bits\n",
      "delete 227: (2, 8)bits\n",
      "delete 228: (4, 2)bits\n",
      "delete 229: (4, 4)bits\n",
      "delete 230: (4, 8)bits\n",
      "delete 234: (2, 2)bits\n",
      "delete 235: (2, 4)bits\n",
      "delete 236: (2, 8)bits\n",
      "delete 237: (4, 2)bits\n",
      "delete 238: (4, 4)bits\n",
      "delete 239: (4, 8)bits\n",
      "delete 243: (2, 2)bits\n",
      "delete 244: (2, 4)bits\n",
      "delete 245: (2, 8)bits\n",
      "delete 246: (4, 2)bits\n",
      "delete 247: (4, 4)bits\n",
      "delete 248: (4, 8)bits\n",
      "delete 252: (2, 2)bits\n",
      "delete 253: (2, 4)bits\n",
      "delete 254: (2, 8)bits\n",
      "delete 255: (4, 2)bits\n",
      "delete 256: (4, 4)bits\n",
      "delete 257: (4, 8)bits\n",
      "delete 261: (2, 2)bits\n",
      "delete 262: (2, 4)bits\n",
      "delete 263: (2, 8)bits\n",
      "delete 264: (4, 2)bits\n",
      "delete 265: (4, 4)bits\n",
      "delete 266: (4, 8)bits\n",
      "delete 270: (2, 2)bits\n",
      "delete 271: (2, 4)bits\n",
      "delete 272: (2, 8)bits\n",
      "delete 273: (4, 2)bits\n",
      "delete 274: (4, 4)bits\n",
      "delete 275: (4, 8)bits\n",
      "delete 279: (2, 2)bits\n",
      "delete 280: (2, 4)bits\n",
      "delete 281: (2, 8)bits\n",
      "delete 282: (4, 2)bits\n",
      "delete 283: (4, 4)bits\n",
      "delete 284: (4, 8)bits\n",
      "delete 288: (2, 2)bits\n",
      "delete 289: (2, 4)bits\n",
      "delete 290: (2, 8)bits\n",
      "delete 291: (4, 2)bits\n",
      "delete 292: (4, 4)bits\n",
      "delete 293: (4, 8)bits\n",
      "delete 297: (2, 2)bits\n",
      "delete 298: (2, 4)bits\n",
      "delete 299: (2, 8)bits\n",
      "delete 300: (4, 2)bits\n",
      "delete 301: (4, 4)bits\n",
      "delete 302: (4, 8)bits\n",
      "delete 306: (2, 2)bits\n",
      "delete 307: (2, 4)bits\n",
      "delete 308: (2, 8)bits\n",
      "delete 309: (4, 2)bits\n",
      "delete 310: (4, 4)bits\n",
      "delete 311: (4, 8)bits\n",
      "delete 315: (2, 2)bits\n",
      "delete 316: (2, 4)bits\n",
      "delete 317: (2, 8)bits\n",
      "delete 318: (4, 2)bits\n",
      "delete 319: (4, 4)bits\n",
      "delete 320: (4, 8)bits\n",
      "delete 324: (2, 2)bits\n",
      "delete 325: (2, 4)bits\n",
      "delete 326: (2, 8)bits\n",
      "delete 327: (4, 2)bits\n",
      "delete 328: (4, 4)bits\n",
      "delete 329: (4, 8)bits\n",
      "delete 333: (2, 2)bits\n",
      "delete 334: (2, 4)bits\n",
      "delete 335: (2, 8)bits\n",
      "delete 336: (4, 2)bits\n",
      "delete 337: (4, 4)bits\n",
      "delete 338: (4, 8)bits\n",
      "delete 342: (2, 2)bits\n",
      "delete 343: (2, 4)bits\n",
      "delete 344: (2, 8)bits\n",
      "delete 345: (4, 2)bits\n",
      "delete 346: (4, 4)bits\n",
      "delete 347: (4, 8)bits\n",
      "delete 351: (2, 2)bits\n",
      "delete 352: (2, 4)bits\n",
      "delete 353: (2, 8)bits\n",
      "delete 354: (4, 2)bits\n",
      "delete 355: (4, 4)bits\n",
      "delete 356: (4, 8)bits\n",
      "delete 360: (2, 2)bits\n",
      "delete 361: (2, 4)bits\n",
      "delete 362: (2, 8)bits\n",
      "delete 363: (4, 2)bits\n",
      "delete 364: (4, 4)bits\n",
      "delete 365: (4, 8)bits\n",
      "delete 369: (2, 2)bits\n",
      "delete 370: (2, 4)bits\n",
      "delete 371: (2, 8)bits\n",
      "delete 372: (4, 2)bits\n",
      "delete 373: (4, 4)bits\n",
      "delete 374: (4, 8)bits\n",
      "delete 378: (2, 2)bits\n",
      "delete 379: (2, 4)bits\n",
      "delete 380: (2, 8)bits\n",
      "delete 381: (4, 2)bits\n",
      "delete 382: (4, 4)bits\n",
      "delete 383: (4, 8)bits\n",
      "delete 387: (2, 2)bits\n",
      "delete 388: (2, 4)bits\n",
      "delete 389: (2, 8)bits\n",
      "delete 390: (4, 2)bits\n",
      "delete 391: (4, 4)bits\n",
      "delete 392: (4, 8)bits\n",
      "delete 396: (2, 2)bits\n",
      "delete 397: (2, 4)bits\n",
      "delete 398: (2, 8)bits\n",
      "delete 399: (4, 2)bits\n",
      "delete 400: (4, 4)bits\n",
      "delete 401: (4, 8)bits\n",
      "delete 405: (2, 2)bits\n",
      "delete 406: (2, 4)bits\n",
      "delete 407: (2, 8)bits\n",
      "delete 408: (4, 2)bits\n",
      "delete 409: (4, 4)bits\n",
      "delete 410: (4, 8)bits\n",
      "delete 414: (2, 2)bits\n",
      "delete 415: (2, 4)bits\n",
      "delete 416: (2, 8)bits\n",
      "delete 417: (4, 2)bits\n",
      "delete 418: (4, 4)bits\n",
      "delete 419: (4, 8)bits\n",
      "delete 423: (2, 2)bits\n",
      "delete 424: (2, 4)bits\n",
      "delete 425: (2, 8)bits\n",
      "delete 426: (4, 2)bits\n",
      "delete 427: (4, 4)bits\n",
      "delete 428: (4, 8)bits\n",
      "delete 432: (2, 2)bits\n",
      "delete 433: (2, 4)bits\n",
      "delete 434: (2, 8)bits\n",
      "delete 435: (4, 2)bits\n",
      "delete 436: (4, 4)bits\n",
      "delete 437: (4, 8)bits\n",
      "delete 441: (2, 2)bits\n",
      "delete 442: (2, 4)bits\n",
      "delete 443: (2, 8)bits\n",
      "delete 444: (4, 2)bits\n",
      "delete 445: (4, 4)bits\n",
      "delete 446: (4, 8)bits\n",
      "delete 450: (2, 2)bits\n",
      "delete 451: (2, 4)bits\n",
      "delete 452: (2, 8)bits\n",
      "delete 453: (4, 2)bits\n",
      "delete 454: (4, 4)bits\n",
      "delete 455: (4, 8)bits\n",
      "delete 459: (2, 2)bits\n",
      "delete 460: (2, 4)bits\n",
      "delete 461: (2, 8)bits\n",
      "delete 462: (4, 2)bits\n",
      "delete 463: (4, 4)bits\n",
      "delete 464: (4, 8)bits\n",
      "delete 468: (2, 2)bits\n",
      "delete 469: (2, 4)bits\n",
      "delete 470: (2, 8)bits\n",
      "delete 471: (4, 2)bits\n",
      "delete 472: (4, 4)bits\n",
      "delete 473: (4, 8)bits\n",
      "delete 477: (2, 2)bits\n",
      "delete 478: (2, 4)bits\n",
      "delete 479: (2, 8)bits\n",
      "delete 480: (4, 2)bits\n",
      "delete 481: (4, 4)bits\n",
      "delete 482: (4, 8)bits\n",
      "delete 486: (2, 2)bits\n",
      "delete 487: (2, 4)bits\n",
      "delete 488: (2, 8)bits\n",
      "delete 489: (4, 2)bits\n",
      "delete 490: (4, 4)bits\n",
      "delete 491: (4, 8)bits\n",
      "delete 495: (2, 2)bits\n",
      "delete 496: (2, 4)bits\n",
      "delete 497: (2, 8)bits\n",
      "delete 498: (4, 2)bits\n",
      "delete 499: (4, 4)bits\n",
      "delete 500: (4, 8)bits\n"
     ]
    }
   ],
   "source": [
    "MPQ_scheme = (2,4,8)\n",
    "aw_scheme = []\n",
    "for a_bits in MPQ_scheme:\n",
    "    for w_bits in MPQ_scheme:\n",
    "        aw_scheme.append((a_bits,w_bits))\n",
    "\n",
    "#aw_scheme = [(4,8),(8,8),(8,4),(4,4)]\n",
    "aw_scheme = [(8,2),(8,4),(8,8)]\n",
    "\n",
    "del_index = []\n",
    "for index in range(len(index2layerscheme_full)):\n",
    "    if eval(index2layerscheme_full[index][1][:-4]) not in aw_scheme:\n",
    "        del_index.append(index)\n",
    "        print(f'delete {index}: {index2layerscheme_full[index][1]}')\n",
    "\n",
    "cached_grad = np.delete(cached_grad_full,[del_index],axis=0)\n",
    "cached_grad = np.delete(cached_grad,[del_index],axis=1)\n",
    "index2layerscheme = np.delete(index2layerscheme_full,[del_index],axis=0)\n",
    "layer_size = np.delete(layer_size_full,[del_index],axis=0)\n",
    "layer_bitops = np.delete(layer_bitops_full,[del_index],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "156eb5fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 2), (8, 4), (8, 8)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aw_scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c7d9760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168, 168)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4bcd5187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize random variable v\n",
    "# use recitfied sigmoid h(v) to represent alpha\n",
    "# freg is 1-(1-2h(v))**beta, annealing beta to \n",
    "\n",
    "if not isinstance(cached_grad,torch.Tensor):\n",
    "    cached_grad = torch.Tensor(cached_grad)\n",
    "\n",
    "layer_size_tensor = torch.Tensor(layer_size)\n",
    "layer_bitops_tensor = torch.Tensor(layer_bitops)\n",
    "\n",
    "def lossfunc(v,beta,lambda1,lambda2,printInfo=False,naive=False,b=None):\n",
    "    \n",
    "    # alpha here is continuous\n",
    "    alpha = torch.nn.Softmax(dim=1)(v.reshape(-1,len(aw_scheme))).reshape(-1,) # force equality constraint in eq.11\n",
    "    \n",
    "    if not naive:\n",
    "        outer_alpha = torch.outer(alpha,alpha)\n",
    "        netloss = torch.sum(outer_alpha * cached_grad)\n",
    "    else:\n",
    "        netloss = torch.sum(torch.diagonal(cached_grad) * alpha)\n",
    "        \n",
    "    model_size = torch.sum(layer_size_tensor * alpha)/8/1024/1024 # model size in MB\n",
    "    model_bitops = torch.sum(layer_bitops_tensor * alpha)/10**9\n",
    "            \n",
    "    regloss = torch.sum(1-(torch.abs(1-2*alpha))**beta)\n",
    "    regloss *= lambda1\n",
    "\n",
    "    if b is None:\n",
    "        closs = lambda2 * model_bitops\n",
    "    else:\n",
    "        closs = lambda2 * torch.clamp(model_bitops-b,0)\n",
    "    \n",
    "    totloss = netloss + regloss + closs\n",
    "    \n",
    "    if printInfo:\n",
    "        print(f'netloss {netloss.item():.4f} regloss {regloss.item():.4f}(beta={beta:.4f}) closs{closs.item():.4f}(bitops: {model_bitops.item():.2f}G constraint:{b})')\n",
    "        print(f'model size: {model_size.item():.4f}MB')\n",
    "        print('alpha:\\n',alpha)\n",
    "        \n",
    "    return totloss    \n",
    "    \n",
    "# for CVXPY\n",
    "# The problem is\n",
    "# minimize alpha Cached_grad alpha\n",
    "# subject to \n",
    "# alpha * layer_size <= size_constraint\n",
    "# alpha * layer_bitops <= bitops_constraint\n",
    "# alpha in the same layer summed up to 1\n",
    "# alpha is integer\n",
    "# 0 <= alpha <=1\n",
    "\n",
    "\n",
    "def CLADO_solver(cached_grad,size_bound,bitops_bound,options_per_layer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04760381",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = cached_grad.size()[0]\n",
    "def optimize(n_iteration,lr,beta,lambda1,lambda2,b=None,naive=False,hardInit=False):\n",
    "    \n",
    "    \n",
    "    #v = torch.nn.Parameter(torch.randn(L))\n",
    "    v = torch.nn.Parameter(torch.zeros(L))\n",
    "    if hardInit:\n",
    "        v = torch.nn.Parameter(torch.zeros(L))\n",
    "        with torch.no_grad():\n",
    "            for i in range(L):\n",
    "                if i % len(aw_scheme) == len(aw_scheme)-1:\n",
    "                    v[i].data += 1.\n",
    "                    \n",
    "    optim = torch.optim.Adam([v,],lr=lr)\n",
    "    bs = np.linspace(beta[0],beta[1],n_iteration)\n",
    "    \n",
    "    for i in range(n_iteration):\n",
    "        if i==0 or (i+1) % 1000 == 0:\n",
    "            printInfo = True\n",
    "            print(f'Iter {i+1}')\n",
    "        else:\n",
    "            printInfo = False\n",
    "            \n",
    "        optim.zero_grad()\n",
    "        loss = lossfunc(v,bs[i],lambda1,lambda2,printInfo=printInfo,b=b,naive=naive)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    \n",
    "    return v\n",
    "\n",
    "def evaluate_decision(v,printInfo=False,test=test):\n",
    "    global mqb_mix_model\n",
    "    v = v.detach()\n",
    "    # alpha = torch.nn.Softmax(dim=1)(v.reshape(-1,len(MPQ_scheme)))\n",
    "    offset = torch.ones(int(L/len(aw_scheme)),dtype=int) * len(aw_scheme)\n",
    "    offset = offset.cumsum(dim=-1) - len(aw_scheme)\n",
    "    select = v.reshape(-1,len(aw_scheme)).argmax(dim=1) + offset\n",
    "    \n",
    "    modelsize = (layer_size[select]).sum()/8/1024/1024\n",
    "    bitops = (layer_bitops[select]).sum()/10**9\n",
    "    \n",
    "    decisions = {}\n",
    "    for scheme_id in select.numpy():\n",
    "        layer,scheme = index2layerscheme[scheme_id]\n",
    "        decisions[layer] = eval(scheme[:-4])\n",
    "    \n",
    "    print(\"evaluate_decision\\n\",decisions)\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # perturb layers\n",
    "        perturb(decisions)\n",
    "            \n",
    "        # do evaluation\n",
    "        res = evaluate(test,mqb_mix_model)\n",
    "        \n",
    "        # recover layers\n",
    "        mqb_mix_model = deepcopy(mqb_fp_model)\n",
    "    return res,modelsize,bitops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0a23a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "\n",
    "def MIQCP_optimize(cached_grad,layer_bitops,layer_size,\n",
    "                   schemes_per_layer=3,\n",
    "                   bitops_bound=np.inf,size_bound=np.inf,\n",
    "                   naive=False):\n",
    "    \n",
    "    if cached_grad.__class__ == torch.Tensor:\n",
    "        cached_grad = cached_grad.cpu().numpy()\n",
    "    \n",
    "    x = cp.Variable(cached_grad.shape[0], boolean=True)\n",
    "    schemes_per_layer = schemes_per_layer\n",
    "    assert cached_grad.shape[0]%schemes_per_layer == 0, 'cached_gradient shape[0] does not divde schemes per layer'\n",
    "    num_layers = cached_grad.shape[0]//schemes_per_layer\n",
    "    \n",
    "    if not naive:\n",
    "        # convexation of cached_grad\n",
    "        es,us = np.linalg.eig(cached_grad)\n",
    "        #es[es<0] = 0\n",
    "        C = us@np.diag(es)@us.T\n",
    "        C = cp.atoms.affine.wraps.psd_wrap(C)\n",
    "        objective = cp.Minimize(cp.quad_form(x,C))\n",
    "    else:\n",
    "        objective = cp.Minimize(np.diagonal(cached_grad)@x)\n",
    "\n",
    "    equality_constraint_matrix = []\n",
    "    for i in range(num_layers):\n",
    "        col = np.zeros(cached_grad.shape[0])\n",
    "        col[i*schemes_per_layer:(i+1)*schemes_per_layer] = 1\n",
    "        equality_constraint_matrix.append(col)\n",
    "\n",
    "    equality_constraint_matrix = np.array(equality_constraint_matrix)\n",
    "\n",
    "    constraints = [equality_constraint_matrix@x == np.ones((num_layers,)),\n",
    "                   layer_bitops@x/10**9<=bitops_bound,\n",
    "                   layer_size@x/8/1024/1024<=size_bound]\n",
    "\n",
    "    prob = cp.Problem(objective,constraints)\n",
    "    prob.solve(verbose=True,solver='GUROBI',TimeLimit=10)\n",
    "    \n",
    "    # Print result.\n",
    "    print(\"\\nThe optimal value is\", prob.value)\n",
    "    print(\"A solution x is\")\n",
    "    print(x.value)\n",
    "    #print(f\"bitops: {x.value@layer_bitops}\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036b4939",
   "metadata": {},
   "source": [
    "## Sanity Check: no constraint optimization\n",
    "Without constraint, optimization should return (ideally) an 8-bit model, or performance close to 8-bit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "45587252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate_decision\n",
      " {'layer1.0.conv1': (8, 8), 'layer1.0.conv2': (8, 8), 'layer1.1.conv1': (8, 8), 'layer1.1.conv2': (8, 8), 'layer1.2.conv1': (8, 8), 'layer1.2.conv2': (8, 8), 'layer1.3.conv1': (8, 8), 'layer1.3.conv2': (8, 8), 'layer1.4.conv1': (8, 8), 'layer1.4.conv2': (8, 8), 'layer1.5.conv1': (8, 8), 'layer1.5.conv2': (8, 8), 'layer1.6.conv1': (8, 8), 'layer1.6.conv2': (8, 8), 'layer1.7.conv1': (8, 8), 'layer1.7.conv2': (8, 8), 'layer1.8.conv1': (8, 8), 'layer1.8.conv2': (8, 8), 'layer2.0.conv1': (8, 8), 'layer2.0.conv2': (8, 8), 'layer2.0.downsample.0': (8, 8), 'layer2.1.conv1': (8, 8), 'layer2.1.conv2': (8, 8), 'layer2.2.conv1': (8, 8), 'layer2.2.conv2': (8, 8), 'layer2.3.conv1': (8, 8), 'layer2.3.conv2': (8, 8), 'layer2.4.conv1': (8, 8), 'layer2.4.conv2': (8, 8), 'layer2.5.conv1': (8, 8), 'layer2.5.conv2': (8, 8), 'layer2.6.conv1': (8, 8), 'layer2.6.conv2': (8, 8), 'layer2.7.conv1': (8, 8), 'layer2.7.conv2': (8, 8), 'layer2.8.conv1': (8, 8), 'layer2.8.conv2': (8, 8), 'layer3.0.conv1': (8, 8), 'layer3.0.conv2': (8, 8), 'layer3.0.downsample.0': (8, 8), 'layer3.1.conv1': (8, 8), 'layer3.1.conv2': (8, 8), 'layer3.2.conv1': (8, 8), 'layer3.2.conv2': (8, 8), 'layer3.3.conv1': (8, 8), 'layer3.3.conv2': (8, 8), 'layer3.4.conv1': (8, 8), 'layer3.4.conv2': (8, 8), 'layer3.5.conv1': (8, 8), 'layer3.5.conv2': (8, 8), 'layer3.6.conv1': (8, 8), 'layer3.6.conv2': (8, 8), 'layer3.7.conv1': (8, 8), 'layer3.7.conv2': (8, 8), 'layer3.8.conv1': (8, 8), 'layer3.8.conv2': (8, 8)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'top1': tensor([72.6400], device='cuda:0'),\n",
       "  'top5': tensor([91.8101], device='cuda:0'),\n",
       "  'loss': tensor(1.3005, device='cuda:0'),\n",
       "  'time': 3.1594929695129395},\n",
       " 0.81103515625,\n",
       " 19.15400192)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = optimize(n_iteration=0,lr=2e-3,beta=[20,2],\n",
    "             lambda1=0,lambda2=0,\n",
    "             naive=False,hardInit=True)\n",
    "evaluate_decision(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fa2f91e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1\n",
      "netloss -565.9978 regloss 0.0000(beta=20.0000) closs0.0000(bitops: 12.06G constraint:None)\n",
      "model size: 0.4731MB\n",
      "alpha:\n",
      " tensor([0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333,\n",
      "        0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333,\n",
      "        0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333,\n",
      "        0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333,\n",
      "        0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333,\n",
      "        0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333,\n",
      "        0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333,\n",
      "        0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333,\n",
      "        0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333,\n",
      "        0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333,\n",
      "        0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333,\n",
      "        0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333,\n",
      "        0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333,\n",
      "        0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333,\n",
      "        0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333,\n",
      "        0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333,\n",
      "        0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333,\n",
      "        0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333,\n",
      "        0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333],\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n",
      "Iter 1000\n",
      "netloss -583.6440 regloss 0.0000(beta=16.4029) closs0.0000(bitops: 13.71G constraint:None)\n",
      "model size: 0.5162MB\n",
      "alpha:\n",
      " tensor([0.0775, 0.3733, 0.5491, 0.0770, 0.4222, 0.5008, 0.0929, 0.3117, 0.5953,\n",
      "        0.0859, 0.3246, 0.5896, 0.1157, 0.3355, 0.5488, 0.2327, 0.3747, 0.3926,\n",
      "        0.0881, 0.4089, 0.5030, 0.0828, 0.3729, 0.5443, 0.0925, 0.4235, 0.4840,\n",
      "        0.1091, 0.4252, 0.4657, 0.0891, 0.5011, 0.4098, 0.0868, 0.4508, 0.4624,\n",
      "        0.1183, 0.3349, 0.5468, 0.1676, 0.4047, 0.4277, 0.0806, 0.4038, 0.5157,\n",
      "        0.0897, 0.4469, 0.4633, 0.0676, 0.4634, 0.4689, 0.0739, 0.4053, 0.5208,\n",
      "        0.1082, 0.5038, 0.3880, 0.1030, 0.3348, 0.5622, 0.1484, 0.1092, 0.7424,\n",
      "        0.0776, 0.4289, 0.4935, 0.1885, 0.4114, 0.4001, 0.1145, 0.4321, 0.4535,\n",
      "        0.1565, 0.4151, 0.4284, 0.0883, 0.3886, 0.5231, 0.1220, 0.4283, 0.4497,\n",
      "        0.0803, 0.4261, 0.4936, 0.3301, 0.3274, 0.3426, 0.0821, 0.4081, 0.5098,\n",
      "        0.2916, 0.3439, 0.3645, 0.1177, 0.3514, 0.5309, 0.2262, 0.3571, 0.4167,\n",
      "        0.0718, 0.4030, 0.5252, 0.0888, 0.4146, 0.4965, 0.0867, 0.3506, 0.5627,\n",
      "        0.1125, 0.3998, 0.4877, 0.1492, 0.2346, 0.6161, 0.0898, 0.2284, 0.6818,\n",
      "        0.2481, 0.4323, 0.3196, 0.1619, 0.3747, 0.4634, 0.2733, 0.3250, 0.4017,\n",
      "        0.0735, 0.4299, 0.4966, 0.1131, 0.3954, 0.4915, 0.2578, 0.3598, 0.3824,\n",
      "        0.4844, 0.2939, 0.2217, 0.1098, 0.3186, 0.5716, 0.4445, 0.2894, 0.2661,\n",
      "        0.0751, 0.4084, 0.5165, 0.1122, 0.4264, 0.4614, 0.2922, 0.3033, 0.4045,\n",
      "        0.0953, 0.4085, 0.4962, 0.6594, 0.1615, 0.1791, 0.6252, 0.1801, 0.1947,\n",
      "        0.8124, 0.0935, 0.0942, 0.7423, 0.1281, 0.1295],\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n",
      "Iter 2000\n",
      "netloss -584.1464 regloss 0.0000(beta=12.8022) closs0.0000(bitops: 13.75G constraint:None)\n",
      "model size: 0.5121MB\n",
      "alpha:\n",
      " tensor([0.0397, 0.3338, 0.6266, 0.0382, 0.4438, 0.5180, 0.0467, 0.3347, 0.6187,\n",
      "        0.0415, 0.3416, 0.6169, 0.1141, 0.3344, 0.5515, 0.2509, 0.3662, 0.3829,\n",
      "        0.0612, 0.4364, 0.5024, 0.0450, 0.3809, 0.5742, 0.0780, 0.4374, 0.4846,\n",
      "        0.1027, 0.4309, 0.4665, 0.0617, 0.5184, 0.4199, 0.0595, 0.4637, 0.4768,\n",
      "        0.1381, 0.3273, 0.5346, 0.2330, 0.3785, 0.3885, 0.0494, 0.4192, 0.5314,\n",
      "        0.0688, 0.4599, 0.4713, 0.0331, 0.4771, 0.4898, 0.0374, 0.4226, 0.5400,\n",
      "        0.1000, 0.5165, 0.3835, 0.1014, 0.3114, 0.5873, 0.1815, 0.0691, 0.7494,\n",
      "        0.0423, 0.4471, 0.5107, 0.2237, 0.3922, 0.3841, 0.1342, 0.4280, 0.4378,\n",
      "        0.1998, 0.3980, 0.4023, 0.0546, 0.4073, 0.5381, 0.1421, 0.4182, 0.4397,\n",
      "        0.0435, 0.4328, 0.5236, 0.3447, 0.3198, 0.3355, 0.0438, 0.4187, 0.5375,\n",
      "        0.2910, 0.3447, 0.3643, 0.1272, 0.3549, 0.5180, 0.2540, 0.3472, 0.3988,\n",
      "        0.0332, 0.4063, 0.5605, 0.0585, 0.4281, 0.5135, 0.0466, 0.3554, 0.5980,\n",
      "        0.0976, 0.4039, 0.4985, 0.2305, 0.1504, 0.6192, 0.0819, 0.2160, 0.7021,\n",
      "        0.2754, 0.4054, 0.3192, 0.2007, 0.3635, 0.4358, 0.3101, 0.3053, 0.3845,\n",
      "        0.0341, 0.4190, 0.5469, 0.1207, 0.3953, 0.4840, 0.2831, 0.3399, 0.3770,\n",
      "        0.5083, 0.2784, 0.2133, 0.0872, 0.3243, 0.5885, 0.4468, 0.2871, 0.2661,\n",
      "        0.0343, 0.3986, 0.5671, 0.1697, 0.4001, 0.4301, 0.3671, 0.2758, 0.3571,\n",
      "        0.0604, 0.4191, 0.5205, 0.6809, 0.1348, 0.1843, 0.6635, 0.1613, 0.1752,\n",
      "        0.9219, 0.0391, 0.0390, 0.8228, 0.0891, 0.0881],\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n",
      "Iter 3000\n",
      "netloss -584.3764 regloss 0.0000(beta=9.2014) closs0.0000(bitops: 13.69G constraint:None)\n",
      "model size: 0.5028MB\n",
      "alpha:\n",
      " tensor([0.0228, 0.3257, 0.6515, 0.0204, 0.4513, 0.5283, 0.0239, 0.3311, 0.6451,\n",
      "        0.0213, 0.3298, 0.6489, 0.1336, 0.3165, 0.5499, 0.2477, 0.3682, 0.3841,\n",
      "        0.0509, 0.4469, 0.5022, 0.0252, 0.3759, 0.5990, 0.0953, 0.4324, 0.4724,\n",
      "        0.1083, 0.4297, 0.4620, 0.0464, 0.5337, 0.4199, 0.0461, 0.4697, 0.4842,\n",
      "        0.1603, 0.3137, 0.5260, 0.2307, 0.3786, 0.3907, 0.0370, 0.4200, 0.5430,\n",
      "        0.0653, 0.4625, 0.4722, 0.0185, 0.4825, 0.4990, 0.0207, 0.4280, 0.5513,\n",
      "        0.1065, 0.5280, 0.3654, 0.1607, 0.3092, 0.5301, 0.2136, 0.0518, 0.7345,\n",
      "        0.0247, 0.4535, 0.5218, 0.2213, 0.3935, 0.3851, 0.1733, 0.4133, 0.4133,\n",
      "        0.2230, 0.3901, 0.3869, 0.0346, 0.4152, 0.5502, 0.1574, 0.4106, 0.4319,\n",
      "        0.0249, 0.4329, 0.5422, 0.3615, 0.3108, 0.3277, 0.0231, 0.4170, 0.5599,\n",
      "        0.2867, 0.3468, 0.3664, 0.1382, 0.3502, 0.5116, 0.2716, 0.3392, 0.3893,\n",
      "        0.0168, 0.4070, 0.5763, 0.0433, 0.4339, 0.5228, 0.0248, 0.3495, 0.6258,\n",
      "        0.0799, 0.4043, 0.5158, 0.3083, 0.1106, 0.5811, 0.1267, 0.1824, 0.6909,\n",
      "        0.2942, 0.3917, 0.3141, 0.2130, 0.3596, 0.4274, 0.3393, 0.2851, 0.3756,\n",
      "        0.0170, 0.4153, 0.5678, 0.1225, 0.3913, 0.4862, 0.3243, 0.3169, 0.3588,\n",
      "        0.5807, 0.2504, 0.1689, 0.0763, 0.3146, 0.6091, 0.4760, 0.2758, 0.2481,\n",
      "        0.0166, 0.3986, 0.5847, 0.1922, 0.3882, 0.4196, 0.4232, 0.2457, 0.3311,\n",
      "        0.0339, 0.4213, 0.5448, 0.7904, 0.0838, 0.1258, 0.7366, 0.1266, 0.1368,\n",
      "        0.9689, 0.0155, 0.0156, 0.9034, 0.0491, 0.0475],\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n",
      "Iter 4000\n",
      "netloss -584.4915 regloss 0.0000(beta=5.6007) closs0.0000(bitops: 13.66G constraint:None)\n",
      "model size: 0.4971MB\n",
      "alpha:\n",
      " tensor([0.0135, 0.3197, 0.6668, 0.0111, 0.4533, 0.5357, 0.0123, 0.3269, 0.6608,\n",
      "        0.0112, 0.3228, 0.6660, 0.1414, 0.3065, 0.5521, 0.2431, 0.3707, 0.3862,\n",
      "        0.0452, 0.4519, 0.5029, 0.0135, 0.3724, 0.6141, 0.1228, 0.4221, 0.4552,\n",
      "        0.1118, 0.4288, 0.4594, 0.0343, 0.5440, 0.4217, 0.0359, 0.4744, 0.4897,\n",
      "        0.1627, 0.3089, 0.5285, 0.2244, 0.3805, 0.3951, 0.0292, 0.4200, 0.5509,\n",
      "        0.0617, 0.4643, 0.4740, 0.0109, 0.4846, 0.5044, 0.0116, 0.4300, 0.5583,\n",
      "        0.1252, 0.5332, 0.3415, 0.2076, 0.3124, 0.4801, 0.2247, 0.0393, 0.7360,\n",
      "        0.0141, 0.4559, 0.5300, 0.2158, 0.3965, 0.3878, 0.1795, 0.4110, 0.4095,\n",
      "        0.2348, 0.3871, 0.3781, 0.0202, 0.4200, 0.5598, 0.1635, 0.4076, 0.4289,\n",
      "        0.0143, 0.4309, 0.5549, 0.3691, 0.3061, 0.3248, 0.0119, 0.4152, 0.5729,\n",
      "        0.2824, 0.3489, 0.3687, 0.1353, 0.3511, 0.5136, 0.2799, 0.3354, 0.3846,\n",
      "        0.0089, 0.4065, 0.5846, 0.0341, 0.4377, 0.5282, 0.0130, 0.3466, 0.6404,\n",
      "        0.0636, 0.4072, 0.5292, 0.3500, 0.0833, 0.5667, 0.1519, 0.1625, 0.6856,\n",
      "        0.3087, 0.3863, 0.3050, 0.2201, 0.3570, 0.4229, 0.3574, 0.2725, 0.3702,\n",
      "        0.0088, 0.4146, 0.5766, 0.1247, 0.3879, 0.4875, 0.3501, 0.3032, 0.3467,\n",
      "        0.6253, 0.2346, 0.1401, 0.0704, 0.3078, 0.6218, 0.4938, 0.2685, 0.2377,\n",
      "        0.0085, 0.3969, 0.5946, 0.2081, 0.3805, 0.4114, 0.4567, 0.2267, 0.3166,\n",
      "        0.0173, 0.4228, 0.5599, 0.8691, 0.0495, 0.0814, 0.7856, 0.1021, 0.1123,\n",
      "        0.9865, 0.0067, 0.0068, 0.9516, 0.0248, 0.0236],\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5000\n",
      "netloss -584.5458 regloss 0.0000(beta=2.0000) closs0.0000(bitops: 13.65G constraint:None)\n",
      "model size: 0.4946MB\n",
      "alpha:\n",
      " tensor([0.0082, 0.3186, 0.6732, 0.0062, 0.4546, 0.5392, 0.0066, 0.3256, 0.6678,\n",
      "        0.0062, 0.3204, 0.6734, 0.1464, 0.3016, 0.5520, 0.2414, 0.3718, 0.3868,\n",
      "        0.0437, 0.4540, 0.5023, 0.0075, 0.3714, 0.6211, 0.1348, 0.4180, 0.4472,\n",
      "        0.1138, 0.4281, 0.4580, 0.0263, 0.5496, 0.4241, 0.0288, 0.4776, 0.4937,\n",
      "        0.1658, 0.3061, 0.5282, 0.2229, 0.3808, 0.3963, 0.0240, 0.4210, 0.5550,\n",
      "        0.0604, 0.4651, 0.4745, 0.0067, 0.4862, 0.5071, 0.0067, 0.4319, 0.5614,\n",
      "        0.1363, 0.5349, 0.3287, 0.2257, 0.3155, 0.4587, 0.2292, 0.0313, 0.7395,\n",
      "        0.0082, 0.4576, 0.5343, 0.2143, 0.3972, 0.3885, 0.1816, 0.4099, 0.4085,\n",
      "        0.2402, 0.3857, 0.3741, 0.0117, 0.4233, 0.5650, 0.1669, 0.4059, 0.4271,\n",
      "        0.0083, 0.4308, 0.5609, 0.3725, 0.3040, 0.3235, 0.0064, 0.4151, 0.5785,\n",
      "        0.2805, 0.3497, 0.3698, 0.1350, 0.3514, 0.5135, 0.2843, 0.3336, 0.3821,\n",
      "        0.0050, 0.4071, 0.5879, 0.0288, 0.4400, 0.5311, 0.0071, 0.3460, 0.6469,\n",
      "        0.0554, 0.4091, 0.5355, 0.3683, 0.0665, 0.5651, 0.1617, 0.1545, 0.6838,\n",
      "        0.3160, 0.3838, 0.3002, 0.2239, 0.3555, 0.4206, 0.3657, 0.2670, 0.3674,\n",
      "        0.0048, 0.4154, 0.5798, 0.1262, 0.3862, 0.4876, 0.3616, 0.2972, 0.3412,\n",
      "        0.6444, 0.2279, 0.1278, 0.0689, 0.3049, 0.6262, 0.5011, 0.2655, 0.2334,\n",
      "        0.0046, 0.3969, 0.5985, 0.2160, 0.3770, 0.4070, 0.4719, 0.2185, 0.3095,\n",
      "        0.0092, 0.4242, 0.5666, 0.9066, 0.0322, 0.0612, 0.8069, 0.0913, 0.1018,\n",
      "        0.9934, 0.0033, 0.0034, 0.9743, 0.0132, 0.0124],\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n",
      "evaluate_decision\n",
      " {'layer1.0.conv1': (8, 8), 'layer1.0.conv2': (8, 8), 'layer1.1.conv1': (8, 8), 'layer1.1.conv2': (8, 8), 'layer1.2.conv1': (8, 8), 'layer1.2.conv2': (8, 8), 'layer1.3.conv1': (8, 8), 'layer1.3.conv2': (8, 8), 'layer1.4.conv1': (8, 8), 'layer1.4.conv2': (8, 8), 'layer1.5.conv1': (8, 4), 'layer1.5.conv2': (8, 8), 'layer1.6.conv1': (8, 8), 'layer1.6.conv2': (8, 8), 'layer1.7.conv1': (8, 8), 'layer1.7.conv2': (8, 8), 'layer1.8.conv1': (8, 8), 'layer1.8.conv2': (8, 8), 'layer2.0.conv1': (8, 4), 'layer2.0.conv2': (8, 8), 'layer2.0.downsample.0': (8, 8), 'layer2.1.conv1': (8, 8), 'layer2.1.conv2': (8, 4), 'layer2.2.conv1': (8, 4), 'layer2.2.conv2': (8, 4), 'layer2.3.conv1': (8, 8), 'layer2.3.conv2': (8, 8), 'layer2.4.conv1': (8, 8), 'layer2.4.conv2': (8, 2), 'layer2.5.conv1': (8, 8), 'layer2.5.conv2': (8, 8), 'layer2.6.conv1': (8, 8), 'layer2.6.conv2': (8, 8), 'layer2.7.conv1': (8, 8), 'layer2.7.conv2': (8, 8), 'layer2.8.conv1': (8, 8), 'layer2.8.conv2': (8, 8), 'layer3.0.conv1': (8, 8), 'layer3.0.conv2': (8, 8), 'layer3.0.downsample.0': (8, 4), 'layer3.1.conv1': (8, 8), 'layer3.1.conv2': (8, 8), 'layer3.2.conv1': (8, 8), 'layer3.2.conv2': (8, 8), 'layer3.3.conv1': (8, 2), 'layer3.3.conv2': (8, 2), 'layer3.4.conv1': (8, 8), 'layer3.4.conv2': (8, 2), 'layer3.5.conv1': (8, 8), 'layer3.5.conv2': (8, 8), 'layer3.6.conv1': (8, 2), 'layer3.6.conv2': (8, 8), 'layer3.7.conv1': (8, 2), 'layer3.7.conv2': (8, 2), 'layer3.8.conv1': (8, 2), 'layer3.8.conv2': (8, 2)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'top1': tensor([44.2500], device='cuda:0'),\n",
       "  'top5': tensor([73.4000], device='cuda:0'),\n",
       "  'loss': tensor(2.2661, device='cuda:0'),\n",
       "  'time': 3.1785309314727783},\n",
       " 0.5760498046875,\n",
       " 16.257769472)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the code below, will see that optimization leads to negative loss\n",
    "# with bad results/ not passing the sanity check\n",
    "v = optimize(n_iteration=5000,lr=2e-3,beta=[20,2],\n",
    "             lambda1=0,lambda2=0,\n",
    "             naive=False,hardInit=False)\n",
    "evaluate_decision(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9556fe5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================\n",
      "                                     CVXPY                                     \n",
      "                                     v1.2.1                                    \n",
      "===============================================================================\n",
      "(CVXPY) Oct 05 07:14:41 AM: Your problem has 168 variables, 3 constraints, and 0 parameters.\n",
      "(CVXPY) Oct 05 07:14:41 AM: It is compliant with the following grammars: DCP, DQCP\n",
      "(CVXPY) Oct 05 07:14:41 AM: (If you need to solve this problem multiple times, but with different data, consider using parameters.)\n",
      "(CVXPY) Oct 05 07:14:41 AM: CVXPY will first compile your problem; then, it will invoke a numerical solver to obtain a solution.\n",
      "-------------------------------------------------------------------------------\n",
      "                                  Compilation                                  \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Oct 05 07:14:41 AM: Compiling problem (target solver=GUROBI).\n",
      "(CVXPY) Oct 05 07:14:41 AM: Reduction chain: Dcp2Cone -> CvxAttr2Constr -> ConeMatrixStuffing -> GUROBI\n",
      "(CVXPY) Oct 05 07:14:41 AM: Applying reduction Dcp2Cone\n",
      "(CVXPY) Oct 05 07:14:41 AM: Applying reduction CvxAttr2Constr\n",
      "(CVXPY) Oct 05 07:14:41 AM: Applying reduction ConeMatrixStuffing\n",
      "(CVXPY) Oct 05 07:14:41 AM: Applying reduction GUROBI\n",
      "(CVXPY) Oct 05 07:14:41 AM: Finished problem compilation (took 1.021e-02 seconds).\n",
      "-------------------------------------------------------------------------------\n",
      "                                Numerical solver                               \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Oct 05 07:14:41 AM: Invoking solver GUROBI  to obtain a solution.\n",
      "Restricted license - for non-production use only - expires 2023-10-25\n",
      "Set parameter QCPDual to value 1\n",
      "Gurobi Optimizer version 9.5.2 build v9.5.2rc0 (linux64)\n",
      "Thread count: 4 physical cores, 4 logical processors, using up to 4 threads\n",
      "Optimize a model with 58 rows, 168 columns and 504 nonzeros\n",
      "Model fingerprint: 0xba60937f\n",
      "Variable types: 0 continuous, 168 integer (168 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-04, 1e+00]\n",
      "  Objective range  [3e-01, 7e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+00]\n",
      "Found heuristic solution: objective 44.1250489\n",
      "Presolve removed 58 rows and 168 columns\n",
      "Presolve time: 0.00s\n",
      "Presolve: All rows and columns removed\n",
      "\n",
      "Explored 0 nodes (0 simplex iterations) in 0.00 seconds (0.00 work units)\n",
      "Thread count was 1 (of 4 available processors)\n",
      "\n",
      "Solution count 2: 21.3283 44.125 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 2.132834342122e+01, best bound 2.132834342122e+01, gap 0.0000%\n",
      "-------------------------------------------------------------------------------\n",
      "                                    Summary                                    \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Oct 05 07:14:41 AM: Problem status: optimal\n",
      "(CVXPY) Oct 05 07:14:41 AM: Optimal value: 2.133e+01\n",
      "(CVXPY) Oct 05 07:14:41 AM: Compilation took 1.021e-02 seconds\n",
      "(CVXPY) Oct 05 07:14:41 AM: Solver (including time spent in interface) took 3.414e-02 seconds\n",
      "\n",
      "The optimal value is 21.32834342122078\n",
      "A solution x is\n",
      "[0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.]\n",
      "evaluate_decision\n",
      " {'layer1.0.conv1': (8, 4), 'layer1.0.conv2': (8, 4), 'layer1.1.conv1': (8, 4), 'layer1.1.conv2': (8, 4), 'layer1.2.conv1': (8, 4), 'layer1.2.conv2': (8, 4), 'layer1.3.conv1': (8, 8), 'layer1.3.conv2': (8, 8), 'layer1.4.conv1': (8, 8), 'layer1.4.conv2': (8, 4), 'layer1.5.conv1': (8, 8), 'layer1.5.conv2': (8, 2), 'layer1.6.conv1': (8, 8), 'layer1.6.conv2': (8, 4), 'layer1.7.conv1': (8, 4), 'layer1.7.conv2': (8, 2), 'layer1.8.conv1': (8, 8), 'layer1.8.conv2': (8, 4), 'layer2.0.conv1': (8, 8), 'layer2.0.conv2': (8, 8), 'layer2.0.downsample.0': (8, 8), 'layer2.1.conv1': (8, 8), 'layer2.1.conv2': (8, 8), 'layer2.2.conv1': (8, 8), 'layer2.2.conv2': (8, 8), 'layer2.3.conv1': (8, 8), 'layer2.3.conv2': (8, 4), 'layer2.4.conv1': (8, 8), 'layer2.4.conv2': (8, 8), 'layer2.5.conv1': (8, 8), 'layer2.5.conv2': (8, 8), 'layer2.6.conv1': (8, 8), 'layer2.6.conv2': (8, 8), 'layer2.7.conv1': (8, 8), 'layer2.7.conv2': (8, 8), 'layer2.8.conv1': (8, 8), 'layer2.8.conv2': (8, 8), 'layer3.0.conv1': (8, 8), 'layer3.0.conv2': (8, 8), 'layer3.0.downsample.0': (8, 8), 'layer3.1.conv1': (8, 8), 'layer3.1.conv2': (8, 8), 'layer3.2.conv1': (8, 8), 'layer3.2.conv2': (8, 8), 'layer3.3.conv1': (8, 8), 'layer3.3.conv2': (8, 8), 'layer3.4.conv1': (8, 8), 'layer3.4.conv2': (8, 8), 'layer3.5.conv1': (8, 8), 'layer3.5.conv2': (8, 8), 'layer3.6.conv1': (8, 8), 'layer3.6.conv2': (8, 8), 'layer3.7.conv1': (8, 4), 'layer3.7.conv2': (8, 4), 'layer3.8.conv1': (8, 4), 'layer3.8.conv2': (8, 4)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'top1': tensor([66.3200], device='cuda:0'),\n",
       "  'top5': tensor([88.4800], device='cuda:0'),\n",
       "  'loss': tensor(1.6261, device='cuda:0'),\n",
       "  'time': 3.169888734817505},\n",
       " 0.7220458984375,\n",
       " 16.267173888)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = MIQCP_optimize(cached_grad=cached_grad,\n",
    "                   layer_bitops=layer_bitops,\n",
    "                   layer_size=layer_size,\n",
    "                   schemes_per_layer=len(aw_scheme),\n",
    "                   bitops_bound=np.inf,size_bound=np.inf,\n",
    "                   naive=True)\n",
    "v = torch.Tensor(v.value)\n",
    "evaluate_decision(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0217953a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================\n",
      "                                     CVXPY                                     \n",
      "                                     v1.2.1                                    \n",
      "===============================================================================\n",
      "(CVXPY) Oct 05 08:03:36 AM: Your problem has 168 variables, 3 constraints, and 0 parameters.\n",
      "(CVXPY) Oct 05 08:03:36 AM: It is compliant with the following grammars: DCP, DQCP\n",
      "(CVXPY) Oct 05 08:03:36 AM: (If you need to solve this problem multiple times, but with different data, consider using parameters.)\n",
      "(CVXPY) Oct 05 08:03:36 AM: CVXPY will first compile your problem; then, it will invoke a numerical solver to obtain a solution.\n",
      "-------------------------------------------------------------------------------\n",
      "                                  Compilation                                  \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Oct 05 08:03:36 AM: Compiling problem (target solver=GUROBI).\n",
      "(CVXPY) Oct 05 08:03:36 AM: Reduction chain: CvxAttr2Constr -> Qp2SymbolicQp -> QpMatrixStuffing -> GUROBI\n",
      "(CVXPY) Oct 05 08:03:36 AM: Applying reduction CvxAttr2Constr\n",
      "(CVXPY) Oct 05 08:03:36 AM: Applying reduction Qp2SymbolicQp\n",
      "(CVXPY) Oct 05 08:03:36 AM: Applying reduction QpMatrixStuffing\n",
      "(CVXPY) Oct 05 08:03:36 AM: Applying reduction GUROBI\n",
      "(CVXPY) Oct 05 08:03:36 AM: Finished problem compilation (took 1.862e-02 seconds).\n",
      "-------------------------------------------------------------------------------\n",
      "                                Numerical solver                               \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Oct 05 08:03:36 AM: Invoking solver GUROBI  to obtain a solution.\n",
      "Set parameter QCPDual to value 1\n",
      "Set parameter TimeLimit to value 10\n",
      "Gurobi Optimizer version 9.5.2 build v9.5.2rc0 (linux64)\n",
      "Thread count: 4 physical cores, 4 logical processors, using up to 4 threads\n",
      "Optimize a model with 58 rows, 168 columns and 504 nonzeros\n",
      "Model fingerprint: 0x24607cd6\n",
      "Model has 14193 quadratic objective terms\n",
      "Variable types: 0 continuous, 168 integer (168 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-04, 1e+00]\n",
      "  Objective range  [0e+00, 0e+00]\n",
      "  QObjective range [8e-10, 1e+01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+00]\n",
      "Warning: Model contains large quadratic objective coefficient range\n",
      "         Consider reformulating model or setting NumericFocus parameter\n",
      "         to avoid numerical issues.\n",
      "Found heuristic solution: objective -538.5880467\n",
      "Presolve removed 2 rows and 0 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 56 rows, 168 columns, 168 nonzeros\n",
      "Presolved model has 14193 quadratic objective terms\n",
      "Variable types: 0 continuous, 168 integer (168 binary)\n",
      "Found heuristic solution: objective -572.1471818\n",
      "\n",
      "Root relaxation: objective -1.706036e+03, 225 iterations, 0.01 seconds (0.02 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0 -1706.0362    0  168 -572.14718 -1706.0362   198%     -    0s\n",
      "H    0     0                    -572.3861007 -1706.0362   198%     -    0s\n",
      "H    0     0                    -572.4337213 -1706.0362   198%     -    0s\n",
      "H    0     0                    -572.4701102 -1706.0362   198%     -    0s\n",
      "     0     0 -1685.7479    0  168 -572.47011 -1685.7479   194%     -    0s\n",
      "     0     2 -1685.7377    0  168 -572.47011 -1685.7377   194%     -    0s\n",
      " 17036 14313 -1561.8689   22  147 -572.47011 -1599.8582   179%   2.2    5s\n",
      " 38226 32450 -1240.1141   45  102 -572.47011 -1582.5824   176%   2.2   10s\n",
      "\n",
      "Explored 38400 nodes (84707 simplex iterations) in 10.00 seconds (13.28 work units)\n",
      "Thread count was 4 (of 4 available processors)\n",
      "\n",
      "Solution count 5: -572.47 -572.434 -572.386 ... -538.588\n",
      "\n",
      "Time limit reached\n",
      "Best objective -5.724701101780e+02, best bound -1.582582353090e+03, gap 176.4480%\n",
      "-------------------------------------------------------------------------------\n",
      "                                    Summary                                    \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Oct 05 08:03:47 AM: Problem status: user_limit\n",
      "(CVXPY) Oct 05 08:03:47 AM: Optimal value: -5.725e+02\n",
      "(CVXPY) Oct 05 08:03:47 AM: Compilation took 1.862e-02 seconds\n",
      "(CVXPY) Oct 05 08:03:47 AM: Solver (including time spent in interface) took 1.002e+01 seconds\n",
      "\n",
      "The optimal value is -572.4701101779938\n",
      "A solution x is\n",
      "[0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1.]\n",
      "evaluate_decision\n",
      " {'layer1.0.conv1': (8, 8), 'layer1.0.conv2': (8, 8), 'layer1.1.conv1': (8, 8), 'layer1.1.conv2': (8, 4), 'layer1.2.conv1': (8, 8), 'layer1.2.conv2': (8, 4), 'layer1.3.conv1': (8, 8), 'layer1.3.conv2': (8, 8), 'layer1.4.conv1': (8, 8), 'layer1.4.conv2': (8, 8), 'layer1.5.conv1': (8, 8), 'layer1.5.conv2': (8, 4), 'layer1.6.conv1': (8, 8), 'layer1.6.conv2': (8, 8), 'layer1.7.conv1': (8, 8), 'layer1.7.conv2': (8, 4), 'layer1.8.conv1': (8, 8), 'layer1.8.conv2': (8, 8), 'layer2.0.conv1': (8, 8), 'layer2.0.conv2': (8, 8), 'layer2.0.downsample.0': (8, 8), 'layer2.1.conv1': (8, 4), 'layer2.1.conv2': (8, 8), 'layer2.2.conv1': (8, 8), 'layer2.2.conv2': (8, 8), 'layer2.3.conv1': (8, 8), 'layer2.3.conv2': (8, 8), 'layer2.4.conv1': (8, 8), 'layer2.4.conv2': (8, 8), 'layer2.5.conv1': (8, 8), 'layer2.5.conv2': (8, 8), 'layer2.6.conv1': (8, 8), 'layer2.6.conv2': (8, 8), 'layer2.7.conv1': (8, 8), 'layer2.7.conv2': (8, 8), 'layer2.8.conv1': (8, 8), 'layer2.8.conv2': (8, 8), 'layer3.0.conv1': (8, 8), 'layer3.0.conv2': (8, 8), 'layer3.0.downsample.0': (8, 8), 'layer3.1.conv1': (8, 8), 'layer3.1.conv2': (8, 8), 'layer3.2.conv1': (8, 8), 'layer3.2.conv2': (8, 8), 'layer3.3.conv1': (8, 8), 'layer3.3.conv2': (8, 8), 'layer3.4.conv1': (8, 8), 'layer3.4.conv2': (8, 8), 'layer3.5.conv1': (8, 8), 'layer3.5.conv2': (8, 8), 'layer3.6.conv1': (8, 8), 'layer3.6.conv2': (8, 8), 'layer3.7.conv1': (8, 8), 'layer3.7.conv2': (8, 4), 'layer3.8.conv1': (8, 4), 'layer3.8.conv2': (8, 8)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'top1': tensor([71.9900], device='cuda:0'),\n",
       "  'top5': tensor([91.5800], device='cuda:0'),\n",
       "  'loss': tensor(1.2911, device='cuda:0'),\n",
       "  'time': 3.3368449211120605},\n",
       " 0.76708984375,\n",
       " 18.031304704)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = MIQCP_optimize(cached_grad=cached_grad,\n",
    "                   layer_bitops=layer_bitops,\n",
    "                   layer_size=layer_size,\n",
    "                   schemes_per_layer=len(aw_scheme),\n",
    "                   bitops_bound=np.inf,size_bound=np.inf,\n",
    "                   naive=False)\n",
    "v = torch.Tensor(v.value)\n",
    "evaluate_decision(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6a85daf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================\n",
      "                                     CVXPY                                     \n",
      "                                     v1.2.1                                    \n",
      "===============================================================================\n",
      "(CVXPY) Oct 05 07:46:13 AM: Your problem has 168 variables, 3 constraints, and 0 parameters.\n",
      "(CVXPY) Oct 05 07:46:13 AM: It is compliant with the following grammars: DCP, DQCP\n",
      "(CVXPY) Oct 05 07:46:13 AM: (If you need to solve this problem multiple times, but with different data, consider using parameters.)\n",
      "(CVXPY) Oct 05 07:46:13 AM: CVXPY will first compile your problem; then, it will invoke a numerical solver to obtain a solution.\n",
      "-------------------------------------------------------------------------------\n",
      "                                  Compilation                                  \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Oct 05 07:46:13 AM: Compiling problem (target solver=GUROBI).\n",
      "(CVXPY) Oct 05 07:46:13 AM: Reduction chain: Dcp2Cone -> CvxAttr2Constr -> ConeMatrixStuffing -> GUROBI\n",
      "(CVXPY) Oct 05 07:46:13 AM: Applying reduction Dcp2Cone\n",
      "(CVXPY) Oct 05 07:46:13 AM: Applying reduction CvxAttr2Constr\n",
      "(CVXPY) Oct 05 07:46:13 AM: Applying reduction ConeMatrixStuffing\n",
      "(CVXPY) Oct 05 07:46:13 AM: Applying reduction GUROBI\n",
      "(CVXPY) Oct 05 07:46:13 AM: Finished problem compilation (took 9.189e-03 seconds).\n",
      "-------------------------------------------------------------------------------\n",
      "                                Numerical solver                               \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Oct 05 07:46:13 AM: Invoking solver GUROBI  to obtain a solution.\n",
      "Set parameter QCPDual to value 1\n",
      "Gurobi Optimizer version 9.5.2 build v9.5.2rc0 (linux64)\n",
      "Thread count: 4 physical cores, 4 logical processors, using up to 4 threads\n",
      "Optimize a model with 58 rows, 168 columns and 504 nonzeros\n",
      "Model fingerprint: 0x5bccaa24\n",
      "Variable types: 0 continuous, 168 integer (168 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-04, 1e+00]\n",
      "  Objective range  [3e-01, 7e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [4e-01, 1e+00]\n",
      "Found heuristic solution: objective 46.6047925\n",
      "Presolve removed 18 rows and 67 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 40 rows, 101 columns, 187 nonzeros\n",
      "Found heuristic solution: objective 24.1966263\n",
      "Variable types: 0 continuous, 101 integer (101 binary)\n",
      "\n",
      "Root relaxation: objective 2.256415e+01, 19 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0   22.56415    0    1   24.19663   22.56415  6.75%     -    0s\n",
      "H    0     0                      22.6483195   22.56415  0.37%     -    0s\n",
      "H    0     0                      22.5917661   22.56415  0.12%     -    0s\n",
      "     0     0   22.57343    0    2   22.59177   22.57343  0.08%     -    0s\n",
      "     0     0   22.57343    0    1   22.59177   22.57343  0.08%     -    0s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 1\n",
      "  Cover: 1\n",
      "\n",
      "Explored 1 nodes (22 simplex iterations) in 0.02 seconds (0.00 work units)\n",
      "Thread count was 4 (of 4 available processors)\n",
      "\n",
      "Solution count 5: 22.5918 22.6483 22.6962 ... 46.6048\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 2.259176611900e+01, best bound 2.259124615788e+01, gap 0.0023%\n",
      "-------------------------------------------------------------------------------\n",
      "                                    Summary                                    \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Oct 05 07:46:13 AM: Problem status: optimal\n",
      "(CVXPY) Oct 05 07:46:13 AM: Optimal value: 2.259e+01\n",
      "(CVXPY) Oct 05 07:46:13 AM: Compilation took 9.189e-03 seconds\n",
      "(CVXPY) Oct 05 07:46:13 AM: Solver (including time spent in interface) took 2.751e-02 seconds\n",
      "\n",
      "The optimal value is 22.591766119003296\n",
      "A solution x is\n",
      "[ 0.  1.  0.  0.  1.  0.  0.  1.  0.  0.  1.  0.  0.  1.  0.  0.  1.  0.\n",
      " -0.  1. -0. -0.  1. -0. -0. -0.  1.  0.  1.  0. -0. -0.  1.  1.  0.  0.\n",
      " -0. -0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. -0. -0.  1.  0.  1.  0.\n",
      "  0. -0.  1.  0. -0.  1. -0. -0.  1.  0.  1. -0.  0.  1. -0.  0.  1. -0.\n",
      "  0.  1. -0.  0.  1. -0.  0.  1.  0.  0.  1. -0.  0.  1. -0.  0.  1. -0.\n",
      "  0.  1. -0.  0.  1.  0.  0.  1. -0.  0.  1. -0.  0.  1. -0.  0.  1. -0.\n",
      "  0.  1. -0.  0. -0.  1.  0.  1. -0. -0. -0.  1.  0.  1. -0.  0.  1. -0.\n",
      "  0.  1. -0.  0.  1. -0.  0.  1. -0.  0.  1. -0.  0.  1. -0.  0.  1. -0.\n",
      "  0.  1. -0.  0.  1. -0.  0.  1. -0.  0.  1. -0.  0.  1.  0.  1. -0.  0.\n",
      "  1. -0.  0.  1. -0.  0.]\n",
      "evaluate_decision\n",
      " {'layer1.0.conv1': (8, 4), 'layer1.0.conv2': (8, 4), 'layer1.1.conv1': (8, 4), 'layer1.1.conv2': (8, 4), 'layer1.2.conv1': (8, 4), 'layer1.2.conv2': (8, 4), 'layer1.3.conv1': (8, 4), 'layer1.3.conv2': (8, 4), 'layer1.4.conv1': (8, 8), 'layer1.4.conv2': (8, 4), 'layer1.5.conv1': (8, 8), 'layer1.5.conv2': (8, 2), 'layer1.6.conv1': (8, 8), 'layer1.6.conv2': (8, 4), 'layer1.7.conv1': (8, 4), 'layer1.7.conv2': (8, 2), 'layer1.8.conv1': (8, 8), 'layer1.8.conv2': (8, 4), 'layer2.0.conv1': (8, 8), 'layer2.0.conv2': (8, 8), 'layer2.0.downsample.0': (8, 8), 'layer2.1.conv1': (8, 4), 'layer2.1.conv2': (8, 4), 'layer2.2.conv1': (8, 4), 'layer2.2.conv2': (8, 4), 'layer2.3.conv1': (8, 4), 'layer2.3.conv2': (8, 4), 'layer2.4.conv1': (8, 4), 'layer2.4.conv2': (8, 4), 'layer2.5.conv1': (8, 4), 'layer2.5.conv2': (8, 4), 'layer2.6.conv1': (8, 4), 'layer2.6.conv2': (8, 4), 'layer2.7.conv1': (8, 4), 'layer2.7.conv2': (8, 4), 'layer2.8.conv1': (8, 4), 'layer2.8.conv2': (8, 4), 'layer3.0.conv1': (8, 8), 'layer3.0.conv2': (8, 4), 'layer3.0.downsample.0': (8, 8), 'layer3.1.conv1': (8, 4), 'layer3.1.conv2': (8, 4), 'layer3.2.conv1': (8, 4), 'layer3.2.conv2': (8, 4), 'layer3.3.conv1': (8, 4), 'layer3.3.conv2': (8, 4), 'layer3.4.conv1': (8, 4), 'layer3.4.conv2': (8, 4), 'layer3.5.conv1': (8, 4), 'layer3.5.conv2': (8, 4), 'layer3.6.conv1': (8, 4), 'layer3.6.conv2': (8, 4), 'layer3.7.conv1': (8, 4), 'layer3.7.conv2': (8, 2), 'layer3.8.conv1': (8, 2), 'layer3.8.conv2': (8, 2)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'top1': tensor([56.4800], device='cuda:0'),\n",
       "  'top5': tensor([82.5700], device='cuda:0'),\n",
       "  'loss': tensor(1.7349, device='cuda:0'),\n",
       "  'time': 3.334693670272827},\n",
       " 0.3990478515625,\n",
       " 11.214422016)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = MIQCP_optimize(cached_grad=cached_grad,\n",
    "                   layer_bitops=layer_bitops,\n",
    "                   layer_size=layer_size,\n",
    "                   schemes_per_layer=len(aw_scheme),\n",
    "                   bitops_bound=np.inf,size_bound=0.4,\n",
    "                   naive=True)\n",
    "v = torch.Tensor(v.value)\n",
    "evaluate_decision(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d269107e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================\n",
      "                                     CVXPY                                     \n",
      "                                     v1.2.1                                    \n",
      "===============================================================================\n",
      "(CVXPY) Oct 05 08:11:37 AM: Your problem has 168 variables, 3 constraints, and 0 parameters.\n",
      "(CVXPY) Oct 05 08:11:37 AM: It is compliant with the following grammars: DCP, DQCP\n",
      "(CVXPY) Oct 05 08:11:37 AM: (If you need to solve this problem multiple times, but with different data, consider using parameters.)\n",
      "(CVXPY) Oct 05 08:11:37 AM: CVXPY will first compile your problem; then, it will invoke a numerical solver to obtain a solution.\n",
      "-------------------------------------------------------------------------------\n",
      "                                  Compilation                                  \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Oct 05 08:11:37 AM: Compiling problem (target solver=GUROBI).\n",
      "(CVXPY) Oct 05 08:11:37 AM: Reduction chain: CvxAttr2Constr -> Qp2SymbolicQp -> QpMatrixStuffing -> GUROBI\n",
      "(CVXPY) Oct 05 08:11:37 AM: Applying reduction CvxAttr2Constr\n",
      "(CVXPY) Oct 05 08:11:37 AM: Applying reduction Qp2SymbolicQp\n",
      "(CVXPY) Oct 05 08:11:37 AM: Applying reduction QpMatrixStuffing\n",
      "(CVXPY) Oct 05 08:11:37 AM: Applying reduction GUROBI\n",
      "(CVXPY) Oct 05 08:11:37 AM: Finished problem compilation (took 1.334e-02 seconds).\n",
      "-------------------------------------------------------------------------------\n",
      "                                Numerical solver                               \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Oct 05 08:11:37 AM: Invoking solver GUROBI  to obtain a solution.\n",
      "Set parameter QCPDual to value 1\n",
      "Set parameter TimeLimit to value 10\n",
      "Gurobi Optimizer version 9.5.2 build v9.5.2rc0 (linux64)\n",
      "Thread count: 4 physical cores, 4 logical processors, using up to 4 threads\n",
      "Optimize a model with 58 rows, 168 columns and 504 nonzeros\n",
      "Model fingerprint: 0x12933fd9\n",
      "Model has 14193 quadratic objective terms\n",
      "Variable types: 0 continuous, 168 integer (168 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-04, 1e+00]\n",
      "  Objective range  [0e+00, 0e+00]\n",
      "  QObjective range [8e-10, 1e+01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [4e-01, 1e+00]\n",
      "Warning: Model contains large quadratic objective coefficient range\n",
      "         Consider reformulating model or setting NumericFocus parameter\n",
      "         to avoid numerical issues.\n",
      "Found heuristic solution: objective -540.6993870\n",
      "Presolve removed 1 rows and 0 columns\n",
      "Presolve time: 0.01s\n",
      "Presolved: 57 rows, 168 columns, 300 nonzeros\n",
      "Presolved model has 14193 quadratic objective terms\n",
      "Variable types: 0 continuous, 168 integer (168 binary)\n",
      "Found heuristic solution: objective -560.2846631\n",
      "\n",
      "Root relaxation: objective -1.676031e+03, 226 iterations, 0.03 seconds (0.02 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0 -1676.0306    0  168 -560.28466 -1676.0306   199%     -    0s\n",
      "H    0     0                    -569.0313656 -1676.0306   195%     -    0s\n",
      "H    0     0                    -570.1061333 -1676.0306   194%     -    0s\n",
      "     0     0 -1658.7494    0  168 -570.10613 -1658.7494   191%     -    0s\n",
      "H    0     0                    -570.5186352 -1658.7494   191%     -    0s\n",
      "     0     2 -1658.7494    0  168 -570.51864 -1658.7494   191%     -    0s\n",
      "H   31    40                    -570.5352856 -1633.1257   186%   2.5    0s\n",
      "H   37    40                    -570.5875750 -1633.1257   186%   2.4    0s\n",
      "H12803 10372                    -570.7393063 -1578.0234   176%   2.3    4s\n",
      " 14424 11699 -1147.0572   40   86 -570.73931 -1574.0572   176%   2.3    5s\n",
      "H15915 13534                    -570.7402499 -1573.8367   176%   2.4    6s\n",
      "H18184 15374                    -570.8640734 -1570.9302   175%   2.4    6s\n",
      " 31507 27101 -903.84187   64   54 -570.86407 -1558.3967   173%   2.4   10s\n",
      "\n",
      "Explored 31706 nodes (75953 simplex iterations) in 10.00 seconds (14.55 work units)\n",
      "Thread count was 4 (of 4 available processors)\n",
      "\n",
      "Solution count 10: -570.864 -570.74 -570.739 ... -540.699\n",
      "\n",
      "Time limit reached\n",
      "Best objective -5.708640733808e+02, best bound -1.558355191541e+03, gap 172.9818%\n",
      "-------------------------------------------------------------------------------\n",
      "                                    Summary                                    \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Oct 05 08:11:47 AM: Problem status: user_limit\n",
      "(CVXPY) Oct 05 08:11:47 AM: Optimal value: -5.709e+02\n",
      "(CVXPY) Oct 05 08:11:47 AM: Compilation took 1.334e-02 seconds\n",
      "(CVXPY) Oct 05 08:11:47 AM: Solver (including time spent in interface) took 1.002e+01 seconds\n",
      "\n",
      "The optimal value is -570.8640733808279\n",
      "A solution x is\n",
      "[0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1.\n",
      " 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0.\n",
      " 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      "evaluate_decision\n",
      " {'layer1.0.conv1': (8, 8), 'layer1.0.conv2': (8, 4), 'layer1.1.conv1': (8, 8), 'layer1.1.conv2': (8, 8), 'layer1.2.conv1': (8, 8), 'layer1.2.conv2': (8, 4), 'layer1.3.conv1': (8, 8), 'layer1.3.conv2': (8, 8), 'layer1.4.conv1': (8, 4), 'layer1.4.conv2': (8, 8), 'layer1.5.conv1': (8, 4), 'layer1.5.conv2': (8, 8), 'layer1.6.conv1': (8, 2), 'layer1.6.conv2': (8, 2), 'layer1.7.conv1': (8, 8), 'layer1.7.conv2': (8, 4), 'layer1.8.conv1': (8, 8), 'layer1.8.conv2': (8, 8), 'layer2.0.conv1': (8, 4), 'layer2.0.conv2': (8, 8), 'layer2.0.downsample.0': (8, 8), 'layer2.1.conv1': (8, 4), 'layer2.1.conv2': (8, 4), 'layer2.2.conv1': (8, 8), 'layer2.2.conv2': (8, 4), 'layer2.3.conv1': (8, 8), 'layer2.3.conv2': (8, 4), 'layer2.4.conv1': (8, 8), 'layer2.4.conv2': (8, 2), 'layer2.5.conv1': (8, 8), 'layer2.5.conv2': (8, 2), 'layer2.6.conv1': (8, 2), 'layer2.6.conv2': (8, 2), 'layer2.7.conv1': (8, 8), 'layer2.7.conv2': (8, 8), 'layer2.8.conv1': (8, 8), 'layer2.8.conv2': (8, 4), 'layer3.0.conv1': (8, 8), 'layer3.0.conv2': (8, 8), 'layer3.0.downsample.0': (8, 4), 'layer3.1.conv1': (8, 2), 'layer3.1.conv2': (8, 2), 'layer3.2.conv1': (8, 4), 'layer3.2.conv2': (8, 4), 'layer3.3.conv1': (8, 4), 'layer3.3.conv2': (8, 4), 'layer3.4.conv1': (8, 4), 'layer3.4.conv2': (8, 2), 'layer3.5.conv1': (8, 4), 'layer3.5.conv2': (8, 4), 'layer3.6.conv1': (8, 2), 'layer3.6.conv2': (8, 4), 'layer3.7.conv1': (8, 2), 'layer3.7.conv2': (8, 2), 'layer3.8.conv1': (8, 2), 'layer3.8.conv2': (8, 2)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'top1': tensor([39.4300], device='cuda:0'),\n",
       "  'top5': tensor([70.0200], device='cuda:0'),\n",
       "  'loss': tensor(2.4650, device='cuda:0'),\n",
       "  'time': 3.318599224090576},\n",
       " 0.399169921875,\n",
       " 12.809281536)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = MIQCP_optimize(cached_grad=cached_grad,\n",
    "                   layer_bitops=layer_bitops,\n",
    "                   layer_size=layer_size,\n",
    "                   schemes_per_layer=len(aw_scheme),\n",
    "                   bitops_bound=np.inf,size_bound=0.4,\n",
    "                   naive=False)\n",
    "v = torch.Tensor(v.value)\n",
    "evaluate_decision(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f242b9d1",
   "metadata": {},
   "source": [
    "## Random MPQ (old code to generate random MPQ schemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c568ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_size = []\n",
    "# random_acc = []\n",
    "# for i in range(500):\n",
    "#     v = torch.randn(L)\n",
    "#     res,size = evaluate_decision(v)\n",
    "#     random_size.append(size)\n",
    "#     random_acc.append(res['mean_acc'])\n",
    "\n",
    "# random_size,random_acc\n",
    "\n",
    "# with open('resnet56_random_baseline.pkl','wb') as f:\n",
    "#     pickle.dump({'size':random_size,'acc':random_acc},f)\n",
    "    \n",
    "\n",
    "# plt.hist(random_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d354c4",
   "metadata": {},
   "source": [
    "## Pareto-Frontier of CLADO vs Inter-Layer Dependency Unaware Optimization (Naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8df83f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_iters = (5000,)\n",
    "#lambda1s = np.logspace(-5,-3,3)\n",
    "lambda1s = (0,1e-1,1)\n",
    "lambda2s = np.logspace(-3,0,100) # for 8 x (8,4)\n",
    "sample_size = 1\n",
    "results = {}\n",
    "for n_iter in n_iters:\n",
    "    for lambda1 in lambda1s:\n",
    "        for lambda2 in lambda2s:\n",
    "            feint_loss,feint_size,feint_bitops = [],[],[]\n",
    "            trial_name = f'{aw_scheme}bits_CLADO_lambda1{lambda1}_lambda2{lambda2}_{n_iter}iters'\n",
    "            print(trial_name)\n",
    "            for repeat in range(sample_size):\n",
    "                v = optimize(n_iteration=n_iter,lr=2e-3,beta=[20,2],lambda1=lambda1,lambda2=lambda2,naive=False)\n",
    "                perf,size,bitops = evaluate_decision(v)\n",
    "                feint_loss.append(perf)\n",
    "                feint_size.append(size)\n",
    "                feint_bitops.append(bitops)\n",
    "            results[trial_name] = {'size':feint_size,'perf':feint_loss,'bitops':feint_bitops}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba10c61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f48f9eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_iters = (5000,)\n",
    "lambda1s = np.logspace(-5,-3,3)\n",
    "lambda1s = (0,1e-1,1)\n",
    "lambda2s = np.logspace(-3,0,100) \n",
    "sample_size = 1\n",
    "\n",
    "for n_iter in n_iters:\n",
    "    for lambda1 in lambda1s:\n",
    "        for lambda2 in lambda2s:\n",
    "            naive_loss,naive_size,naive_bitops = [],[],[]\n",
    "            print('lambda2:',lambda2)\n",
    "            trial_name = f'{aw_scheme}bits_NAIVE_lambda1{lambda1}_lambda2{lambda2}_{n_iter}iters'\n",
    "            for repeat in range(sample_size):\n",
    "                v = optimize(n_iteration=n_iter,lr=2e-3,beta=[20,2],lambda1=lambda1,lambda2=lambda2,naive=True)\n",
    "                perf,size,bitops = evaluate_decision(v)\n",
    "                naive_loss.append(perf)\n",
    "                naive_size.append(size)\n",
    "                naive_bitops.append(bitops)\n",
    "            results[trial_name] = {'size':naive_size,'perf':naive_loss,'bitops':naive_bitops}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8d7490",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('xx','wb') as f:\n",
    "    pickle.dump(results,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c56c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('saved/general48c10resnet56results.pkl','rb') as f:\n",
    "#     c48 = pickle.load(f)\n",
    "# with open('saved/general248c10resnet56results.pkl','rb') as f:\n",
    "#     c248 = pickle.load(f)\n",
    "# with open('i1k_resnet50_CachedGrad_((8, 2), (8, 4), (8, 8))i1k_resnet50.pkl','rb') as f:\n",
    "with open('cifar100_resnet56_CachedGrad_a248w248c100_resnet56.pkl','rb') as f:\n",
    "    c248 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ba2e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "#with open('i1k_resnet50_CachedGrad_((8, 2), (8, 4), (8, 8))i1k_resnet50.pkl','rb') as f:\n",
    "with open('cifar100_resnet56_CachedGrad_a248w248c100_resnet56.pkl','rb') as f:\n",
    "    res = pickle.load(f)\n",
    "\n",
    "clado_test_acc,clado_calib_acc,clado_size,clado_bitops = [],[],[],[]\n",
    "naive_test_acc,naive_calib_acc,naive_size,naive_bitops = [],[],[],[]\n",
    "\n",
    "for item in res['clado_res']:\n",
    "    test_perf,calib_perf,size,bitops,_ = item\n",
    "    clado_test_acc.append(test_perf['top1'].cpu().numpy())\n",
    "    clado_calib_acc.append(calib_perf['top1'].cpu().numpy())\n",
    "    clado_size.append(size)\n",
    "    clado_bitops.append(bitops)\n",
    "\n",
    "for item in res['naive_res']:\n",
    "    test_perf,calib_perf,size,bitops,_ = item\n",
    "    naive_test_acc.append(test_perf['top1'].cpu().numpy())\n",
    "    naive_calib_acc.append(calib_perf['top1'].cpu().numpy())\n",
    "    naive_size.append(size)\n",
    "    naive_bitops.append(bitops)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12,8)\n",
    "plt.plot(clado_size,clado_test_acc,label='TEST cross-layer dependency aware',marker='v')\n",
    "plt.plot(naive_size,naive_test_acc,label='TEST cross-layer dependency unaware',marker='^')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12,8)\n",
    "plt.plot(clado_size,clado_calib_acc,label='CALIB cross-layer dependency aware',marker='v')\n",
    "plt.plot(naive_size,naive_calib_acc,label='CALIB cross-layer dependency unaware',marker='^')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ffd654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "with open('i1k_resnet50_CachedGrad_((8, 2), (8, 4), (8, 8))i1k_resnet50KL.pkl','rb') as f:\n",
    "    res = pickle.load(f)\n",
    "\n",
    "clado_test_acc,clado_calib_acc,clado_size,clado_bitops = [],[],[],[]\n",
    "naive_test_acc,naive_calib_acc,naive_size,naive_bitops = [],[],[],[]\n",
    "\n",
    "for item in res['clado_res']:\n",
    "    test_perf,calib_perf,size,bitops,_ = item\n",
    "    clado_test_acc.append(test_perf['top1'].cpu().numpy())\n",
    "    clado_calib_acc.append(calib_perf['top1'].cpu().numpy())\n",
    "    clado_size.append(size)\n",
    "    clado_bitops.append(bitops)\n",
    "\n",
    "for item in res['naive_res']:\n",
    "    test_perf,calib_perf,size,bitops,_ = item\n",
    "    naive_test_acc.append(test_perf['top1'].cpu().numpy())\n",
    "    naive_calib_acc.append(calib_perf['top1'].cpu().numpy())\n",
    "    naive_size.append(size)\n",
    "    naive_bitops.append(bitops)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12,8)\n",
    "plt.plot(clado_size,clado_test_acc,label='TEST cross-layer dependency aware',marker='v')\n",
    "plt.plot(naive_size,naive_test_acc,label='TEST cross-layer dependency unaware',marker='^')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12,8)\n",
    "plt.plot(clado_size,clado_calib_acc,label='CALIB cross-layer dependency aware',marker='v')\n",
    "plt.plot(naive_size,naive_calib_acc,label='CALIB cross-layer dependency unaware',marker='^')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad0322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPF(xs,ys):\n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "    \n",
    "    order = np.argsort(xs)\n",
    "    \n",
    "    xs = xs[order]\n",
    "    ys = ys[order]\n",
    "    \n",
    "    cur_max = -1\n",
    "    for i in range(ys.shape[0]):\n",
    "        if ys[i] > cur_max:\n",
    "            cur_max = ys[i]\n",
    "        ys[i] = cur_max\n",
    "    \n",
    "    return xs,ys\n",
    "plt.rcParams['figure.figsize'] = (12,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15010711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clado_size,clado_acc = [], []\n",
    "# naive_size,naive_acc = [], []\n",
    "# for trial in c48:\n",
    "#     size = c48[trial]['size']\n",
    "#     perf = c48[trial]['perf']\n",
    "#     perf = [x['mean_acc'] for x in perf]\n",
    "#     if 'NAIVE' in trial:\n",
    "#         naive_size,naive_acc = naive_size+size,naive_acc+perf\n",
    "#     if 'CLADO' in trial:\n",
    "#         clado_size,clado_acc = clado_size+size,clado_acc+perf \n",
    "#     #size = np.array(size)\n",
    "#     #perf = np.array(perf)\n",
    "#     #size,perf = getPF(size,perf)\n",
    "#     #plt.plot(size,perf,label=trial)\n",
    "# c48_naive_pf = getPF(np.array(naive_size),np.array(naive_acc))\n",
    "# c48_clado_pf = getPF(np.array(clado_size),np.array(clado_acc))\n",
    "# plt.plot(c48_naive_pf[0],c48_naive_pf[1],label='(4,8)bits naive MPQ')\n",
    "# plt.plot(c48_clado_pf[0],c48_clado_pf[1],label='(4,8)bits clado MPQ')\n",
    "\n",
    "clado_size,clado_acc = [], []\n",
    "naive_size,naive_acc = [], []\n",
    "for trial in c248:\n",
    "    size = c248[trial]['size']\n",
    "    perf = c248[trial]['perf']\n",
    "    perf = [x['mean_acc'] for x in perf]\n",
    "    if 'NAIVE' in trial:\n",
    "        naive_size,naive_acc = naive_size+size,naive_acc+perf\n",
    "    if 'CLADO' in trial:\n",
    "        clado_size,clado_acc = clado_size+size,clado_acc+perf \n",
    "    #size = np.array(size)\n",
    "    #perf = np.array(perf)\n",
    "    #size,perf = getPF(size,perf)\n",
    "    #plt.plot(size,perf,label=trial)\n",
    "c248_naive_pf = getPF(np.array(naive_size),np.array(naive_acc))\n",
    "c248_clado_pf = getPF(np.array(clado_size),np.array(clado_acc))\n",
    "plt.plot(c248_naive_pf[0],c248_naive_pf[1],label='(2,4,8)bits naive MPQ')\n",
    "plt.plot(c248_clado_pf[0],c248_clado_pf[1],label='(2,4,8)bits clado MPQ')\n",
    "plt.legend()\n",
    "\n",
    "plt.xlim([0.4,0.7])\n",
    "plt.ylim([0.88,0.95])\n",
    "plt.xlabel('Hardware Cost (Model Size in MB)',fontsize=20)\n",
    "plt.ylabel('Performance (Accuracy)',fontsize=20)\n",
    "plt.legend()\n",
    "# plt.savefig('c10resnet56_w248.pdf',transparent=True, bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40074e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "c248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f38ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12,8)\n",
    "clado_bitops,clado_acc = [], []\n",
    "naive_bitops,naive_acc = [], []\n",
    "for trial in c248:\n",
    "    bitops = c248[trial]['bitops']\n",
    "    perf = c248[trial]['perf']\n",
    "    perf = [x['top1'].cpu() for x in perf]\n",
    "    if 'NAIVE' in trial:\n",
    "        naive_bitops,naive_acc = naive_bitops+bitops,naive_acc+perf\n",
    "    if 'CLADO' in trial:\n",
    "        clado_bitops,clado_acc = clado_bitops+bitops,clado_acc+perf \n",
    "    #size = np.array(size)\n",
    "    #perf = np.array(perf)\n",
    "    #size,perf = getPF(size,perf)\n",
    "    #plt.plot(size,perf,label=trial)\n",
    "c248_naive_pf = (np.array(naive_bitops),np.array(naive_acc))\n",
    "c248_clado_pf = (np.array(clado_bitops),np.array(clado_acc))\n",
    "plt.scatter(c248_naive_pf[0],c248_naive_pf[1],label='naive MPQ')\n",
    "plt.scatter(c248_clado_pf[0],c248_clado_pf[1],label='CLADO MPQ')\n",
    "plt.legend()\n",
    "\n",
    "# plt.xlim([0.4,0.7])\n",
    "# plt.ylim([0.88,0.95])\n",
    "plt.xlabel('Hardware Cost (Model bitops in Gops)',fontsize=20)\n",
    "plt.ylabel('Performance (Accuracy)',fontsize=20)\n",
    "plt.legend()\n",
    "#plt.savefig('c100resnet56_a48w48.pdf',transparent=True, bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebb97d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "c248_naive_pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10701d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "c248_clado_pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b3d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "c248_naive_pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12da08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "c48_naive_size = np.array(c48['naive_size'])\n",
    "c48_naive_loss = c48['naive_loss']\n",
    "c48_feint_size = np.array(c48['feint_size'])\n",
    "c48_feint_loss = c48['feint_loss']\n",
    "\n",
    "c48_naive_acc = []\n",
    "for i in range(len(c48_naive_loss)):\n",
    "    c48_naive_acc.append(c48_naive_loss[i]['mean_acc'])\n",
    "\n",
    "c48_feint_acc = []\n",
    "for i in range(len(c48_feint_loss)):\n",
    "    c48_feint_acc.append(c48_feint_loss[i]['mean_acc'])\n",
    "\n",
    "c248_naive_size = np.array(c248['naive_size'])\n",
    "c248_naive_loss = c248['naive_loss']\n",
    "c248_feint_size = np.array(c248['feint_size'])\n",
    "c248_feint_loss = c248['feint_loss']\n",
    "\n",
    "c248_naive_acc = []\n",
    "for i in range(len(c248_naive_loss)):\n",
    "    c248_naive_acc.append(c248_naive_loss[i]['mean_acc'])\n",
    "\n",
    "c248_feint_acc = []\n",
    "for i in range(len(c248_feint_loss)):\n",
    "    c248_feint_acc.append(c248_feint_loss[i]['mean_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa465467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPF(xs,ys):\n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "    \n",
    "    order = np.argsort(xs)\n",
    "    \n",
    "    xs = xs[order]\n",
    "    ys = ys[order]\n",
    "    \n",
    "    cur_max = -1\n",
    "    for i in range(ys.shape[0]):\n",
    "        if ys[i] > cur_max:\n",
    "            cur_max = ys[i]\n",
    "        ys[i] = cur_max\n",
    "    \n",
    "    return xs,ys\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,10)\n",
    "\n",
    "c48_feint_size,c48_feint_acc = getPF(c48_feint_size,c48_feint_acc)\n",
    "\n",
    "c48_naive_size,c48_naive_acc = getPF(c48_naive_size,c48_naive_acc)\n",
    "\n",
    "c248_feint_size,c248_feint_acc = getPF(c248_feint_size,c248_feint_acc)\n",
    "\n",
    "c248_naive_size,c248_naive_acc = getPF(c248_naive_size,c248_naive_acc)\n",
    "\n",
    "plt.scatter(c48_naive_size,c48_naive_acc,color='lightcoral',alpha=0.5,label='c48 Inter-Layer Depedency Unaware Optimization')\n",
    "plt.scatter(c48_feint_size,c48_feint_acc,color='lightblue',alpha=0.5,label='c48 FeintLady Optimization')\n",
    "# plt.scatter(c248_naive_size,c248_naive_acc,color='red',alpha=0.5,label='c248 CLADO Used')\n",
    "# plt.scatter(c248_feint_size,c248_feint_acc,color='blue',alpha=0.5,label='c248 CLADO Not Used')\n",
    "\n",
    "plt.xlabel('Hardware cost')\n",
    "plt.ylabel('Performance')\n",
    "plt.legend()\n",
    "#plt.savefig('c100resnet56FeintEffecacy.pdf',transparent=True, bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a211db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(naive_size,naive_acc,color='red',alpha=0.5,label='naive')\n",
    "# plt.scatter(naive_size,naive_acc,color='blue',alpha=0.5,label='feint')\n",
    "plt.xlabel('hardware cost')\n",
    "plt.ylabel('performance')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fc8630",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8252488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fname = 'result_cifar100_shufflenetv2_x2_0_mode0_useaccFalse.pkl'\n",
    "with open(fname,'rb') as f:\n",
    "    res_sfn20 = pickle.load(f)\n",
    "\n",
    "fname = 'result_cifar100_shufflenetv2_x1_5_mode0_useaccFalse.pkl'\n",
    "with open(fname,'rb') as f:\n",
    "    res_sfn15 = pickle.load(f)\n",
    "    \n",
    "fname = 'result_cifar100_mobilenetv2_x1_4_mode0_useaccFalse.pkl'\n",
    "with open(fname,'rb') as f:\n",
    "    res_mbn14 = pickle.load(f)\n",
    "\n",
    "fname = 'result_cifar100_mobilenetv2_x0_75_mode0_useaccFalse.pkl'\n",
    "with open(fname,'rb') as f:\n",
    "    res_mbn075 = pickle.load(f)\n",
    "    \n",
    "\n",
    "fname = 'result_cifar100_resnet56_mode0_useaccFalse.pkl'\n",
    "with open(fname,'rb') as f:\n",
    "    res_rsn56 = pickle.load(f)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f52e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in res_rsn56: print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cbf308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPF_(xs,ys,mode='max',roundtoprecision=1):\n",
    "    pf = {}\n",
    "    for x,y in zip(xs,ys):\n",
    "        new_x = round(x,roundtoprecision)\n",
    "        if new_x in pf:\n",
    "            pf[new_x] = eval(mode)(pf[new_x],y)\n",
    "        else:\n",
    "            pf[new_x] = y\n",
    "    \n",
    "    pf_x,pf_y = [],[]\n",
    "    \n",
    "    for x in pf:\n",
    "        pf_x.append(x)\n",
    "        pf_y.append(pf[x])\n",
    "    \n",
    "    pf_x, pf_y = np.array(pf_x),np.array(pf_y)\n",
    "    \n",
    "    return pf_x,pf_y\n",
    "\n",
    "def getPF(xs,ys):\n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "    \n",
    "    order = np.argsort(xs)\n",
    "    \n",
    "    xs = xs[order]\n",
    "    ys = ys[order]\n",
    "    \n",
    "    cur_max = -1\n",
    "    for i in range(ys.shape[0]):\n",
    "        if ys[i] > cur_max:\n",
    "            cur_max = ys[i]\n",
    "        ys[i] = cur_max\n",
    "    \n",
    "    return xs,ys\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de56442",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_1_mbn075,y_1_mbn075 = getPF(res_mbn075['naive_size'],res_mbn075['naive_acc'])\n",
    "x_2_mbn075,y_2_mbn075 = getPF(res_mbn075['feint_size'],res_mbn075['feint_acc'])\n",
    "\n",
    "x_1_mbn14,y_1_mbn14 = getPF(res_mbn14['naive_size'],res_mbn14['naive_acc'])\n",
    "x_2_mbn14,y_2_mbn14 = getPF(res_mbn14['feint_size'],res_mbn14['feint_acc'])\n",
    "\n",
    "x_1_sfn20,y_1_sfn20 = getPF(res_sfn20['naive_size'],res_sfn20['naive_acc'])\n",
    "x_2_sfn20,y_2_sfn20 = getPF(res_sfn20['feint_size'],res_sfn20['feint_acc'])\n",
    "\n",
    "x_1_sfn15,y_1_sfn15 = getPF(res_sfn15['naive_size'],res_sfn15['naive_acc'])\n",
    "x_2_sfn15,y_2_sfn15 = getPF(res_sfn15['feint_size'],res_sfn15['feint_acc'])\n",
    "\n",
    "x_1_rsn56,y_1_rsn56 = getPF(res_rsn56['naive_size'],res_rsn56['naive_acc'])\n",
    "x_2_rsn56,y_2_rsn56 = getPF(res_rsn56['feint_size'],res_rsn56['feint_acc'])\n",
    "\n",
    "#x_random,y_random = getPF(random_size,random_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b260d86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random baseline vs use/not use gradient on resnet56\n",
    "# plt.rcParams['figure.figsize'] = (12,8)\n",
    "fname = 'result_cifar10_resnet56_mode0_useaccFalse.pkl'\n",
    "with open(fname,'rb') as f:\n",
    "    res_rsn = pickle.load(f)\n",
    "fname = 'resnet56_random_baseline.pkl'\n",
    "with open(fname,'rb') as f:\n",
    "    rand_rsn = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8529a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(res_rsn['feint_size'][:],res_rsn['feint_acc'][:],color='blue',\n",
    "            marker='o',s=20,alpha=0.5,label='Cross-layer Gradients Used')\n",
    "\n",
    "plt.scatter(res_rsn['naive_size'][:],res_rsn['naive_acc'][:],color='red',\n",
    "            marker='o',s=20,alpha=0.5,label='Cross-layer Gradients Ignored')\n",
    "\n",
    "plt.scatter(rand_rsn['size'],rand_rsn['acc'],color='black',marker='o',s=20,alpha=0.5,\n",
    "            label='Random Guess')\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel('Hardware Cost (Model Size in MB)',fontsize=20)\n",
    "plt.ylabel('Performance (Accuracy)',fontsize=20)\n",
    "plt.legend()\n",
    "plt.savefig('c10resnet.pdf',transparent=True, bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a105629d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f175f816",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12,8)\n",
    "\n",
    "plt.plot(x_1_mbn14,y_1_mbn14,color='red',\n",
    "         #marker='^',markersize=3,alpha=0.5,\n",
    "         linewidth=1,label='mobilenetv2_x1_4(N)')\n",
    "plt.plot(x_2_mbn14,y_2_mbn14,color='blue',\n",
    "         #marker='v',markersize=3,alpha=0.5,\n",
    "         linewidth=1,label='mobilenetv2_x1_4(A)')\n",
    "\n",
    "plt.plot(x_1_sfn20,y_1_sfn20,color='lightcoral',\n",
    "         #marker='^',markersize=3,alpha=0.5,\n",
    "         linewidth=1,label='shufflenetv2_x2_0(N)')\n",
    "plt.plot(x_2_sfn20,y_2_sfn20,color='lightblue',\n",
    "         #marker='v',markersize=3,alpha=0.5,\n",
    "         linewidth=1,label='sufflenetv2_x2_0(A)')\n",
    "\n",
    "plt.plot(x_1_sfn15,y_1_sfn15,color='orangered',\n",
    "         #marker='^',markersize=3,alpha=0.5,\n",
    "         linewidth=1,label='shufflenetv2_x1_5(N)')\n",
    "plt.plot(x_2_sfn15,y_2_sfn15,color='cyan',\n",
    "         #marker='v',markersize=3,alpha=0.5,\n",
    "         linewidth=1,label='sufflenetv2_x1_5(A)')\n",
    "\n",
    "plt.plot(x_1_rsn56,y_1_rsn56,color='darkred',\n",
    "         #marker='^',markersize=3,alpha=0.5,\n",
    "         linewidth=1,label='resnet56(N)')\n",
    "plt.plot(x_2_rsn56,y_2_rsn56,color='darkblue',\n",
    "         #marker='v',markersize=3,alpha=0.5,\n",
    "         linewidth=1,label='resnet56(A)')\n",
    "\n",
    "\n",
    "# plt.scatter(x_1,y_1,color='red',marker='^',s=10,alpha=0.5)\n",
    "# plt.scatter(x_2,y_2,color='blue',marker='v',s=10,alpha=0.5)\n",
    "\n",
    "plt.ylim([0.68,0.76])\n",
    "plt.xlim([0.,4])\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel('Hardware Cost (Model Size in MB)',fontsize=20)\n",
    "plt.ylabel('Performance (Accuracy)',fontsize=20)\n",
    "plt.legend()\n",
    "\n",
    "# plt.ylim([0.7,0.755])\n",
    "# plt.xlim([2.7,4.0])\n",
    "plt.savefig('c100pareto_3nets.pdf',transparent=True, bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365e6627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
