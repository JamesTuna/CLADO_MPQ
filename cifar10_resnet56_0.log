Using cache found in /homes/zdeng/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master
Downloading: "https://github.com/chenyaofo/pytorch-cifar-models/releases/download/resnet/cifar10_resnet56-187c023a.pt" to /homes/zdeng/.cache/torch/hub/checkpoints/cifar10_resnet56-187c023a.pt
  0%|          | 0.00/3.39M [00:00<?, ?B/s]100%|██████████| 3.39M/3.39M [00:00<00:00, 126MB/s]
Files already downloaded and verified
Files already downloaded and verified
[MQBENCH] INFO: Quantize model Scheme: BackendType.Academic Mode: Eval
[MQBENCH] INFO: Weight Qconfig:
    FakeQuantize: FixedFakeQuantize Params: {}
    Oberver:      MSEObserver Params: Symmetric: True / Bitwidth: 4 / Per channel: False / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Activation Qconfig:
    FakeQuantize: QDropFakeQuantize Params: {}
    Oberver:      EMAMSEObserver Params: Symmetric: False / Bitwidth: 8 / Per channel: False / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Replace module to qat module.
[MQBENCH] INFO: Set layer conv1 to 8 bit.
[MQBENCH] INFO: Set layer fc to 8 bit.
dbg node_to_quantize_output
 odict_keys([x, relu, layer1_0_relu, layer1_0_relu_1, layer1_1_relu, layer1_1_relu_1, layer1_2_relu, layer1_2_relu_1, layer1_3_relu, layer1_3_relu_1, layer1_4_relu, layer1_4_relu_1, layer1_5_relu, layer1_5_relu_1, layer1_6_relu, layer1_6_relu_1, layer1_7_relu, layer1_7_relu_1, layer1_8_relu, layer1_8_relu_1, layer2_0_relu, layer2_0_relu_1, layer2_1_relu, layer2_1_relu_1, layer2_2_relu, layer2_2_relu_1, layer2_3_relu, layer2_3_relu_1, layer2_4_relu, layer2_4_relu_1, layer2_5_relu, layer2_5_relu_1, layer2_6_relu, layer2_6_relu_1, layer2_7_relu, layer2_7_relu_1, layer2_8_relu, layer2_8_relu_1, layer3_0_relu, layer3_0_relu_1, layer3_1_relu, layer3_1_relu_1, layer3_2_relu, layer3_2_relu_1, layer3_3_relu, layer3_3_relu_1, layer3_4_relu, layer3_4_relu_1, layer3_5_relu, layer3_5_relu_1, layer3_6_relu, layer3_6_relu_1, layer3_7_relu, layer3_7_relu_1, layer3_8_relu, view])
[MQBENCH] INFO: Set x post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant x_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_4_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_5_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_6_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_7_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_8_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_4_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_5_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_6_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_7_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_8_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_4_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_5_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_6_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_6_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_7_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_7_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_8_relu_post_act_fake_quantizer
[MQBENCH] INFO: Set view post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant view_post_act_fake_quantizer
[MQBENCH] INFO: Enable observer and Disable quantize.
fully quantized model:
[MQBENCH] INFO: Disable observer and Enable quantize.
{'mean_acc': 0.8774, 'qtl_acc': 0.8774, 'mean_loss': 0.5196113412893271, 'qtl_loss': 0.5196113412893271, 'test time': 4.373382329940796, 'acc_list': array([0.8774]), 'loss_list': array([0.51961134])}
full precision model:
