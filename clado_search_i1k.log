/homes/zdeng/.conda/envs/mltls/lib/python3.9/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Prepare 2bits model using MQBench
[MQBENCH] INFO: Quantize model Scheme: BackendType.Academic Mode: Eval
[MQBENCH] INFO: Weight Qconfig:
    FakeQuantize: FixedFakeQuantize Params: {}
    Oberver:      MSEObserver Params: Symmetric: True / Bitwidth: 2 / Per channel: False / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Activation Qconfig:
    FakeQuantize: FixedFakeQuantize Params: {}
    Oberver:      EMAMSEObserver Params: Symmetric: False / Bitwidth: 8 / Per channel: False / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Replace module to qat module.
[MQBENCH] INFO: Set layer conv1 to 8 bit.
[MQBENCH] INFO: Set layer fc to 8 bit.
dbg node_to_quantize_output
 odict_keys([x, maxpool, layer1_0_relu, layer1_0_relu_1, layer1_0_relu_2, layer1_1_relu, layer1_1_relu_1, layer1_1_relu_2, layer1_2_relu, layer1_2_relu_1, layer1_2_relu_2, layer2_0_relu, layer2_0_relu_1, layer2_0_relu_2, layer2_1_relu, layer2_1_relu_1, layer2_1_relu_2, layer2_2_relu, layer2_2_relu_1, layer2_2_relu_2, layer2_3_relu, layer2_3_relu_1, layer2_3_relu_2, layer3_0_relu, layer3_0_relu_1, layer3_0_relu_2, layer3_1_relu, layer3_1_relu_1, layer3_1_relu_2, layer3_2_relu, layer3_2_relu_1, layer3_2_relu_2, layer3_3_relu, layer3_3_relu_1, layer3_3_relu_2, layer3_4_relu, layer3_4_relu_1, layer3_4_relu_2, layer3_5_relu, layer3_5_relu_1, layer3_5_relu_2, layer4_0_relu, layer4_0_relu_1, layer4_0_relu_2, layer4_1_relu, layer4_1_relu_1, layer4_1_relu_2, layer4_2_relu, layer4_2_relu_1, flatten])
[MQBENCH] INFO: Set x post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant x_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant maxpool_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_0_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_1_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_2_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_0_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_1_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_2_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_3_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_0_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_1_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_2_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_3_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_4_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_4_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_5_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_5_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_0_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_1_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Set flatten post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant flatten_post_act_fake_quantizer
[MQBENCH] INFO: Enable observer and Disable quantize.
Prepare 4bits model using MQBench
[MQBENCH] INFO: Quantize model Scheme: BackendType.Academic Mode: Eval
[MQBENCH] INFO: Weight Qconfig:
    FakeQuantize: FixedFakeQuantize Params: {}
    Oberver:      MSEObserver Params: Symmetric: True / Bitwidth: 4 / Per channel: False / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Activation Qconfig:
    FakeQuantize: FixedFakeQuantize Params: {}
    Oberver:      EMAMSEObserver Params: Symmetric: False / Bitwidth: 8 / Per channel: False / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Replace module to qat module.
[MQBENCH] INFO: Set layer conv1 to 8 bit.
[MQBENCH] INFO: Set layer fc to 8 bit.
dbg node_to_quantize_output
 odict_keys([x, maxpool, layer1_0_relu, layer1_0_relu_1, layer1_0_relu_2, layer1_1_relu, layer1_1_relu_1, layer1_1_relu_2, layer1_2_relu, layer1_2_relu_1, layer1_2_relu_2, layer2_0_relu, layer2_0_relu_1, layer2_0_relu_2, layer2_1_relu, layer2_1_relu_1, layer2_1_relu_2, layer2_2_relu, layer2_2_relu_1, layer2_2_relu_2, layer2_3_relu, layer2_3_relu_1, layer2_3_relu_2, layer3_0_relu, layer3_0_relu_1, layer3_0_relu_2, layer3_1_relu, layer3_1_relu_1, layer3_1_relu_2, layer3_2_relu, layer3_2_relu_1, layer3_2_relu_2, layer3_3_relu, layer3_3_relu_1, layer3_3_relu_2, layer3_4_relu, layer3_4_relu_1, layer3_4_relu_2, layer3_5_relu, layer3_5_relu_1, layer3_5_relu_2, layer4_0_relu, layer4_0_relu_1, layer4_0_relu_2, layer4_1_relu, layer4_1_relu_1, layer4_1_relu_2, layer4_2_relu, layer4_2_relu_1, flatten])
[MQBENCH] INFO: Set x post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant x_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant maxpool_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_0_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_1_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_2_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_0_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_1_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_2_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_3_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_0_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_1_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_2_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_3_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_4_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_4_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_5_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_5_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_0_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_1_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Set flatten post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant flatten_post_act_fake_quantizer
[MQBENCH] INFO: Enable observer and Disable quantize.
Prepare 8bits model using MQBench
[MQBENCH] INFO: Quantize model Scheme: BackendType.Academic Mode: Eval
[MQBENCH] INFO: Weight Qconfig:
    FakeQuantize: FixedFakeQuantize Params: {}
    Oberver:      MSEObserver Params: Symmetric: True / Bitwidth: 8 / Per channel: False / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Activation Qconfig:
    FakeQuantize: FixedFakeQuantize Params: {}
    Oberver:      EMAMSEObserver Params: Symmetric: False / Bitwidth: 8 / Per channel: False / Pot scale: False / Extra kwargs: {}
[MQBENCH] INFO: Replace module to qat module.
[MQBENCH] INFO: Set layer conv1 to 8 bit.
[MQBENCH] INFO: Set layer fc to 8 bit.
dbg node_to_quantize_output
 odict_keys([x, maxpool, layer1_0_relu, layer1_0_relu_1, layer1_0_relu_2, layer1_1_relu, layer1_1_relu_1, layer1_1_relu_2, layer1_2_relu, layer1_2_relu_1, layer1_2_relu_2, layer2_0_relu, layer2_0_relu_1, layer2_0_relu_2, layer2_1_relu, layer2_1_relu_1, layer2_1_relu_2, layer2_2_relu, layer2_2_relu_1, layer2_2_relu_2, layer2_3_relu, layer2_3_relu_1, layer2_3_relu_2, layer3_0_relu, layer3_0_relu_1, layer3_0_relu_2, layer3_1_relu, layer3_1_relu_1, layer3_1_relu_2, layer3_2_relu, layer3_2_relu_1, layer3_2_relu_2, layer3_3_relu, layer3_3_relu_1, layer3_3_relu_2, layer3_4_relu, layer3_4_relu_1, layer3_4_relu_2, layer3_5_relu, layer3_5_relu_1, layer3_5_relu_2, layer4_0_relu, layer4_0_relu_1, layer4_0_relu_2, layer4_1_relu, layer4_1_relu_1, layer4_1_relu_2, layer4_2_relu, layer4_2_relu_1, flatten])
[MQBENCH] INFO: Set x post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant x_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant maxpool_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_0_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_1_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_2_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_0_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_1_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_2_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_3_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_0_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_1_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_2_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_3_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_3_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_3_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_4_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_4_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_4_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_5_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_5_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_5_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_0_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_0_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_0_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_1_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_1_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_1_relu_2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_2_relu_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_2_relu_1_post_act_fake_quantizer
[MQBENCH] INFO: Set flatten post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant flatten_post_act_fake_quantizer
[MQBENCH] INFO: Enable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
evaluate mqb quantized model
{'top1': tensor([0.0940], device='cuda:0'), 'top5': tensor([0.5000], device='cuda:0'), 'loss': tensor(6.9105, device='cuda:0'), 'time': 148.97695231437683}
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
evaluate mqb quantized model
{'top1': tensor([44.4221], device='cuda:0'), 'top5': tensor([69.3481], device='cuda:0'), 'loss': tensor(2.6117, device='cuda:0'), 'time': 150.80368518829346}
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
evaluate mqb quantized model
{'top1': tensor([75.9321], device='cuda:0'), 'top5': tensor([92.7282], device='cuda:0'), 'loss': tensor(0.9643, device='cuda:0'), 'time': 151.52126097679138}
[MQBENCH] INFO: Disable observer and Disable quantize.
input of  conv1  is  x_post_act_fake_quantizer
input of  layer1.0.conv1  is  maxpool_post_act_fake_quantizer
input of  layer1.0.conv2  is  layer1_0_relu_post_act_fake_quantizer
input of  layer1.0.conv3  is  layer1_0_relu_1_post_act_fake_quantizer
input of  layer1.0.downsample.0  is  maxpool_post_act_fake_quantizer
input of  layer1.1.conv1  is  layer1_0_relu_2_post_act_fake_quantizer
input of  layer1.1.conv2  is  layer1_1_relu_post_act_fake_quantizer
input of  layer1.1.conv3  is  layer1_1_relu_1_post_act_fake_quantizer
input of  layer1.2.conv1  is  layer1_1_relu_2_post_act_fake_quantizer
input of  layer1.2.conv2  is  layer1_2_relu_post_act_fake_quantizer
input of  layer1.2.conv3  is  layer1_2_relu_1_post_act_fake_quantizer
input of  layer2.0.conv1  is  layer1_2_relu_2_post_act_fake_quantizer
input of  layer2.0.conv2  is  layer2_0_relu_post_act_fake_quantizer
input of  layer2.0.conv3  is  layer2_0_relu_1_post_act_fake_quantizer
input of  layer2.0.downsample.0  is  layer1_2_relu_2_post_act_fake_quantizer
input of  layer2.1.conv1  is  layer2_0_relu_2_post_act_fake_quantizer
input of  layer2.1.conv2  is  layer2_1_relu_post_act_fake_quantizer
input of  layer2.1.conv3  is  layer2_1_relu_1_post_act_fake_quantizer
input of  layer2.2.conv1  is  layer2_1_relu_2_post_act_fake_quantizer
input of  layer2.2.conv2  is  layer2_2_relu_post_act_fake_quantizer
input of  layer2.2.conv3  is  layer2_2_relu_1_post_act_fake_quantizer
input of  layer2.3.conv1  is  layer2_2_relu_2_post_act_fake_quantizer
input of  layer2.3.conv2  is  layer2_3_relu_post_act_fake_quantizer
input of  layer2.3.conv3  is  layer2_3_relu_1_post_act_fake_quantizer
input of  layer3.0.conv1  is  layer2_3_relu_2_post_act_fake_quantizer
input of  layer3.0.conv2  is  layer3_0_relu_post_act_fake_quantizer
input of  layer3.0.conv3  is  layer3_0_relu_1_post_act_fake_quantizer
input of  layer3.0.downsample.0  is  layer2_3_relu_2_post_act_fake_quantizer
input of  layer3.1.conv1  is  layer3_0_relu_2_post_act_fake_quantizer
input of  layer3.1.conv2  is  layer3_1_relu_post_act_fake_quantizer
input of  layer3.1.conv3  is  layer3_1_relu_1_post_act_fake_quantizer
input of  layer3.2.conv1  is  layer3_1_relu_2_post_act_fake_quantizer
input of  layer3.2.conv2  is  layer3_2_relu_post_act_fake_quantizer
input of  layer3.2.conv3  is  layer3_2_relu_1_post_act_fake_quantizer
input of  layer3.3.conv1  is  layer3_2_relu_2_post_act_fake_quantizer
input of  layer3.3.conv2  is  layer3_3_relu_post_act_fake_quantizer
input of  layer3.3.conv3  is  layer3_3_relu_1_post_act_fake_quantizer
input of  layer3.4.conv1  is  layer3_3_relu_2_post_act_fake_quantizer
input of  layer3.4.conv2  is  layer3_4_relu_post_act_fake_quantizer
input of  layer3.4.conv3  is  layer3_4_relu_1_post_act_fake_quantizer
input of  layer3.5.conv1  is  layer3_4_relu_2_post_act_fake_quantizer
input of  layer3.5.conv2  is  layer3_5_relu_post_act_fake_quantizer
input of  layer3.5.conv3  is  layer3_5_relu_1_post_act_fake_quantizer
input of  layer4.0.conv1  is  layer3_5_relu_2_post_act_fake_quantizer
input of  layer4.0.conv2  is  layer4_0_relu_post_act_fake_quantizer
input of  layer4.0.conv3  is  layer4_0_relu_1_post_act_fake_quantizer
input of  layer4.0.downsample.0  is  layer3_5_relu_2_post_act_fake_quantizer
input of  layer4.1.conv1  is  layer4_0_relu_2_post_act_fake_quantizer
input of  layer4.1.conv2  is  layer4_1_relu_post_act_fake_quantizer
input of  layer4.1.conv3  is  layer4_1_relu_1_post_act_fake_quantizer
input of  layer4.2.conv1  is  layer4_1_relu_2_post_act_fake_quantizer
input of  layer4.2.conv2  is  layer4_2_relu_post_act_fake_quantizer
input of  layer4.2.conv3  is  layer4_2_relu_1_post_act_fake_quantizer
input of  fc  is  flatten_post_act_fake_quantizer
index 0 layer layer1.0.conv1 scheme (8, 2)bits Ltilde 0.130802
index 1 layer layer1.0.conv1 scheme (8, 4)bits Ltilde 0.054223
index 2 layer layer1.0.conv1 scheme (8, 8)bits Ltilde 0.004730
index 3 layer layer1.0.conv2 scheme (8, 2)bits Ltilde 0.356665
index 4 layer layer1.0.conv2 scheme (8, 4)bits Ltilde 0.097498
index 5 layer layer1.0.conv2 scheme (8, 8)bits Ltilde -0.000108
index 6 layer layer1.0.conv3 scheme (8, 2)bits Ltilde 0.693197
index 7 layer layer1.0.conv3 scheme (8, 4)bits Ltilde 0.016067
index 8 layer layer1.0.conv3 scheme (8, 8)bits Ltilde 0.000699
index 9 layer layer1.0.downsample.0 scheme (8, 2)bits Ltilde 1.486260
index 10 layer layer1.0.downsample.0 scheme (8, 4)bits Ltilde 0.194110
index 11 layer layer1.0.downsample.0 scheme (8, 8)bits Ltilde 0.005257
index 12 layer layer1.1.conv1 scheme (8, 2)bits Ltilde 0.134360
index 13 layer layer1.1.conv1 scheme (8, 4)bits Ltilde -0.003364
index 14 layer layer1.1.conv1 scheme (8, 8)bits Ltilde 0.000073
index 15 layer layer1.1.conv2 scheme (8, 2)bits Ltilde 0.140249
index 16 layer layer1.1.conv2 scheme (8, 4)bits Ltilde 0.007265
index 17 layer layer1.1.conv2 scheme (8, 8)bits Ltilde 0.001535
index 18 layer layer1.1.conv3 scheme (8, 2)bits Ltilde 0.092644
index 19 layer layer1.1.conv3 scheme (8, 4)bits Ltilde 0.005417
index 20 layer layer1.1.conv3 scheme (8, 8)bits Ltilde -0.000432
index 21 layer layer1.2.conv1 scheme (8, 2)bits Ltilde 0.188805
index 22 layer layer1.2.conv1 scheme (8, 4)bits Ltilde 0.002319
index 23 layer layer1.2.conv1 scheme (8, 8)bits Ltilde -0.000453
index 24 layer layer1.2.conv2 scheme (8, 2)bits Ltilde 1.005990
index 25 layer layer1.2.conv2 scheme (8, 4)bits Ltilde 0.001744
index 26 layer layer1.2.conv2 scheme (8, 8)bits Ltilde 0.000118
index 27 layer layer1.2.conv3 scheme (8, 2)bits Ltilde 0.072608
index 28 layer layer1.2.conv3 scheme (8, 4)bits Ltilde 0.004096
index 29 layer layer1.2.conv3 scheme (8, 8)bits Ltilde 0.000196
index 30 layer layer2.0.conv1 scheme (8, 2)bits Ltilde 0.267926
index 31 layer layer2.0.conv1 scheme (8, 4)bits Ltilde 0.019574
index 32 layer layer2.0.conv1 scheme (8, 8)bits Ltilde 0.000210
index 33 layer layer2.0.conv2 scheme (8, 2)bits Ltilde 0.496144
index 34 layer layer2.0.conv2 scheme (8, 4)bits Ltilde 0.011880
index 35 layer layer2.0.conv2 scheme (8, 8)bits Ltilde 0.000091
index 36 layer layer2.0.conv3 scheme (8, 2)bits Ltilde 0.797521
index 37 layer layer2.0.conv3 scheme (8, 4)bits Ltilde 0.031933
index 38 layer layer2.0.conv3 scheme (8, 8)bits Ltilde 0.000369
index 39 layer layer2.0.downsample.0 scheme (8, 2)bits Ltilde 0.978014
index 40 layer layer2.0.downsample.0 scheme (8, 4)bits Ltilde 0.037240
index 41 layer layer2.0.downsample.0 scheme (8, 8)bits Ltilde 0.000654
index 42 layer layer2.1.conv1 scheme (8, 2)bits Ltilde 0.092998
index 43 layer layer2.1.conv1 scheme (8, 4)bits Ltilde 0.003416
index 44 layer layer2.1.conv1 scheme (8, 8)bits Ltilde -0.000202
index 45 layer layer2.1.conv2 scheme (8, 2)bits Ltilde 0.775205
index 46 layer layer2.1.conv2 scheme (8, 4)bits Ltilde 0.033189
index 47 layer layer2.1.conv2 scheme (8, 8)bits Ltilde 0.000840
index 48 layer layer2.1.conv3 scheme (8, 2)bits Ltilde 0.102309
index 49 layer layer2.1.conv3 scheme (8, 4)bits Ltilde 0.009155
index 50 layer layer2.1.conv3 scheme (8, 8)bits Ltilde -0.000235
index 51 layer layer2.2.conv1 scheme (8, 2)bits Ltilde 0.137360
index 52 layer layer2.2.conv1 scheme (8, 4)bits Ltilde 0.005976
index 53 layer layer2.2.conv1 scheme (8, 8)bits Ltilde -0.000000
index 54 layer layer2.2.conv2 scheme (8, 2)bits Ltilde 0.352079
index 55 layer layer2.2.conv2 scheme (8, 4)bits Ltilde 0.008049
index 56 layer layer2.2.conv2 scheme (8, 8)bits Ltilde -0.000115
index 57 layer layer2.2.conv3 scheme (8, 2)bits Ltilde 0.313350
index 58 layer layer2.2.conv3 scheme (8, 4)bits Ltilde 0.018006
index 59 layer layer2.2.conv3 scheme (8, 8)bits Ltilde -0.000037
index 60 layer layer2.3.conv1 scheme (8, 2)bits Ltilde 0.116694
index 61 layer layer2.3.conv1 scheme (8, 4)bits Ltilde 0.005473
index 62 layer layer2.3.conv1 scheme (8, 8)bits Ltilde -0.000136
index 63 layer layer2.3.conv2 scheme (8, 2)bits Ltilde 1.723594
index 64 layer layer2.3.conv2 scheme (8, 4)bits Ltilde 0.004790
index 65 layer layer2.3.conv2 scheme (8, 8)bits Ltilde 0.000140
index 66 layer layer2.3.conv3 scheme (8, 2)bits Ltilde 0.103851
index 67 layer layer2.3.conv3 scheme (8, 4)bits Ltilde 0.004649
index 68 layer layer2.3.conv3 scheme (8, 8)bits Ltilde -0.000431
index 69 layer layer3.0.conv1 scheme (8, 2)bits Ltilde 0.883172
index 70 layer layer3.0.conv1 scheme (8, 4)bits Ltilde 0.042729
index 71 layer layer3.0.conv1 scheme (8, 8)bits Ltilde 0.001141
index 72 layer layer3.0.conv2 scheme (8, 2)bits Ltilde 0.435271
index 73 layer layer3.0.conv2 scheme (8, 4)bits Ltilde 0.015247
index 74 layer layer3.0.conv2 scheme (8, 8)bits Ltilde 0.000257
index 75 layer layer3.0.conv3 scheme (8, 2)bits Ltilde 0.416748
index 76 layer layer3.0.conv3 scheme (8, 4)bits Ltilde 0.001632
index 77 layer layer3.0.conv3 scheme (8, 8)bits Ltilde 0.000156
index 78 layer layer3.0.downsample.0 scheme (8, 2)bits Ltilde 0.483450
index 79 layer layer3.0.downsample.0 scheme (8, 4)bits Ltilde 0.026420
index 80 layer layer3.0.downsample.0 scheme (8, 8)bits Ltilde 0.000886
index 81 layer layer3.1.conv1 scheme (8, 2)bits Ltilde 0.996193
index 82 layer layer3.1.conv1 scheme (8, 4)bits Ltilde 0.010571
index 83 layer layer3.1.conv1 scheme (8, 8)bits Ltilde -0.000358
index 84 layer layer3.1.conv2 scheme (8, 2)bits Ltilde 0.969009
index 85 layer layer3.1.conv2 scheme (8, 4)bits Ltilde 0.005299
index 86 layer layer3.1.conv2 scheme (8, 8)bits Ltilde 0.000177
index 87 layer layer3.1.conv3 scheme (8, 2)bits Ltilde 0.138479
index 88 layer layer3.1.conv3 scheme (8, 4)bits Ltilde 0.017401
index 89 layer layer3.1.conv3 scheme (8, 8)bits Ltilde 0.000385
index 90 layer layer3.2.conv1 scheme (8, 2)bits Ltilde 0.252008
index 91 layer layer3.2.conv1 scheme (8, 4)bits Ltilde 0.006990
index 92 layer layer3.2.conv1 scheme (8, 8)bits Ltilde 0.000482
index 93 layer layer3.2.conv2 scheme (8, 2)bits Ltilde 0.344524
index 94 layer layer3.2.conv2 scheme (8, 4)bits Ltilde 0.005193
index 95 layer layer3.2.conv2 scheme (8, 8)bits Ltilde 0.000343
index 96 layer layer3.2.conv3 scheme (8, 2)bits Ltilde 0.123008
index 97 layer layer3.2.conv3 scheme (8, 4)bits Ltilde 0.010130
index 98 layer layer3.2.conv3 scheme (8, 8)bits Ltilde 0.000371
index 99 layer layer3.3.conv1 scheme (8, 2)bits Ltilde 0.178137
index 100 layer layer3.3.conv1 scheme (8, 4)bits Ltilde 0.008178
index 101 layer layer3.3.conv1 scheme (8, 8)bits Ltilde 0.000470
index 102 layer layer3.3.conv2 scheme (8, 2)bits Ltilde 0.155399
index 103 layer layer3.3.conv2 scheme (8, 4)bits Ltilde 0.003506
index 104 layer layer3.3.conv2 scheme (8, 8)bits Ltilde 0.000533
index 105 layer layer3.3.conv3 scheme (8, 2)bits Ltilde 0.105909
index 106 layer layer3.3.conv3 scheme (8, 4)bits Ltilde 0.009011
index 107 layer layer3.3.conv3 scheme (8, 8)bits Ltilde 0.001752
index 108 layer layer3.4.conv1 scheme (8, 2)bits Ltilde 0.147447
index 109 layer layer3.4.conv1 scheme (8, 4)bits Ltilde 0.010365
index 110 layer layer3.4.conv1 scheme (8, 8)bits Ltilde 0.000512
index 111 layer layer3.4.conv2 scheme (8, 2)bits Ltilde 0.155234
index 112 layer layer3.4.conv2 scheme (8, 4)bits Ltilde 0.002734
index 113 layer layer3.4.conv2 scheme (8, 8)bits Ltilde -0.000392
index 114 layer layer3.4.conv3 scheme (8, 2)bits Ltilde 0.093185
index 115 layer layer3.4.conv3 scheme (8, 4)bits Ltilde 0.010720
index 116 layer layer3.4.conv3 scheme (8, 8)bits Ltilde 0.001603
index 117 layer layer3.5.conv1 scheme (8, 2)bits Ltilde 0.201583
index 118 layer layer3.5.conv1 scheme (8, 4)bits Ltilde 0.004377
index 119 layer layer3.5.conv1 scheme (8, 8)bits Ltilde -0.000736
index 120 layer layer3.5.conv2 scheme (8, 2)bits Ltilde 0.238340
index 121 layer layer3.5.conv2 scheme (8, 4)bits Ltilde 0.005466
index 122 layer layer3.5.conv2 scheme (8, 8)bits Ltilde -0.000314
index 123 layer layer3.5.conv3 scheme (8, 2)bits Ltilde 0.100316
index 124 layer layer3.5.conv3 scheme (8, 4)bits Ltilde 0.005782
index 125 layer layer3.5.conv3 scheme (8, 8)bits Ltilde 0.000523
index 126 layer layer4.0.conv1 scheme (8, 2)bits Ltilde 0.600066
index 127 layer layer4.0.conv1 scheme (8, 4)bits Ltilde 0.004337
index 128 layer layer4.0.conv1 scheme (8, 8)bits Ltilde -0.000796
index 129 layer layer4.0.conv2 scheme (8, 2)bits Ltilde 0.195218
index 130 layer layer4.0.conv2 scheme (8, 4)bits Ltilde 0.010143
index 131 layer layer4.0.conv2 scheme (8, 8)bits Ltilde 0.001896
index 132 layer layer4.0.conv3 scheme (8, 2)bits Ltilde 0.318946
index 133 layer layer4.0.conv3 scheme (8, 4)bits Ltilde 0.003992
index 134 layer layer4.0.conv3 scheme (8, 8)bits Ltilde -0.001074
index 135 layer layer4.0.downsample.0 scheme (8, 2)bits Ltilde 0.060818
index 136 layer layer4.0.downsample.0 scheme (8, 4)bits Ltilde 0.021895
index 137 layer layer4.0.downsample.0 scheme (8, 8)bits Ltilde 0.004642
index 138 layer layer4.1.conv1 scheme (8, 2)bits Ltilde 0.039947
index 139 layer layer4.1.conv1 scheme (8, 4)bits Ltilde 0.010710
index 140 layer layer4.1.conv1 scheme (8, 8)bits Ltilde 0.003213
index 141 layer layer4.1.conv2 scheme (8, 2)bits Ltilde 0.363135
index 142 layer layer4.1.conv2 scheme (8, 4)bits Ltilde 0.015846
index 143 layer layer4.1.conv2 scheme (8, 8)bits Ltilde 0.000047
index 144 layer layer4.1.conv3 scheme (8, 2)bits Ltilde 0.142006
index 145 layer layer4.1.conv3 scheme (8, 4)bits Ltilde 0.010747
index 146 layer layer4.1.conv3 scheme (8, 8)bits Ltilde -0.001517
index 147 layer layer4.2.conv1 scheme (8, 2)bits Ltilde 1.527987
index 148 layer layer4.2.conv1 scheme (8, 4)bits Ltilde 0.001758
index 149 layer layer4.2.conv1 scheme (8, 8)bits Ltilde -0.000310
index 150 layer layer4.2.conv2 scheme (8, 2)bits Ltilde 0.557345
index 151 layer layer4.2.conv2 scheme (8, 4)bits Ltilde 0.003442
index 152 layer layer4.2.conv2 scheme (8, 8)bits Ltilde 0.000381
index 153 layer layer4.2.conv3 scheme (8, 2)bits Ltilde 0.461197
index 154 layer layer4.2.conv3 scheme (8, 4)bits Ltilde 0.000514
index 155 layer layer4.2.conv3 scheme (8, 8)bits Ltilde -0.000502
Set size bound to 10 MB
Restricted license - for non-production use only - expires 2023-10-25

The optimal value is 3.6096560529154544
A solution x is
[0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0.
 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1.
 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.
 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.
 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.
 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0.
 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.]
Traceback (most recent call last):
  File "/homes/zdeng/sum2022/CLADO_MPQ/clado_search_i1k.py", line 595, in <module>
    v1 = torch.Tensor(v.value)
NameError: name 'v' is not defined
